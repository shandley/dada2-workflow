---
title: "DADA2 Workflow with Parameter Optimization for 16S rRNA Amplicon Sequence Variants"
author: "Your Name"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
    latex_engine: xelatex
params:
  generate_report: FALSE
  output_format: "html"
  output_dir: "reports"
  output_file: "dada2_workflow_report"
  multi_run: FALSE
  run_dir: NULL
  big_data: FALSE
---

# Introduction

This document implements an optimized workflow for processing 16S rRNA gene amplicon data using DADA2. The workflow includes automatic parameter optimization, quality filtering, denoising, merging paired reads, chimera removal, and taxonomic assignment to generate a table of amplicon sequence variants (ASVs).

Key features of this optimized workflow:

- **Automatic detection of sequencing platform** (MiSeq, NovaSeq, etc.) - Identifies platform based on read length and quality patterns
- **Optimization of quality filtering parameters** - Custom-tuned for your specific sequence data characteristics
- **Adaptive truncation lengths** - Dynamically adjust to your read lengths and quality profiles for optimal results
- **Expected error threshold optimization** - Balances quality control and read retention for maximum data utilization
- **Primer detection** - Automatically identifies primer sequences to assist with amplicon size calculations
- **Read merging optimization** - Ensures sufficient overlap for reliable ASV calling with minimal artifacts
- **Memory-optimized processing** - Efficient batched operations for large datasets with automatic memory management
- **Enhanced checkpointing system** - Robust recovery from interruptions with comprehensive progress tracking
- **Parallelized execution** - Automatic multi-core utilization with adaptive worker allocation
- **Reference-based taxonomy confidence scoring** - Bootstrap confidence values for all taxonomic assignments
- **Multi-method taxonomy assignment** - Combines results from multiple classifiers for improved accuracy
- **Phylogenetic tree construction** - Integrates phylogenetic information using optimized alignment methods
- **Rarefaction analysis** - Depth optimization with saturation detection for proper diversity comparisons
- **Detailed quality visualization** - Enhanced plots with quality interpretation zones and percentile distributions
- **Comprehensive tracking** - Monitors reads and ASVs through each processing step with detailed reporting
- **Batch effect detection** - For multi-run studies, identifies and quantifies potential batch effects
- **Detailed results export** - Comprehensive data files for downstream analysis and sharing

> **Note on amplicon size and sequencing cycles:** While Illumina sequencers run a fixed number of cycles (e.g., 2x250 for MiSeq, 2x150 for NovaSeq), the expected amplicon size is still crucial for parameter optimization. For example, the V4 region using 515F-806R primers produces a ~253 bp amplicon. For a MiSeq 2x250 run, this means the forward read covers positions 1-250 and the reverse read covers positions 253-4 (reading backward), creating substantial overlap (~247 bp). The algorithm needs this information to properly balance quality trimming while maintaining sufficient overlap for read merging. Even if individual reads are limited by cycle count, knowing the true amplicon size helps optimize trimming parameters and validates that merged sequences represent genuine amplicons rather than artifacts.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Checkpointing functions
checkpoint_dir <- "checkpoints"

# Enhanced package checking and loading function
check_and_load_packages <- function() {
  required_packages <- c(
    "dada2", "ggplot2", "phyloseq", "Biostrings", "ShortRead", 
    "tidyverse", "gridExtra", "knitr", "future", "future.apply"
  )
  
  optional_packages <- c(
    "DECIPHER", "phangorn", "msa", "taxize"
  )
  
  missing_required <- required_packages[!sapply(required_packages, requireNamespace, quietly = TRUE)]
  missing_optional <- optional_packages[!sapply(optional_packages, requireNamespace, quietly = TRUE)]
  
  # Alert about missing required packages
  if (length(missing_required) > 0) {
    message("Installing required packages: ", paste(missing_required, collapse = ", "))
    for (pkg in missing_required) {
      if (pkg %in% c("dada2", "DECIPHER", "phyloseq", "ShortRead", "Biostrings")) {
        if (!requireNamespace("BiocManager", quietly = TRUE)) {
          install.packages("BiocManager")
        }
        BiocManager::install(pkg, update = FALSE)
      } else {
        install.packages(pkg)
      }
    }
  }
  
  # Alert about missing optional packages but don't install automatically
  if (length(missing_optional) > 0) {
    message("NOTE: The following optional packages are not installed: ", 
            paste(missing_optional, collapse = ", "))
    message("These packages enable additional functionality like phylogenetic tree building.")
    message("You can install them with:")
    
    for (pkg in missing_optional) {
      if (pkg %in% c("DECIPHER", "phangorn")) {
        message("  BiocManager::install('", pkg, "')")
      } else {
        message("  install.packages('", pkg, "')")
      }
    }
  }
  
  # Load all required packages
  for (pkg in required_packages) {
    suppressPackageStartupMessages(library(pkg, character.only = TRUE))
  }
  
  # Load available optional packages
  for (pkg in optional_packages) {
    if (requireNamespace(pkg, quietly = TRUE)) {
      suppressPackageStartupMessages(library(pkg, character.only = TRUE))
    }
  }
  
  return(list(
    required = required_packages,
    optional = intersect(optional_packages, 
                        optional_packages[sapply(optional_packages, requireNamespace, quietly = TRUE)]),
    missing = missing_optional
  ))
}

# Create a checkpoint
save_checkpoint <- function(checkpoint_name, object_list, overwrite = FALSE) {
  # Validate inputs
  if(!is.character(checkpoint_name) || length(checkpoint_name) != 1) {
    warning("Invalid checkpoint name. Must be a single character string.")
    return(FALSE)
  }
  
  if(!is.character(object_list) || length(object_list) < 1) {
    warning("Invalid object list. Must be a character vector with at least one item.")
    return(FALSE)
  }
  
  # Create checkpoints directory if it doesn't exist
  tryCatch({
    if(!dir.exists(checkpoint_dir)) {
      dir.create(checkpoint_dir, recursive = TRUE)
      cat("Created checkpoints directory:", checkpoint_dir, "\n")
    }
  }, error = function(e) {
    warning("Failed to create checkpoint directory: ", conditionMessage(e))
    return(FALSE)
  })
  
  # Construct the filename
  filename <- file.path(checkpoint_dir, paste0(checkpoint_name, ".rds"))
  
  # Check if file exists and handle overwrite option
  if(file.exists(filename) && !overwrite) {
    cat("Checkpoint file already exists. Use overwrite = TRUE to overwrite.\n")
    return(FALSE)
  }
  
  # Create a list of objects to save
  checkpoint_data <- list()
  missing_objects <- character(0)
  
  for(obj_name in object_list) {
    if(exists(obj_name, envir = .GlobalEnv)) {
      tryCatch({
        checkpoint_data[[obj_name]] <- get(obj_name, envir = .GlobalEnv)
      }, error = function(e) {
        missing_objects <- c(missing_objects, obj_name)
        warning("Error retrieving object '", obj_name, "': ", conditionMessage(e))
      })
    } else {
      missing_objects <- c(missing_objects, obj_name)
      cat("Warning: Object", obj_name, "not found and not included in checkpoint.\n")
    }
  }
  
  # Report missing objects
  if(length(missing_objects) > 0) {
    cat("The following objects were not included in the checkpoint:", 
        paste(missing_objects, collapse = ", "), "\n")
    
    # If all objects are missing, return an error
    if(length(missing_objects) == length(object_list)) {
      warning("No objects could be saved. Checkpoint creation failed.")
      return(FALSE)
    }
  }
  
  # Add metadata
  checkpoint_data$timestamp <- Sys.time()
  checkpoint_data$object_names <- setdiff(object_list, missing_objects)
  checkpoint_data$missing_objects <- missing_objects
  
  # Calculate approximate size of data to be saved
  approx_size_mb <- utils::object.size(checkpoint_data) / (1024^2)
  cat("Approximate checkpoint size:", round(approx_size_mb, 2), "MB\n")
  
  # Save the checkpoint
  tryCatch({
    saveRDS(checkpoint_data, filename)
    cat("Checkpoint saved:", filename, "\n")
    return(TRUE)
  }, error = function(e) {
    warning("Failed to save checkpoint: ", conditionMessage(e))
    
    # Check for disk space issues
    if(grepl("cannot open|No space left", conditionMessage(e))) {
      df <- system("df -h .", intern = TRUE)
      cat("Disk space information:\n", paste(df, collapse = "\n"), "\n")
    }
    
    return(FALSE)
  })
}

# Load a checkpoint
load_checkpoint <- function(checkpoint_name, confirm = TRUE) {
  # Validate input
  if(!is.character(checkpoint_name) || length(checkpoint_name) != 1) {
    warning("Invalid checkpoint name. Must be a single character string.")
    return(FALSE)
  }
  
  # Construct the filename
  filename <- file.path(checkpoint_dir, paste0(checkpoint_name, ".rds"))
  
  # Check if file exists
  if(!file.exists(filename)) {
    cat("Checkpoint file not found:", filename, "\n")
    available <- list.files(checkpoint_dir, pattern = "\\.rds$")
    if(length(available) > 0) {
      cat("Available checkpoints:", paste(gsub("\\.rds$", "", available), collapse = ", "), "\n")
    }
    return(FALSE)
  }
  
  # Load the checkpoint data
  tryCatch({
    checkpoint_data <- readRDS(filename)
    
    # Verify that loaded checkpoint has the expected structure
    if(!is.list(checkpoint_data) || 
       !all(c("timestamp", "object_names") %in% names(checkpoint_data))) {
      warning("Invalid checkpoint file: missing required metadata.")
      return(FALSE)
    }
    
    # Calculate checkpoint file size
    filesize_mb <- round(file.info(filename)$size / (1024^2), 2)
    
    # Display checkpoint info
    cat("Checkpoint:", checkpoint_name, "\n")
    cat("Created:", format(checkpoint_data$timestamp), "\n")
    cat("File size:", filesize_mb, "MB\n")
    cat("Contains", length(checkpoint_data$object_names), "objects:", 
        paste(checkpoint_data$object_names, collapse = ", "), "\n")
    
    # Show missing objects if any
    if("missing_objects" %in% names(checkpoint_data) && 
       length(checkpoint_data$missing_objects) > 0) {
      cat("Note: The following objects were missing when this checkpoint was created:\n",
          paste(checkpoint_data$missing_objects, collapse = ", "), "\n")
    }
    
    # Confirm before loading
    if(confirm) {
      response <- readline("Load this checkpoint? (y/n): ")
      if(tolower(response) != "y") {
        cat("Checkpoint loading cancelled.\n")
        return(FALSE)
      }
    }
    
    # Check for potential conflicts
    conflicts <- character(0)
    for(obj_name in checkpoint_data$object_names) {
      if(exists(obj_name, envir = .GlobalEnv)) {
        conflicts <- c(conflicts, obj_name)
      }
    }
    
    if(length(conflicts) > 0) {
      cat("Warning: The following objects already exist in your environment and will be overwritten:\n",
          paste(conflicts, collapse = ", "), "\n")
      
      if(confirm) {
        response <- readline("Continue and overwrite these objects? (y/n): ")
        if(tolower(response) != "y") {
          cat("Checkpoint loading cancelled.\n")
          return(FALSE)
        }
      }
    }
    
    # Load objects into global environment
    loaded_objects <- character(0)
    for(obj_name in checkpoint_data$object_names) {
      if(obj_name %in% names(checkpoint_data)) {
        tryCatch({
          assign(obj_name, checkpoint_data[[obj_name]], envir = .GlobalEnv)
          loaded_objects <- c(loaded_objects, obj_name)
        }, error = function(e) {
          warning("Error loading object '", obj_name, "': ", conditionMessage(e))
        })
      }
    }
    
    # Report outcome
    if(length(loaded_objects) == 0) {
      warning("No objects were loaded from the checkpoint.")
      return(FALSE)
    } else if(length(loaded_objects) < length(checkpoint_data$object_names)) {
      cat("Partially loaded checkpoint. Loaded", length(loaded_objects), "of", 
          length(checkpoint_data$object_names), "objects.\n")
    } else {
      cat("Checkpoint loaded successfully!\n")
    }
    
    return(TRUE)
  }, error = function(e) {
    warning("Error loading checkpoint: ", conditionMessage(e))
    
    # Check file corruption
    if(grepl("unexpected|corrupt|read error", conditionMessage(e))) {
      cat("The checkpoint file appears to be corrupted or unreadable.\n")
    }
    
    return(FALSE)
  })
}

# List all available checkpoints
list_checkpoints <- function() {
  # Check if checkpoint directory exists
  if(!dir.exists(checkpoint_dir)) {
    cat("No checkpoints directory found.\n")
    return(NULL)
  }
  
  # Get list of checkpoint files
  tryCatch({
    checkpoint_files <- list.files(checkpoint_dir, pattern = "\\.rds$")
    
    if(length(checkpoint_files) == 0) {
      cat("No checkpoints found in", checkpoint_dir, "\n")
      return(NULL)
    }
    
    # Create info dataframe
    checkpoint_info <- data.frame(
      checkpoint = gsub("\\.rds$", "", checkpoint_files),
      file = checkpoint_files,
      timestamp = NA,
      size_mb = NA,
      objects_count = NA,
      objects = NA,
      status = NA,
      stringsAsFactors = FALSE
    )
    
    # Loop through files and extract info
    for(i in 1:nrow(checkpoint_info)) {
      filename <- file.path(checkpoint_dir, checkpoint_info$file[i])
      
      # Get file size
      file_info <- file.info(filename)
      checkpoint_info$size_mb[i] <- round(file_info$size / (1024^2), 2)
      
      # Try to read checkpoint data
      tryCatch({
        checkpoint_data <- readRDS(filename)
        
        # Basic validation of checkpoint structure
        if(!is.list(checkpoint_data) || 
           !all(c("timestamp", "object_names") %in% names(checkpoint_data))) {
          checkpoint_info$timestamp[i] <- as.character(file_info$mtime)
          checkpoint_info$objects_count[i] <- NA
          checkpoint_info$objects[i] <- NA
          checkpoint_info$status[i] <- "Invalid format"
        } else {
          # Extract metadata
          checkpoint_info$timestamp[i] <- as.character(checkpoint_data$timestamp)
          checkpoint_info$objects_count[i] <- length(checkpoint_data$object_names)
          checkpoint_info$objects[i] <- paste(checkpoint_data$object_names, collapse = ", ")
          checkpoint_info$status[i] <- "Valid"
          
          # Check for missing objects
          if("missing_objects" %in% names(checkpoint_data) && 
             length(checkpoint_data$missing_objects) > 0) {
            checkpoint_info$status[i] <- paste0("Valid (", 
                                             length(checkpoint_data$missing_objects), 
                                             " missing objects)")
          }
        }
      }, error = function(e) {
        checkpoint_info$timestamp[i] <- as.character(file_info$mtime)
        checkpoint_info$objects_count[i] <- NA
        checkpoint_info$objects[i] <- "Unknown"
        checkpoint_info$status[i] <- paste0("Error: ", conditionMessage(e))
      })
    }
    
    # Sort by timestamp (most recent first)
    checkpoint_info <- checkpoint_info[order(checkpoint_info$timestamp, decreasing = TRUE), ]
    
    return(checkpoint_info)
  }, error = function(e) {
    warning("Error listing checkpoints: ", conditionMessage(e))
    return(NULL)
  })
}
```

# Setup

```{r load-packages}
# Check and load all required and optional packages
package_status <- check_and_load_packages()

# Display information about available packages
cat("Required packages loaded:", length(package_status$required), "\n")
if (length(package_status$optional) > 0) {
  cat("Optional packages loaded:", paste(package_status$optional, collapse = ", "), "\n")
}
if (length(package_status$missing) > 0) {
  cat("Optional packages not available:", paste(package_status$missing, collapse = ", "), "\n")
  cat("Some advanced features may be disabled.\n")
}

# Check if phylogenetic tree building is available
can_build_trees <- all(c("phangorn", "DECIPHER") %in% package_status$optional)
if (can_build_trees) {
  cat("✓ Phylogenetic tree construction is available\n")
} else {
  cat("✗ Phylogenetic tree construction is disabled (install phangorn and DECIPHER to enable)\n")
}

# Set up parallelization for faster processing
# Automatically detect optimal number of workers based on system
num_cores <- parallel::detectCores()
optimal_workers <- min(num_cores - 1, 8)  # Keep at least 1 core free for system

# Use progressive parallelization strategy
if (num_cores >= 16) {
  # For high-performance systems, use multisession with worker reservation
  cat(sprintf("Detected %d cores. Using multisession with %d workers.\n", num_cores, optimal_workers))
  plan(multisession, workers = optimal_workers)
} else if (num_cores >= 4) {
  # For mid-range systems, use multisession with fewer workers
  workers_to_use <- min(num_cores - 1, 4)
  cat(sprintf("Detected %d cores. Using multisession with %d workers.\n", num_cores, workers_to_use))
  plan(multisession, workers = workers_to_use)
} else {
  # For low-end systems, use minimal parallelization
  cat(sprintf("Detected only %d cores. Using minimal parallelization.\n", num_cores))
  plan(multisession, workers = 2)
}

# Configure memory usage for parallel operations
options(future.globals.maxSize = 4 * 1024^3)  # 4GB limit for globals - increased for larger datasets

# Set the random seed for reproducibility
set.seed(100)

# Check for existing checkpoints and allow recovery
tryCatch({
  if(dir.exists("checkpoints")) {
    cat("Checking for available checkpoints...\n")
    checkpoint_table <- list_checkpoints()
    
    if(!is.null(checkpoint_table) && nrow(checkpoint_table) > 0) {
      # Display checkpoint information in a readable format
      display_table <- checkpoint_table[, c("checkpoint", "timestamp", "size_mb", "objects_count", "status")]
      colnames(display_table) <- c("Checkpoint", "Created", "Size (MB)", "Objects", "Status")
      
      # Print formatted table
      cat("Available checkpoints:\n")
      print(display_table)
      
      # Prompt user to restore from checkpoint
      cat("\nWould you like to restore from a checkpoint? If so, enter the checkpoint name, or press Enter to start fresh: ")
      checkpoint_to_load <- readline()
      
      if(checkpoint_to_load != "") {
        # Check if entered checkpoint exists
        if(checkpoint_to_load %in% checkpoint_table$checkpoint) {
          # Try to load the specified checkpoint
          load_result <- load_checkpoint(checkpoint_to_load, confirm = TRUE)
          
          if(load_result) {
            cat("Workflow will continue from checkpoint", checkpoint_to_load, "\n")
          } else {
            cat("Starting fresh workflow run\n")
          }
        } else {
          cat("Checkpoint '", checkpoint_to_load, "' not found. Available checkpoints are:\n", 
              paste(checkpoint_table$checkpoint, collapse = ", "), "\n")
          cat("Starting fresh workflow run\n")
        }
      } else {
        cat("Starting fresh workflow run\n")
      }
    } else {
      cat("No valid checkpoints found. Starting fresh workflow run.\n")
    }
  } else {
    cat("No checkpoint directory found. Starting fresh workflow run.\n")
  }
}, error = function(e) {
  warning("Error checking for checkpoints: ", conditionMessage(e))
  cat("Starting fresh workflow run due to error in checkpoint system.\n")
})
```

# Set Working Directory and Configure for Single or Multi-Run Processing

```{r run-configuration}
# Check for multi-run mode specified in parameters
is_multi_run <- params$multi_run
is_big_data <- params$big_data

if(is_multi_run) {
  cat("Multi-run processing mode enabled\n")
  
  # Check if run directory is specified
  if(!is.null(params$run_dir) && nchar(params$run_dir) > 0) {
    root_path <- params$run_dir
    cat("Using specified run directory:", root_path, "\n")
  } else {
    # Default to data directory with subdirectories for runs
    root_path <- "data"
    cat("Using default data directory for multi-run processing\n")
  }
  
  # Check if the run directory exists
  if(!dir.exists(root_path)) {
    stop("Run directory does not exist: ", root_path)
  }
  
  # Look for run subdirectories
  run_dirs <- list.dirs(root_path, full.names = TRUE, recursive = FALSE)
  
  # Filter to only include directories that have fastq files
  run_dirs <- run_dirs[sapply(run_dirs, function(dir) {
    length(list.files(dir, pattern = "\\.(fastq|fq)(\\.gz)?$", recursive = TRUE)) > 0
  })]
  
  if(length(run_dirs) == 0) {
    cat("No run directories with fastq files found in", root_path, "\n")
    cat("Falling back to single-run mode with data directory\n")
    is_multi_run <- FALSE
    path <- "data"
  } else {
    cat("Found", length(run_dirs), "run directories with fastq files:\n")
    cat(paste(" -", basename(run_dirs)), sep = "\n")
  }
} else {
  # Single run mode
  cat("Single-run processing mode enabled\n")
  path <- "data"
}

# For big data mode, we'll use more aggressive memory management
if(is_big_data) {
  cat("Big data mode enabled - optimizing for memory efficiency\n")
}

# Create a function to handle file finding for both single and multi-run modes
find_fastq_files <- function(search_path, recursive = FALSE) {
  # Common function to handle various file naming patterns
  
  # Try standard Illumina naming pattern first (_R1_001.fastq.gz)
  fnFs.illumina <- sort(list.files(search_path, pattern="_R1_001\\.fastq\\.gz$", 
                                  full.names = TRUE, recursive = recursive))
  fnRs.illumina <- sort(list.files(search_path, pattern="_R2_001\\.fastq\\.gz$", 
                                  full.names = TRUE, recursive = recursive))
  
  # Try alternative naming patterns (_R1.fastq.gz)
  fnFs.alt1 <- sort(list.files(search_path, pattern="_R1\\.fastq\\.gz$", 
                              full.names = TRUE, recursive = recursive))
  fnRs.alt1 <- sort(list.files(search_path, pattern="_R2\\.fastq\\.gz$", 
                              full.names = TRUE, recursive = recursive))
  
  # Try other naming patterns (forward/reverse in the name)
  fnFs.alt2 <- sort(list.files(search_path, pattern="[_\\.]F[_\\.]|[_\\.]forward[_\\.]", 
                              full.names = TRUE, ignore.case = TRUE, recursive = recursive))
  fnRs.alt2 <- sort(list.files(search_path, pattern="[_\\.]R[_\\.]|[_\\.]reverse[_\\.]", 
                              full.names = TRUE, ignore.case = TRUE, recursive = recursive))
  
  # Check naming pattern and use the appropriate files
  if (length(fnFs.illumina) > 0 && length(fnRs.illumina) > 0) {
    # Use Illumina naming pattern
    fnFs <- fnFs.illumina
    fnRs <- fnRs.illumina
    cat("Using Illumina naming pattern (_R1_001.fastq.gz)\n")
    file_pattern <- "_R\\d_001\\.fastq\\.gz$"
  } else if (length(fnFs.alt1) > 0 && length(fnRs.alt1) > 0) {
    # Use alternative naming pattern
    fnFs <- fnFs.alt1
    fnRs <- fnRs.alt1
    cat("Using alternative naming pattern (_R1.fastq.gz)\n")
    file_pattern <- "_R\\d\\.fastq\\.gz$"
  } else if (length(fnFs.alt2) > 0 && length(fnRs.alt2) > 0) {
    # Use other alternative naming pattern
    fnFs <- fnFs.alt2
    fnRs <- fnRs.alt2
    cat("Using alternative naming pattern with forward/reverse in the name\n")
    file_pattern <- "\\.(fastq|fq)(\\.gz)?$"
  } else {
    # Look for any fastq files
    all_fastqs <- sort(list.files(search_path, pattern="\\.(fastq|fq)(\\.gz)?$", 
                                 full.names = TRUE, recursive = recursive))
    if (length(all_fastqs) > 0) {
      cat("WARNING: Found", length(all_fastqs), "fastq files but couldn't determine read direction.\n")
      cat("Please ensure your files follow naming conventions with _R1/_R2 or similar pattern.\n")
    }
    return(NULL)  # Return NULL to indicate no valid files found
  }
  
  # Return both file lists and the pattern
  return(list(fnFs = fnFs, fnRs = fnRs, pattern = file_pattern))
}
```

```{r locate-files}
# Function to extract sample names from filenames that works with multiple naming conventions
extract_sample_names <- function(file_paths, pattern, run_names = NULL) {
  sample_names <- basename(file_paths)
  
  # First handle the standard Illumina pattern
  if (grepl("_R\\d_001\\.fastq\\.gz$", pattern)) {
    sample_names <- gsub("_R\\d_001\\.fastq\\.gz$", "", sample_names)
  } 
  # Then handle the simplified R1/R2 pattern
  else if (grepl("_R\\d\\.fastq\\.gz$", pattern)) {
    sample_names <- gsub("_R\\d\\.fastq\\.gz$", "", sample_names)
  }
  # General case: remove file extension and the "R1"/"R2"/"forward"/"reverse" pattern
  else {
    # Remove fastq/fq file extension
    sample_names <- gsub("\\.(fastq|fq)(\\.gz)?$", "", sample_names)
    # Remove R1/R2 or forward/reverse pattern
    sample_names <- gsub("_R[12]_?|_?[FR]_?|_forward_?|_reverse_?", "", sample_names, ignore.case = TRUE)
  }
  
  # Remove trailing underscores if present
  sample_names <- gsub("_+$", "", sample_names)
  
  # If we have run names and are in multi-run mode, prefix with run name
  if(!is.null(run_names)) {
    # Extract the run name for each file by matching its path against the run directories
    file_runs <- sapply(file_paths, function(fp) {
      # Find which run directory this file belongs to
      run_idx <- which(sapply(names(run_names), function(r) grepl(r, fp, fixed = TRUE)))
      if(length(run_idx) > 0) {
        return(run_names[run_idx[1]])  # Return the run name
      } else {
        return("unknown")  # Fallback
      }
    })
    
    # Prefix sample names with run names (if in multi-run mode)
    if(is_multi_run) {
      sample_names <- paste(file_runs, sample_names, sep = "_")
    }
    
    # Store the run information (useful for multi-run handling)
    attr(sample_names, "runs") <- file_runs
  }
  
  return(sample_names)
}

# Detect primers from filenames
detect_primers <- function(file_names) {
  # Common 16S rRNA primers to check for
  primer_patterns <- c(
    "515F-806R" = "515f.*806r|806r.*515f",
    "341F-805R" = "341f.*805r|805r.*341f",
    "515F-926R" = "515f.*926r|926r.*515f",
    "27F-338R" = "27f.*338r|338r.*27f",
    "V4" = "v4",
    "V3-V4" = "v3.*v4|v3-v4",
    "V1-V2" = "v1.*v2|v1-v2",
    "806R" = "806r"
  )
  
  # Check for primer patterns in filenames
  all_filenames <- paste(file_names, collapse = " ")
  matches <- sapply(primer_patterns, function(pattern) {
    grepl(pattern, all_filenames, ignore.case = TRUE)
  })
  
  if (any(matches)) {
    primers <- names(which(matches))
    cat("Detected primers/regions:", paste(primers, collapse = ", "), "\n")
    return(primers[1])  # Return the first match
  } else {
    cat("No specific primers detected from filenames\n")
    return(NULL)
  }
}

# Map primer to expected amplicon length
get_expected_amplicon_size <- function(primer) {
  if (is.null(primer)) return(250)  # Default if unknown
  
  # Approximate amplicon sizes for common primer sets
  amplicon_sizes <- c(
    "515F-806R" = 253,  # V4 region
    "341F-805R" = 464,  # V3-V4 region
    "515F-926R" = 411,  # V4-V5 region
    "27F-338R" = 311,   # V1-V2 region
    "V4" = 253,
    "V3-V4" = 464,
    "V1-V2" = 311,
    "806R" = 253  # Assuming 515F-806R when only 806R is mentioned
  )
  
  # Display informative message about the primers and amplicon size
  if (primer %in% names(amplicon_sizes)) {
    size <- amplicon_sizes[primer]
    
    # For common primer sets, provide additional information
    if (primer %in% c("515F-806R", "806R", "V4")) {
      cat("Using the 515F-806R primer set (V4 region). Expected amplicon size:", size, "bp\n")
      cat("Note: While MiSeq runs 2x250 cycles, the ~253 bp amplicon size is important for\n")
      cat("      proper read merging and quality filtering parameter optimization.\n")
    } else if (primer %in% c("341F-805R", "V3-V4")) {
      cat("Using the 341F-805R primer set (V3-V4 region). Expected amplicon size:", size, "bp\n")
      cat("Note: This longer amplicon requires high-quality overlapping regions for proper merging.\n")
    } else {
      cat("Using", primer, "primers. Expected amplicon size:", size, "bp\n")
    }
    
    return(size)
  } else {
    cat("Unknown primer set. Using default amplicon size of 250 bp\n")
    cat("Warning: Parameter optimization will be less precise without\n")
    cat("         accurate amplicon size information.\n")
    return(250)  # Default if not in the list
  }
}

# Process files differently based on single or multi-run mode
if(is_multi_run) {
  # Initialize lists to store file information for each run
  all_fnFs <- list()
  all_fnRs <- list()
  all_sample_names <- list()
  run_names <- character()
  
  # Process each run directory
  for(i in seq_along(run_dirs)) {
    run_dir <- run_dirs[i]
    run_name <- basename(run_dir)
    cat("\nProcessing run directory:", run_name, "\n")
    
    # Find fastq files for this run
    files <- find_fastq_files(run_dir)
    
    if(is.null(files)) {
      cat("No valid fastq files found in", run_dir, "- skipping\n")
      next
    }
    
    # Store the files for this run
    all_fnFs[[run_name]] <- files$fnFs
    all_fnRs[[run_name]] <- files$fnRs
    
    # Extract sample names for this run
    run_sample_names <- extract_sample_names(files$fnFs, files$pattern)
    all_sample_names[[run_name]] <- run_sample_names
    
    # Store the run name
    run_names[run_name] <- run_name
    
    cat("Found", length(files$fnFs), "samples in run", run_name, "\n")
  }
  
  # Check if we found any valid runs
  if(length(all_fnFs) == 0) {
    stop("No valid fastq files found in any run directories")
  }
  
  # Combine all runs into single vectors
  fnFs <- unlist(all_fnFs, use.names = FALSE)
  fnRs <- unlist(all_fnRs, use.names = FALSE)
  
  # Get run for each file for later use in naming
  file_to_run <- rep(names(all_fnFs), sapply(all_fnFs, length))
  names(file_to_run) <- NULL  # Clean names to avoid confusion
  
  # Extract sample names with run prefixes
  sample.names <- extract_sample_names(fnFs, "combined", run_names = file_to_run)
  
  cat("\nCombined all runs:", length(fnFs), "total samples from", length(all_fnFs), "runs\n")
} else {
  # Single run mode
  cat("\nProcessing in single-run mode\n")
  
  # Find fastq files
  files <- find_fastq_files(path)
  
  if(is.null(files)) {
    stop("No valid fastq files found in data directory. Please check your file paths and naming conventions.")
  }
  
  fnFs <- files$fnFs
  fnRs <- files$fnRs
  file_pattern <- files$pattern
  
  # Extract sample names
  sample.names <- extract_sample_names(fnFs, file_pattern)
  
  cat("Found", length(fnFs), "samples\n")
}

# Verify sample names were extracted properly
sample.df <- data.frame(
  ForwardFile = basename(fnFs),
  ReverseFile = basename(fnRs),
  SampleName = sample.names
)
kable(head(sample.df, min(10, nrow(sample.df))), caption = "Sample Names Extracted from Files")

# Verify forward and reverse files match
if (length(fnFs) != length(fnRs)) {
  stop("Number of forward and reverse files don't match. Check your files.")
}

# Get primer information based on combined files if in multi-run mode
primer_info <- detect_primers(c(fnFs, fnRs))
expected_amplicon_size <- get_expected_amplicon_size(primer_info)
cat("Expected amplicon size:", expected_amplicon_size, "bp\n")

# Create run information table if in multi-run mode
if(is_multi_run && exists("file_to_run")) {
  run_table <- table(file_to_run)
  run_df <- data.frame(
    Run = names(run_table),
    Samples = as.numeric(run_table)
  )
  kable(run_df, caption = "Sample Counts by Run")
}
```

# Initial Quality Assessment

First, we'll examine the quality profiles of the raw reads to get a baseline understanding of the data quality.

```{r quality-plots}
# Plot quality profiles for a few forward reads
forward_qual <- plotQualityProfile(fnFs[1:min(3, length(fnFs))])
# Plot quality profiles for a few reverse reads
reverse_qual <- plotQualityProfile(fnRs[1:min(3, length(fnRs))])

# Display the quality plots
print(forward_qual)
print(reverse_qual)
```

# Automatic Parameter Optimization

Based on the quality profiles, we'll optimize key parameters like truncation lengths and error thresholds to maximize both data quality and sequence retention.

```{r parameter-optimization}
# Function to determine optimal truncation lengths based on quality profiles
optimize_truncation_lengths <- function(forward_files, reverse_files, quality_threshold = 30,
                                        min_overlap = 20, target_amplicon_size = NULL) {
  # Sample a subset of files for efficiency (max 5)
  n_sample <- min(5, length(forward_files))
  sample_idx <- sample(1:length(forward_files), n_sample)
  
  sample_forwards <- forward_files[sample_idx]
  sample_reverses <- reverse_files[sample_idx]
  
  # Generate quality profiles
  forward_qual <- plotQualityProfile(sample_forwards, aggregate = TRUE)
  reverse_qual <- plotQualityProfile(sample_reverses, aggregate = TRUE)
  
  # Extract quality data
  forward_data <- forward_qual$data
  reverse_data <- reverse_qual$data
  
  # Detect maximum read length from data
  max_forward_cycle <- max(forward_data$Cycle)
  max_reverse_cycle <- max(reverse_data$Cycle)
  
  cat("Detected read lengths: Forward =", max_forward_cycle, "bp, Reverse =", max_reverse_cycle, "bp\n")
  
  # Determine platform type based on read length
  platform <- "Unknown"
  if (max_forward_cycle >= 250 && max_reverse_cycle >= 250) {
    platform <- "MiSeq/HiSeq (2x250 or longer)"
  } else if (max_forward_cycle >= 150 && max_reverse_cycle >= 150) {
    platform <- "MiniSeq/NextSeq (2x150)"
  } else if (max_forward_cycle >= 100 && max_reverse_cycle >= 100) {
    platform <- "NovaSeq/NextSeq (2x100)"
  }
  cat("Likely sequencing platform:", platform, "\n")
  
  # Calculate mean quality by position
  forward_mean_qual <- aggregate(Score ~ Cycle, data = forward_data, FUN = mean)
  reverse_mean_qual <- aggregate(Score ~ Cycle, data = reverse_data, FUN = mean)
  
  # Find position where quality drops below threshold
  quality_drop_f <- which(forward_mean_qual$Score < quality_threshold)
  quality_drop_r <- which(reverse_mean_qual$Score < quality_threshold)
  
  # Determine initial truncation points
  if (length(quality_drop_f) > 0) {
    forward_trunc <- max(quality_drop_f[1] - 10, floor(max_forward_cycle * 0.5))
  } else {
    forward_trunc <- max_forward_cycle  # Use full length if quality remains high
  }
  
  if (length(quality_drop_r) > 0) {
    reverse_trunc <- max(quality_drop_r[1] - 10, floor(max_reverse_cycle * 0.5))
  } else {
    reverse_trunc <- max_reverse_cycle  # Use full length if quality remains high
  }
  
  # Safety checks to ensure quality is maintained
  # Find the last position where quality is good (Q25+)
  good_quality_f <- max(which(forward_mean_qual$Score >= 25))
  good_quality_r <- max(which(reverse_mean_qual$Score >= 25))
  
  # Limit truncation to positions with good quality
  forward_trunc <- min(forward_trunc, good_quality_f)
  reverse_trunc <- min(reverse_trunc, good_quality_r)
  
  # Account for expected amplicon size if provided
  if (!is.null(target_amplicon_size)) {
    # Adjust truncation to ensure enough overlap for merging
    min_read_sum <- target_amplicon_size + min_overlap
    
    if (forward_trunc + reverse_trunc < min_read_sum) {
      # Need to adjust truncation to ensure sufficient bp for assembly
      cat("Warning: Detected quality-based truncation might not preserve enough sequence for assembly\n")
      cat("Target amplicon size:", target_amplicon_size, "bp, Minimum required read sum:", min_read_sum, "bp\n")
      
      # Prefer keeping higher quality read at full length
      if (mean(forward_mean_qual$Score) > mean(reverse_mean_qual$Score)) {
        # Keep forward at max quality-based truncation, extend reverse as needed
        forward_trunc <- min(forward_trunc, max_forward_cycle)
        reverse_trunc <- max(min_read_sum - forward_trunc, 0.7 * max_reverse_cycle)
      } else {
        # Keep reverse at max quality-based truncation, extend forward as needed
        reverse_trunc <- min(reverse_trunc, max_reverse_cycle)
        forward_trunc <- max(min_read_sum - reverse_trunc, 0.7 * max_forward_cycle)
      }
    }
  }
  
  # Ensure truncation doesn't exceed read length
  forward_trunc <- min(forward_trunc, max_forward_cycle)
  reverse_trunc <- min(reverse_trunc, max_reverse_cycle)
  
  # Round to integers
  forward_trunc <- as.integer(forward_trunc)
  reverse_trunc <- as.integer(reverse_trunc)
  
  # Create plots with platform-specific recommendations
  forward_plot <- ggplot(forward_mean_qual, aes(x = Cycle, y = Score)) +
    geom_line(size = 1) +
    geom_hline(yintercept = quality_threshold, linetype = "dashed", color = "red") +
    geom_vline(xintercept = forward_trunc, linetype = "dashed", color = "blue") +
    scale_y_continuous(limits = c(0, 40)) +
    labs(title = paste("Forward Read Quality -", platform), 
         subtitle = paste("Suggested truncation at position", forward_trunc),
         x = "Cycle", y = "Quality Score") +
    theme_minimal()
  
  reverse_plot <- ggplot(reverse_mean_qual, aes(x = Cycle, y = Score)) +
    geom_line(size = 1) +
    geom_hline(yintercept = quality_threshold, linetype = "dashed", color = "red") +
    geom_vline(xintercept = reverse_trunc, linetype = "dashed", color = "blue") +
    scale_y_continuous(limits = c(0, 40)) +
    labs(title = paste("Reverse Read Quality -", platform), 
         subtitle = paste("Suggested truncation at position", reverse_trunc),
         x = "Cycle", y = "Quality Score") +
    theme_minimal()
  
  # Display plots together
  grid.arrange(forward_plot, reverse_plot, ncol = 2)
  
  # Estimate expected overlap after trimming
  expected_overlap <- forward_trunc + reverse_trunc - target_amplicon_size
  cat("Expected overlap after trimming:", expected_overlap, "bp\n")
  if (expected_overlap < min_overlap) {
    cat("WARNING: Expected overlap is less than recommended minimum (", min_overlap, "bp)\n")
    cat("Consider adjusting truncation lengths or using a more relaxed merging criteria\n")
  } else {
    cat("Overlap is sufficient for reliable read merging\n")
  }
  
  # Return recommendations with platform info
  return(list(
    forward = forward_trunc, 
    reverse = reverse_trunc,
    platform = platform,
    max_forward_len = max_forward_cycle,
    max_reverse_len = max_reverse_cycle,
    expected_overlap = expected_overlap
  ))
}

# Function to optimize expected error thresholds
optimize_maxEE <- function(trunc_forward, trunc_reverse, forward_files, reverse_files, 
                          sample_names, ee_range = c(1, 2, 3, 4)) {
  # Make a temporary directory for testing
  temp_dir <- file.path(path, "temp_filter")
  if(!dir.exists(temp_dir)) dir.create(temp_dir)
  
  # Use a small subset of files to test parameters efficiently
  n_test <- min(3, length(forward_files))
  test_fnFs <- forward_files[1:n_test]
  test_fnRs <- reverse_files[1:n_test]
  test_names <- sample_names[1:n_test]
  
  # Generate test file paths
  test_filtFs <- file.path(temp_dir, paste0(test_names, "_F_filt.fastq.gz"))
  test_filtRs <- file.path(temp_dir, paste0(test_names, "_R_filt.fastq.gz"))
  names(test_filtFs) <- test_names
  names(test_filtRs) <- test_names
  
  # Track results for each maxEE value
  results <- data.frame(maxEE_F = numeric(), 
                        maxEE_R = numeric(),
                        reads_in = numeric(),
                        reads_kept = numeric(),
                        percent_kept = numeric())
  
  # Generate combinations of expected error thresholds to test
  ee_combinations <- expand.grid(maxEE_F = ee_range, maxEE_R = ee_range)
  
  # Filter out combinations where reverse EE is lower than forward
  # (since reverse reads typically have lower quality)
  ee_combinations <- ee_combinations[ee_combinations$maxEE_R >= ee_combinations$maxEE_F, ]
  
  # Test different expected error thresholds
  for (i in 1:nrow(ee_combinations)) {
    f_ee <- ee_combinations$maxEE_F[i]
    r_ee <- ee_combinations$maxEE_R[i]
    
    # Filter and track results
    test_out <- suppressWarnings(
      filterAndTrim(test_fnFs, test_filtFs, test_fnRs, test_filtRs,
                   truncLen = c(trunc_forward, trunc_reverse),
                   maxN = 0, maxEE = c(f_ee, r_ee), truncQ = 2,
                   rm.phix = TRUE, compress = TRUE, multithread = TRUE)
    )
    
    # Calculate statistics
    reads_in <- sum(test_out[, 1])
    reads_kept <- sum(test_out[, 2])
    percent_kept <- round(reads_kept / reads_in * 100, 1)
    
    # Store results
    results <- rbind(results, data.frame(
      maxEE_F = f_ee,
      maxEE_R = r_ee,
      reads_in = reads_in,
      reads_kept = reads_kept,
      percent_kept = percent_kept
    ))
  }
  
  # Clean up temporary files
  unlink(temp_dir, recursive = TRUE)
  
  # Find optimal parameters (target ~80% retention while being stringent)
  # Sort by percent kept descending
  results <- results[order(results$percent_kept, decreasing = TRUE), ]
  
  # Create a label column for the plot
  results$label <- paste0(results$maxEE_F, "-", results$maxEE_R)
  
  # Make the plot
  ee_plot <- ggplot(results, aes(x = reorder(label, percent_kept), y = percent_kept)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(percent_kept, "%")), vjust = -0.3, size = 3) +
    labs(title = "Read Retention by Expected Error Parameters",
         x = "maxEE (Forward-Reverse)",
         y = "Percent Reads Kept") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(ee_plot)
  
  # Determine optimal values
  # Look for >70% retention while using lowest possible maxEE values
  optimal <- results[results$percent_kept > 70, ]
  if (nrow(optimal) > 0) {
    # Sort by forward then reverse EE to get most stringent option
    optimal <- optimal[order(optimal$maxEE_F, optimal$maxEE_R), ]
    optimal_maxEE <- c(optimal$maxEE_F[1], optimal$maxEE_R[1])
    optimal_pct <- optimal$percent_kept[1]
  } else {
    # If retention is too low with 70% threshold, drop to 50%
    optimal <- results[results$percent_kept > 50, ]
    if (nrow(optimal) > 0) {
      optimal <- optimal[order(optimal$maxEE_F, optimal$maxEE_R), ]
      optimal_maxEE <- c(optimal$maxEE_F[1], optimal$maxEE_R[1])
      optimal_pct <- optimal$percent_kept[1]
      cat("Warning: Using relaxed retention threshold (50%)\n")
    } else {
      # If all else fails, use the combination with highest retention
      optimal_maxEE <- c(results$maxEE_F[1], results$maxEE_R[1])
      optimal_pct <- results$percent_kept[1]
      cat("Warning: All tested parameters had low retention (<50%)\n")
    }
  }
  
  cat("Optimal maxEE parameters:", optimal_maxEE[1], "(forward),", 
      optimal_maxEE[2], "(reverse) with", optimal_pct, "% read retention\n")
  
  return(list(
    maxEE = optimal_maxEE,
    percent_kept = optimal_pct,
    all_results = results
  ))
}

# Run truncation length optimization
cat("Optimizing truncation lengths...\n")
truncation_lengths <- optimize_truncation_lengths(
  fnFs, fnRs, 
  quality_threshold = 25, 
  min_overlap = 20, 
  target_amplicon_size = expected_amplicon_size
)

# Run expected error threshold optimization
cat("Optimizing expected error thresholds...\n")
maxEE_params <- optimize_maxEE(
  truncation_lengths$forward, 
  truncation_lengths$reverse, 
  fnFs, fnRs, 
  sample.names
)

# Create a parameter summary table
param_summary <- data.frame(
  Parameter = c("Platform", "Forward Read Length", "Reverse Read Length", 
                "Forward Truncation Length", "Reverse Truncation Length", 
                "Forward maxEE", "Reverse maxEE",
                "Expected Amplicon Size", "Expected Overlap"),
  Value = c(truncation_lengths$platform, 
            truncation_lengths$max_forward_len,
            truncation_lengths$max_reverse_len,
            truncation_lengths$forward, 
            truncation_lengths$reverse, 
            maxEE_params$maxEE[1], 
            maxEE_params$maxEE[2],
            expected_amplicon_size,
            truncation_lengths$expected_overlap),
  Notes = c("Detected sequencing platform",
            "Maximum cycle number in forward reads",
            "Maximum cycle number in reverse reads",
            "Position where quality drops below threshold",
            "Position where quality drops below threshold",
            "Maximum expected errors allowed in forward reads",
            "Maximum expected errors allowed in reverse reads",
            "Expected amplicon size based on primer",
            "Expected overlap after truncation")
)
kable(param_summary, caption = "Optimized Filtering Parameters")

# Set the optimized parameters for use in the workflow
truncLen_forward <- truncation_lengths$forward  
truncLen_reverse <- truncation_lengths$reverse
maxEE_forward <- maxEE_params$maxEE[1]
maxEE_reverse <- maxEE_params$maxEE[2]

# Allow parameter override (uncomment to override)
# truncLen_forward <- 240  # Uncomment to override
# truncLen_reverse <- 200  # Uncomment to override
# maxEE_forward <- 2       # Uncomment to override
# maxEE_reverse <- 2       # Uncomment to override

cat("Final parameters for filtering:\n")
cat("truncLen =", c(truncLen_forward, truncLen_reverse), "\n")
cat("maxEE =", c(maxEE_forward, maxEE_reverse), "\n")
```

# Filter and Trim

Now we apply the optimized filtering parameters to our dataset.

```{r filter-trim}
# Create directory for filtered files
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# Make sure the filtered directory exists
if(!dir.exists(file.path(path, "filtered"))) {
  dir.create(file.path(path, "filtered"))
}

# Filter and trim with optimized parameters and parallelization
cat("Filtering and trimming reads with optimized parameters and parallelization...\n")

# For large datasets, use batched filtering
if(length(fnFs) > 50) {
  cat("Using batched filtering for large dataset...\n")
  
  # Process in batches to better manage memory
  batch_size <- min(20, ceiling(length(fnFs)/4))
  cat(sprintf("Processing in batches of %d samples...\n", batch_size))
  
  # Create batches
  batches <- ceiling(seq_along(fnFs) / batch_size)
  
  # Initialize out matrix
  out <- matrix(0, nrow = length(sample.names), ncol = 2)
  rownames(out) <- sample.names
  colnames(out) <- c("reads.in", "reads.out")
  
  # Process each batch
  for(batch in unique(batches)) {
    cat(sprintf("Processing batch %d of %d...\n", batch, max(batches)))
    
    # Get indices for this batch
    idx <- which(batches == batch)
    
    # Filter this batch
    batch_out <- filterAndTrim(
      fnFs[idx], filtFs[idx], fnRs[idx], filtRs[idx],
      truncLen = c(truncLen_forward, truncLen_reverse), 
      maxN = 0, maxEE = c(maxEE_forward, maxEE_reverse), truncQ = 2,
      rm.phix = TRUE, compress = TRUE, multithread = TRUE
    )
    
    # Store results
    out[rownames(batch_out), ] <- batch_out
    
    # Force garbage collection
    gc()
  }
} else {
  # For smaller datasets, use standard approach
  out <- filterAndTrim(
    fnFs, filtFs, fnRs, filtRs,
    truncLen = c(truncLen_forward, truncLen_reverse), 
    maxN = 0, maxEE = c(maxEE_forward, maxEE_reverse), truncQ = 2,
    rm.phix = TRUE, compress = TRUE, multithread = TRUE
  )
}

# Create summary of filtering results
filter_summary <- as.data.frame(out)
filter_summary$SampleName <- rownames(filter_summary)
filter_summary$PercentRetained <- round(filter_summary$reads.out / filter_summary$reads.in * 100, 1)
filter_summary <- filter_summary[, c("SampleName", "reads.in", "reads.out", "PercentRetained")]
colnames(filter_summary) <- c("Sample", "Input Reads", "Filtered Reads", "% Retained")

# Create checkpoint after filter and trim step with additional tracking info
save_checkpoint("after_filter_trim", 
               c("fnFs", "fnRs", "filtFs", "filtRs", "out", "sample.names", 
                 "truncLen_forward", "truncLen_reverse", "maxEE_forward", "maxEE_reverse",
                 "truncation_lengths", "maxEE_params", "expected_amplicon_size",
                 "filter_summary", "is_multi_run"),
               overwrite = TRUE)
cat("Checkpoint saved after filter and trim step\n")

# Also save a separate checkpoint with just the filter summary for quick access
if (!dir.exists("results")) dir.create("results")
saveRDS(filter_summary, "results/filter_summary.rds")
cat("Filter summary saved to results/filter_summary.rds\n")

# View filtering statistics
kable(filter_summary, caption = "Filtering Statistics", row.names = FALSE)

# Plot filtering results
filter_plot <- ggplot(filter_summary, aes(x = reorder(Sample, -`Filtered Reads`), y = `Filtered Reads`)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = paste0(`% Retained`, "%")), vjust = -0.3, size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Reads Retained After Filtering", x = "Sample", y = "Filtered Reads")

print(filter_plot)

# Count number of samples that retained >50% of reads
samples_gt_50pct <- sum(filter_summary$`% Retained` > 50)
cat(samples_gt_50pct, "out of", nrow(filter_summary), "samples retained >50% of reads after filtering\n")

# Check if any samples have very low retention
low_retention_samples <- filter_summary[filter_summary$`% Retained` < 30, ]
if (nrow(low_retention_samples) > 0) {
  cat("WARNING:", nrow(low_retention_samples), "samples have very low read retention (<30%):\n")
  print(low_retention_samples)
}

# Check if any samples have zero reads after filtering
zero_samples <- filter_summary[filter_summary$`Filtered Reads` == 0, ]
if (nrow(zero_samples) > 0) {
  cat("WARNING:", nrow(zero_samples), "samples have zero reads after filtering and will be removed:\n")
  print(zero_samples)
  
  # Remove samples with zero reads after filtering
  samples_to_keep <- filter_summary$Sample[filter_summary$`Filtered Reads` > 0]
  filtFs <- filtFs[samples_to_keep]
  filtRs <- filtRs[samples_to_keep]
  sample.names <- sample.names[sample.names %in% samples_to_keep]
  cat("Continuing with", length(sample.names), "samples\n")
}
```

# Quality Profiles After Filtering

Let's check the quality profiles of the filtered reads to confirm our filtering parameters worked as expected.

```{r quality-filtered}
# Enhanced quality visualization for filtered reads
if (length(filtFs) > 0) {
  # Function to create enhanced quality profile visualization
  create_enhanced_quality_viz <- function(fastq_files, read_type = "Forward", aggregate = FALSE) {
    # Generate standard quality profile
    qual_plot <- plotQualityProfile(fastq_files, aggregate = aggregate)
    
    if (aggregate) {
      # Extract data for enhanced visualization
      plot_data <- qual_plot$data
      
      # Calculate mean, median, and percentile quality by position
      qual_stats <- plot_data %>%
        group_by(Cycle) %>%
        summarize(
          Mean = mean(Score),
          Median = median(Score),
          Q10 = quantile(Score, 0.1),
          Q25 = quantile(Score, 0.25),
          Q75 = quantile(Score, 0.75),
          Q90 = quantile(Score, 0.9)
        )
      
      # Create enhanced visualization
      enhanced_plot <- ggplot(qual_stats, aes(x = Cycle)) +
        # Add shaded regions for quality interpretation
        annotate("rect", xmin = -Inf, xmax = Inf, ymin = 0, ymax = 20, 
                 fill = "red", alpha = 0.2) +
        annotate("rect", xmin = -Inf, xmax = Inf, ymin = 20, ymax = 28, 
                 fill = "yellow", alpha = 0.2) +
        annotate("rect", xmin = -Inf, xmax = Inf, ymin = 28, ymax = Inf, 
                 fill = "green", alpha = 0.2) +
        # Add percentile ribbons
        geom_ribbon(aes(ymin = Q10, ymax = Q90), alpha = 0.2, fill = "grey50") +
        geom_ribbon(aes(ymin = Q25, ymax = Q75), alpha = 0.3, fill = "grey40") +
        # Add lines for mean and median
        geom_line(aes(y = Mean, color = "Mean"), size = 1.2) +
        geom_line(aes(y = Median, color = "Median"), size = 1.2, linetype = "dashed") +
        # Add quality thresholds
        geom_hline(yintercept = 30, linetype = "dashed", color = "blue") +
        geom_hline(yintercept = 20, linetype = "dashed", color = "red") +
        # Add trimming position indicators if available
        {if (read_type == "Forward" && exists("truncLen_forward")) 
          geom_vline(xintercept = truncLen_forward, color = "purple", size = 1, alpha = 0.7)} +
        {if (read_type == "Reverse" && exists("truncLen_reverse")) 
          geom_vline(xintercept = truncLen_reverse, color = "purple", size = 1, alpha = 0.7)} +
        # Labels and theme
        labs(
          title = paste(read_type, "Read Quality After Filtering"),
          subtitle = "With quality interpretation zones and percentile distribution",
          x = "Cycle (bp position)",
          y = "Quality Score",
          caption = "Red: Q<20 (poor) | Yellow: Q20-28 (acceptable) | Green: Q>28 (excellent)"
        ) +
        scale_color_manual(
          name = "Statistic", 
          values = c("Mean" = "blue", "Median" = "darkgreen")
        ) +
        scale_y_continuous(limits = c(0, 40)) +
        theme_minimal() +
        theme(
          legend.position = "bottom",
          plot.caption = element_text(hjust = 0.5),
          panel.grid.minor = element_blank()
        ) +
        # Add annotations
        annotate("text", x = 5, y = 38, label = "Excellent", color = "darkgreen", hjust = 0) +
        annotate("text", x = 5, y = 24, label = "Acceptable", color = "darkgoldenrod4", hjust = 0) +
        annotate("text", x = 5, y = 10, label = "Poor", color = "darkred", hjust = 0)
      
      # Add trimming point annotation
      if (read_type == "Forward" && exists("truncLen_forward")) {
        enhanced_plot <- enhanced_plot + 
          annotate("text", x = truncLen_forward, y = 35, 
                   label = paste("Trim position:", truncLen_forward), 
                   color = "purple", hjust = 1)
      } else if (read_type == "Reverse" && exists("truncLen_reverse")) {
        enhanced_plot <- enhanced_plot + 
          annotate("text", x = truncLen_reverse, y = 35, 
                   label = paste("Trim position:", truncLen_reverse), 
                   color = "purple", hjust = 1)
      }
      
      return(enhanced_plot)
    } else {
      # Return standard plot for non-aggregated visualization
      return(qual_plot + 
               ggtitle(paste(read_type, "Read Quality After Filtering (Individual Samples)")))
    }
  }
  
  # Create enhanced visualizations for filtered reads
  n_sample_viz <- min(3, length(filtFs))
  
  # Individual sample plots
  filtered_forward_qual <- create_enhanced_quality_viz(filtFs[1:n_sample_viz], "Forward")
  filtered_reverse_qual <- create_enhanced_quality_viz(filtRs[1:n_sample_viz], "Reverse")
  print(filtered_forward_qual)
  print(filtered_reverse_qual)
  
  # Aggregated profiles with enhanced visualization
  cat("Creating aggregated quality profiles with enhanced visualization...\n")
  
  # Use more samples for aggregated view if available (up to 10)
  n_sample_agg <- min(10, length(filtFs))
  
  # Aggregated plots with enhanced visualization
  forward_enhanced <- create_enhanced_quality_viz(filtFs[1:n_sample_agg], "Forward", aggregate = TRUE)
  reverse_enhanced <- create_enhanced_quality_viz(filtRs[1:n_sample_agg], "Reverse", aggregate = TRUE)
  
  # Display enhanced plots
  print(forward_enhanced)
  print(reverse_enhanced)
  
  # Side-by-side view
  grid.arrange(forward_enhanced, reverse_enhanced, ncol = 2,
               top = "Quality Profiles After Filtering (Aggregated)")
  
  # Create a quality comparison visualization (before vs after filtering)
  if (exists("forward_qual") && exists("reverse_qual")) {
    cat("Creating before vs after filtering quality comparison...\n")
    
    # Extract data from before filtering plots
    before_forward_data <- forward_qual$data
    before_reverse_data <- reverse_qual$data
    
    # Sample a subset of filtered reads quality data for comparison
    filtered_forward_data <- plotQualityProfile(filtFs[1:n_sample_agg], aggregate = TRUE)$data
    filtered_reverse_data <- plotQualityProfile(filtRs[1:n_sample_agg], aggregate = TRUE)$data
    
    # Add a 'Stage' column to both datasets
    before_forward_data$Stage <- "Before Filtering"
    filtered_forward_data$Stage <- "After Filtering"
    before_reverse_data$Stage <- "Before Filtering"
    filtered_reverse_data$Stage <- "After Filtering"
    
    # Combine data
    combined_forward <- rbind(before_forward_data, filtered_forward_data)
    combined_reverse <- rbind(before_reverse_data, filtered_reverse_data)
    
    # Create comparison plots
    forward_comparison <- ggplot(combined_forward, aes(x = Cycle, y = Score, color = Stage)) +
      stat_summary(fun = mean, geom = "line", size = 1.2) +
      geom_hline(yintercept = 30, linetype = "dashed", color = "blue") +
      geom_hline(yintercept = 20, linetype = "dashed", color = "red") +
      {if (exists("truncLen_forward")) 
        geom_vline(xintercept = truncLen_forward, color = "purple", size = 1, alpha = 0.7)} +
      scale_color_manual(values = c("Before Filtering" = "gray40", "After Filtering" = "steelblue")) +
      labs(
        title = "Forward Reads: Quality Before vs After Filtering",
        x = "Cycle (bp position)",
        y = "Mean Quality Score"
      ) +
      theme_minimal()
    
    reverse_comparison <- ggplot(combined_reverse, aes(x = Cycle, y = Score, color = Stage)) +
      stat_summary(fun = mean, geom = "line", size = 1.2) +
      geom_hline(yintercept = 30, linetype = "dashed", color = "blue") +
      geom_hline(yintercept = 20, linetype = "dashed", color = "red") +
      {if (exists("truncLen_reverse")) 
        geom_vline(xintercept = truncLen_reverse, color = "purple", size = 1, alpha = 0.7)} +
      scale_color_manual(values = c("Before Filtering" = "gray40", "After Filtering" = "steelblue")) +
      labs(
        title = "Reverse Reads: Quality Before vs After Filtering",
        x = "Cycle (bp position)",
        y = "Mean Quality Score"
      ) +
      theme_minimal()
    
    # Display comparison plots
    grid.arrange(forward_comparison, reverse_comparison, ncol = 2,
                 top = "Quality Improvement After Filtering")
  }
  
} else {
  cat("No samples remain after filtering. Please review filtering parameters.\n")
  knitr::knit_exit()
}
```

# Learn Error Rates

```{r learn-errors}
# Learn error rates with optimized parallelization
cat("Learning error rates with optimized parallelization...\n")

# Determine sample size for error learning
# For large datasets, use a subset of samples for efficiency
MAX_ERROR_LEARN_SAMPLES <- 40  # Maximum samples to use for error learning

# Sample selection for error learning
if(length(filtFs) > MAX_ERROR_LEARN_SAMPLES) {
  cat(sprintf("Using random subset of %d samples for error learning...\n", MAX_ERROR_LEARN_SAMPLES))
  
  # Select a random subset for error learning
  set.seed(100)  # For reproducibility
  error_learn_samples <- sample(seq_along(filtFs), MAX_ERROR_LEARN_SAMPLES)
  
  # Learn forward error rates using subset
  cat("Learning error rates for forward reads...\n")
  errF <- learnErrors(filtFs[error_learn_samples], multithread = TRUE, randomize = TRUE, verbose = TRUE)
  
  # Learn reverse error rates using subset
  cat("Learning error rates for reverse reads...\n")
  errR <- learnErrors(filtRs[error_learn_samples], multithread = TRUE, randomize = TRUE, verbose = TRUE)
  
  # Ensure garbage collection runs after each step
  gc()
} else {
  # For smaller datasets, use all samples
  cat("Learning error rates for forward reads using all samples...\n")
  errF <- learnErrors(filtFs, multithread = TRUE, randomize = TRUE, verbose = TRUE)
  
  # Force garbage collection between error learning steps
  gc()
  
  cat("Learning error rates for reverse reads using all samples...\n")
  errR <- learnErrors(filtRs, multithread = TRUE, randomize = TRUE, verbose = TRUE)
}

# Plot error rates for forward reads
errF_plot <- plotErrors(errF, nominalQ = TRUE)
print(errF_plot + ggtitle("Error Rates - Forward Reads"))

# Plot error rates for reverse reads
errR_plot <- plotErrors(errR, nominalQ = TRUE)
print(errR_plot + ggtitle("Error Rates - Reverse Reads"))

# Create error rate plots with more informative titles
errF_plot <- plotErrors(errF, nominalQ = TRUE) + 
  ggtitle("Error Rates - Forward Reads") + 
  labs(subtitle = "Points: observed error rates; Lines: fitted error model")

errR_plot <- plotErrors(errR, nominalQ = TRUE) + 
  ggtitle("Error Rates - Reverse Reads") + 
  labs(subtitle = "Points: observed error rates; Lines: fitted error model")

# Create a side-by-side plot
grid.arrange(errF_plot, errR_plot, ncol = 2)

# Create checkpoint after learning error rates with performance metrics
save_checkpoint("after_error_learning", 
               c("filtFs", "filtRs", "errF", "errR", "sample.names",
                 "truncLen_forward", "truncLen_reverse", "maxEE_forward", "maxEE_reverse",
                 "is_multi_run"),  # Added multi-run tracking
               overwrite = TRUE)
cat("Checkpoint saved after error learning step\n")

# Create a separate file with error model parameters for reproducibility
if (!dir.exists("results")) dir.create("results")
error_models <- list(
  forward = errF,
  reverse = errR,
  timestamp = Sys.time(),
  parameters = list(
    truncation = c(forward = truncLen_forward, reverse = truncLen_reverse),
    maxEE = c(forward = maxEE_forward, reverse = maxEE_reverse)
  )
)
saveRDS(error_models, "results/error_models.rds")
cat("Error models saved to results/error_models.rds\n")
```

# Dereplication

```{r dereplication}
# Dereplicate identical reads
cat("Dereplicating forward reads...\n")
derepFs <- derepFastq(filtFs, verbose = TRUE)

cat("Dereplicating reverse reads...\n")
derepRs <- derepFastq(filtRs, verbose = TRUE)

# Name the derep-class objects by the sample names
names(derepFs) <- names(filtFs)
names(derepRs) <- names(filtRs)

# Create summary of dereplication
derep_summary <- data.frame(
  Sample = names(derepFs),
  InputReads = sapply(derepFs, function(x) sum(getUniques(x))),
  UniqueSequences = sapply(derepFs, function(x) length(getUniques(x))),
  CompressionRatio = sapply(derepFs, function(x) sum(getUniques(x)) / length(getUniques(x)))
)

kable(head(derep_summary, 10), caption = "Dereplication Summary (First 10 Samples)")

# Average compression ratio
avg_compression <- mean(derep_summary$CompressionRatio)
cat("Average compression ratio:", round(avg_compression, 1), 
    "reads per unique sequence\n")
    
# Create checkpoint after dereplication step
save_checkpoint("after_dereplication", 
               c("filtFs", "filtRs", "errF", "errR", "derepFs", "derepRs", "sample.names", 
                 "derep_summary", "avg_compression"),
               overwrite = TRUE)
cat("Checkpoint saved after dereplication step\n")

# Save dereplication summary for easier access
if (!dir.exists("results")) dir.create("results")
saveRDS(derep_summary, "results/derep_summary.rds")
write.csv(derep_summary, "results/derep_summary.csv", row.names = FALSE)
cat("Dereplication summary saved to results/derep_summary.csv\n")
```

# Sample Inference

DADA2 will now denoise the data and infer the true biological sequences.

```{r dada2}
# Apply the DADA2 algorithm to forward reads with optimized parallelization
cat("Denoising forward reads with optimized parallelization...\n")

# Determine pooling option based on multi-run status
# For multi-run data, we can use "pseudo" pooling within each run
# For a single run, use "pseudo" pooling overall
pooling_option <- ifelse(is_multi_run && exists("file_to_run"), FALSE, "pseudo")

if(is_multi_run && exists("file_to_run")) {
  cat("Multi-run mode: Each run will be processed separately with pseudo-pooling within each run\n")
} else {
  cat("Using pseudo-pooling across all samples\n")
}

# Use parallel batch processing for large datasets
if (length(derepFs) > 20 || is_multi_run) {
  # Process in batches for better memory management
  batch_size <- min(10, ceiling(length(derepFs)/4))
  
  # Calculate optimal batch size based on available memory
  mem_info <- tryCatch({
    if (.Platform$OS.type == "unix") {
      mem <- system("vmstat -s | grep 'free memory'", intern = TRUE)
      free_mem_kb <- as.numeric(gsub("\\D", "", mem))
      free_mem_gb <- free_mem_kb / 1024^2
      # Adjust batch size based on available memory (rough heuristic)
      mem_based_batch <- max(1, floor(free_mem_gb / 0.5)) # Assume ~0.5GB per 10 samples
      batch_size <- min(batch_size, mem_based_batch)
      cat(sprintf("Detected %.1fGB free memory, adjusted batch size to %d\n", free_mem_gb, batch_size))
    }
  }, error = function(e) {
    # If memory detection fails, use conservative default
    cat("Memory detection failed, using default batch size\n")
  })
  
  cat(sprintf("Processing forward reads in batches of %d samples...\n", batch_size))
  
  if(is_multi_run && exists("file_to_run")) {
    # In multi-run mode, use runs for batching
    run_names <- unique(file_to_run)
    cat("Processing each sequencing run independently\n")
    
    # Process each run separately
    dadaFs_list <- list()
    for(run in run_names) {
      cat(sprintf("Processing run '%s' forward reads...\n", run))
      # Get samples for this run
      run_samples <- names(derepFs)[file_to_run == run]
      
      if(length(run_samples) > batch_size) {
        # Further divide into batches if the run has many samples
        run_batches <- split(run_samples, ceiling(seq_along(run_samples)/batch_size))
        
        # Process each batch with pooling within the run
        run_dadaFs <- list()
        for(j in seq_along(run_batches)) {
          cat(sprintf("  Processing batch %d of %d within run '%s'...\n", 
                    j, length(run_batches), run))
          batch_samples <- run_batches[[j]]
          # Use pseudo-pooling within each run
          batch_dadaFs <- dada(derepFs[batch_samples], err = errF, multithread = TRUE, pool = "pseudo")
          run_dadaFs <- c(run_dadaFs, batch_dadaFs)
        }
      } else {
        # Process all samples in this run at once with pooling
        run_dadaFs <- dada(derepFs[run_samples], err = errF, multithread = TRUE, pool = "pseudo")
      }
      
      # Add to the full list
      dadaFs_list <- c(dadaFs_list, run_dadaFs)
      
      # Force garbage collection
      gc()
    }
    dadaFs <- dadaFs_list
  } else {
    # Standard batch processing (not separated by run)
    sample_names <- names(derepFs)
    batch_groups <- split(sample_names, ceiling(seq_along(sample_names)/batch_size))
    
    # Process each batch and combine results
    dadaFs_list <- list()
    for (i in seq_along(batch_groups)) {
      cat(sprintf("Processing forward batch %d of %d...\n", i, length(batch_groups)))
      batch_samples <- batch_groups[[i]]
      dadaFs_batch <- dada(derepFs[batch_samples], err = errF, multithread = TRUE, pool = pooling_option)
      dadaFs_list <- c(dadaFs_list, dadaFs_batch)
    }
    dadaFs <- dadaFs_list
  }
} else {
  # For smaller datasets, process all at once
  dadaFs <- dada(derepFs, err = errF, multithread = TRUE, pool = pooling_option)
}

# Apply the DADA2 algorithm to reverse reads with optimized parallelization
cat("Denoising reverse reads with optimized parallelization...\n")

# Use parallel batch processing for large datasets
if (length(derepRs) > 20 || is_multi_run) {
  # Process in batches for better memory management
  batch_size <- min(10, ceiling(length(derepRs)/4))
  cat(sprintf("Processing reverse reads in batches of %d samples...\n", batch_size))
  
  if(is_multi_run && exists("file_to_run")) {
    # In multi-run mode, use runs for batching
    run_names <- unique(file_to_run)
    cat("Processing each sequencing run independently\n")
    
    # Process each run separately
    dadaRs_list <- list()
    for(run in run_names) {
      cat(sprintf("Processing run '%s' reverse reads...\n", run))
      # Get samples for this run
      run_samples <- names(derepRs)[file_to_run == run]
      
      if(length(run_samples) > batch_size) {
        # Further divide into batches if the run has many samples
        run_batches <- split(run_samples, ceiling(seq_along(run_samples)/batch_size))
        
        # Process each batch with pooling within the run
        run_dadaRs <- list()
        for(j in seq_along(run_batches)) {
          cat(sprintf("  Processing batch %d of %d within run '%s'...\n", 
                    j, length(run_batches), run))
          batch_samples <- run_batches[[j]]
          # Use pseudo-pooling within each run
          batch_dadaRs <- dada(derepRs[batch_samples], err = errR, multithread = TRUE, pool = "pseudo")
          run_dadaRs <- c(run_dadaRs, batch_dadaRs)
        }
      } else {
        # Process all samples in this run at once with pooling
        run_dadaRs <- dada(derepRs[run_samples], err = errR, multithread = TRUE, pool = "pseudo")
      }
      
      # Add to the full list
      dadaRs_list <- c(dadaRs_list, run_dadaRs)
      
      # Force garbage collection
      gc()
    }
    dadaRs <- dadaRs_list
  } else {
    # Standard batch processing (not separated by run)
    sample_names <- names(derepRs)
    batch_groups <- split(sample_names, ceiling(seq_along(sample_names)/batch_size))
    
    # Process each batch and combine results
    dadaRs_list <- list()
    for (i in seq_along(batch_groups)) {
      cat(sprintf("Processing reverse batch %d of %d...\n", i, length(batch_groups)))
      batch_samples <- batch_groups[[i]]
      dadaRs_batch <- dada(derepRs[batch_samples], err = errR, multithread = TRUE, pool = pooling_option)
      dadaRs_list <- c(dadaRs_list, dadaRs_batch)
    }
    dadaRs <- dadaRs_list
  }
} else {
  # For smaller datasets, process all at once
  dadaRs <- dada(derepRs, err = errR, multithread = TRUE, pool = pooling_option)
}

# Inspect the returned dada-class object for the first sample
cat("\nDADA2 denoising results for first sample (forward reads):\n")
dadaFs[[1]]

# Create summary of denoising
denoise_summary <- data.frame(
  Sample = names(dadaFs),
  InputReads = sapply(derepFs, function(x) sum(getUniques(x))),
  DenoisedReads = sapply(dadaFs, function(x) sum(getUniques(x$denoised))),
  ASVsFound = sapply(dadaFs, function(x) length(getUniques(x$denoised))),
  PercentRetained = sapply(dadaFs, function(x) round(sum(getUniques(x$denoised)) / sum(getUniques(x)) * 100, 1))
)

# Add run information if in multi-run mode
if(is_multi_run && exists("file_to_run")) {
  denoise_summary$Run <- file_to_run[match(denoise_summary$Sample, names(derepFs))]
}

kable(head(denoise_summary, 10), caption = "Denoising Summary (First 10 Samples)")

# Average denoising stats
avg_denoised_pct <- mean(denoise_summary$PercentRetained)
total_asvs_forward <- sum(sapply(dadaFs, function(x) length(x$denoised)))
cat("Average percentage of reads retained after denoising:", round(avg_denoised_pct, 1), "%\n")
cat("Total ASVs identified in forward reads:", total_asvs_forward, "\n")

# Create checkpoint after sample inference step with better tracking
save_checkpoint("after_sample_inference", 
               c("derepFs", "derepRs", "errF", "errR", "dadaFs", "dadaRs", "sample.names",
                 "is_multi_run", "file_to_run", "denoise_summary", "pooling_option"),
               overwrite = TRUE)
cat("Checkpoint saved after sample inference step\n")

# Save denoising summary for easier access and dashboard visualization
if (!dir.exists("results")) dir.create("results")
saveRDS(denoise_summary, "results/denoise_summary.rds")
write.csv(denoise_summary, "results/denoise_summary.csv", row.names = FALSE)
cat("Denoising summary saved to results/denoise_summary.csv\n")

# Record the pooling strategy used
pooling_info <- list(
  strategy = pooling_option,
  multi_run = is_multi_run,
  timestamp = Sys.time(),
  forward_asvs = total_asvs_forward
)
saveRDS(pooling_info, "results/pooling_strategy.rds")
cat("Pooling strategy information saved to results/pooling_strategy.rds\n")
```

# Merge Paired Reads

Now we'll merge the forward and reverse reads to create full-length sequences.

```{r merge-reads}
# Merge paired reads with optimized parallelization
cat("Merging paired reads with optimized parallelization...\n")

# Determine if we need to use batched merging based on dataset size
if (length(dadaFs) > 20) {
  cat("Using batched merging for large dataset...\n")
  
  # Set up batch processing
  batch_size <- min(10, ceiling(length(dadaFs)/4))
  sample_names <- names(dadaFs)
  batch_groups <- split(sample_names, ceiling(seq_along(sample_names)/batch_size))
  
  # Initialize results list
  mergers <- list()
  
  # Process each batch
  for (i in seq_along(batch_groups)) {
    cat(sprintf("Merging batch %d of %d...\n", i, length(batch_groups)))
    batch_samples <- batch_groups[[i]]
    
    # Calculate appropriate overlap parameter
    min_overlap <- if(truncation_lengths$expected_overlap < 20) 10 else 12
    
    # Process this batch
    batch_mergers <- mergePairs(
      dadaFs[batch_samples], 
      derepFs[batch_samples], 
      dadaRs[batch_samples], 
      derepRs[batch_samples],
      verbose = TRUE,
      minOverlap = min_overlap
    )
    
    # Add to results
    mergers <- c(mergers, batch_mergers)
    
    # Force garbage collection to free memory
    gc()
  }
} else {
  # For smaller datasets, use standard approach
  min_overlap <- if(truncation_lengths$expected_overlap < 20) 10 else 12
  mergers <- mergePairs(
    dadaFs, 
    derepFs, 
    dadaRs, 
    derepRs,
    verbose = TRUE,
    minOverlap = min_overlap
  )
}

# Inspect the merger data.frame for the first sample
cat("\nMerged read pairs for first sample:\n")
head(mergers[[1]])

# Create summary of merging
merge_summary <- data.frame(
  Sample = names(mergers),
  DenoisedReads = sapply(dadaFs, function(x) sum(getUniques(x$denoised))),
  MergedReads = sapply(mergers, function(x) sum(getUniques(x))),
  PercentMerged = sapply(names(mergers), function(sample) 
    round(sum(getUniques(mergers[[sample]])) / sum(getUniques(dadaFs[[sample]]$denoised)) * 100, 1))
)

kable(head(merge_summary, 10), caption = "Read Merging Summary (First 10 Samples)")

# Average merging stats
avg_merged_pct <- mean(merge_summary$PercentMerged)
cat("Average percentage of denoised reads successfully merged:", round(avg_merged_pct, 1), "%\n")

# Check if any samples have very low merging rates
low_merge_samples <- merge_summary[merge_summary$PercentMerged < 50, ]
if (nrow(low_merge_samples) > 0) {
  cat("WARNING:", nrow(low_merge_samples), "samples have low merging rates (<50%):\n")
  print(low_merge_samples)
}

# Create checkpoint after merging reads step with more tracking
save_checkpoint("after_merge_reads", 
               c("derepFs", "derepRs", "dadaFs", "dadaRs", "mergers", "sample.names",
                 "truncation_lengths", "merge_summary", "avg_merged_pct", "low_merge_samples"),
               overwrite = TRUE)
cat("Checkpoint saved after merging paired reads step\n")

# Save merging summary for dashboard visualization
if (!dir.exists("results")) dir.create("results")
saveRDS(merge_summary, "results/merge_summary.rds")
write.csv(merge_summary, "results/merge_summary.csv", row.names = FALSE)
cat("Merging summary saved to results/merge_summary.csv\n")

# Save problematic samples information if any exist
if (nrow(low_merge_samples) > 0) {
  saveRDS(low_merge_samples, "results/low_merge_samples.rds")
  write.csv(low_merge_samples, "results/low_merge_samples.csv", row.names = FALSE)
  cat("List of problematic samples with low merging rates saved to results/low_merge_samples.csv\n")
}
```

# Construct ASV Table

```{r seqtab}
# Construct sequence table
cat("Constructing ASV sequence table...\n")

# Special handling for multi-run data
if (is_multi_run && exists("file_to_run")) {
  cat("Multi-run mode: Creating separate sequence tables for each run first\n")
  
  # Get unique runs
  run_names <- unique(file_to_run)
  
  # Create a separate sequence table for each run
  run_seqtabs <- list()
  for (run in run_names) {
    cat(sprintf("Creating sequence table for run '%s'...\n", run))
    # Get mergers for this run
    run_samples <- names(mergers)[file_to_run == run]
    
    # Make sequence table for this run
    run_seqtab <- makeSequenceTable(mergers[run_samples])
    run_seqtabs[[run]] <- run_seqtab
    
    cat(sprintf("  Run '%s' sequence table dimensions: %d samples x %d ASVs\n", 
              run, nrow(run_seqtab), ncol(run_seqtab)))
  }
  
  # Merge all sequence tables
  cat("Merging sequence tables from all runs...\n")
  seqtab <- mergeSequenceTables(tables = run_seqtabs)
  
  # Add run info to sequence table attributes for tracking
  attr(seqtab, "run_info") <- file_to_run
} else {
  # Standard processing for single run
  seqtab <- makeSequenceTable(mergers)
}

# Display table dimensions
cat("Dimensions of ASV table:", dim(seqtab), "(samples × ASVs)\n")

# Count ASVs by length
asv_lengths <- table(nchar(getSequences(seqtab)))
asv_lengths_df <- data.frame(
  Length = names(asv_lengths),
  Count = as.vector(asv_lengths)
)

# Create a histogram of ASV lengths
asv_length_plot <- ggplot(asv_lengths_df, aes(x = Length, y = Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Distribution of ASV Lengths",
       x = "Sequence Length (bp)",
       y = "Number of ASVs")

print(asv_length_plot)

# Get the most common ASV length
modal_length <- as.numeric(names(which.max(asv_lengths)))
cat("Most common ASV length:", modal_length, "bp\n")

# Compare to expected amplicon size
cat("Expected amplicon size based on primers:", expected_amplicon_size, "bp\n")
if (abs(modal_length - expected_amplicon_size) <= 10) {
  cat("ASV lengths match expected amplicon size ✓\n")
} else {
  cat("WARNING: Most common ASV length differs from expected amplicon size\n")
}

# Filter out suspiciously short or long ASVs
min_allowed_len <- max(expected_amplicon_size - 50, 0.8 * modal_length)
max_allowed_len <- min(expected_amplicon_size + 50, 1.2 * modal_length)

cat("Filtering ASVs with lengths outside the range:", min_allowed_len, "-", max_allowed_len, "bp\n")
seqtab_filtered <- seqtab[, nchar(colnames(seqtab)) >= min_allowed_len & 
                           nchar(colnames(seqtab)) <= max_allowed_len]

# Report filtering results
cat("Removed", ncol(seqtab) - ncol(seqtab_filtered), "ASVs with suspicious lengths\n")
cat("Retained", ncol(seqtab_filtered), "ASVs for further analysis\n")
original_reads <- sum(seqtab)
filtered_reads <- sum(seqtab_filtered)
cat("Removed", original_reads - filtered_reads, "reads (", 
    round((original_reads - filtered_reads) / original_reads * 100, 1), "% of total)\n")

# Use filtered sequence table for further analysis
seqtab <- seqtab_filtered

# If multi-run, create a summary of ASVs per run
if (is_multi_run && exists("file_to_run") && exists("run_seqtabs")) {
  # Create a table showing how many ASVs were found in each run
  run_asv_counts <- sapply(run_seqtabs, ncol)
  shared_asv_count <- ncol(seqtab)
  
  # Calculate percentage of ASVs shared across runs
  unique_asvs_all_runs <- length(unique(unlist(lapply(run_seqtabs, colnames))))
  shared_pct <- round(shared_asv_count / unique_asvs_all_runs * 100, 1)
  
  run_summary <- data.frame(
    Run = names(run_asv_counts),
    ASVs = run_asv_counts,
    SamplesInRun = sapply(run_seqtabs, nrow)
  )
  
  # Add overall row
  run_summary <- rbind(run_summary, 
                     c("COMBINED", ncol(seqtab), nrow(seqtab)))
  
  # Display summary table
  kable(run_summary, caption = "ASV Counts by Run")
  
  cat("Found", unique_asvs_all_runs, "unique ASVs across all runs\n")
  cat("After merging:", ncol(seqtab), "ASVs in the combined dataset\n")
}

# Create checkpoint after ASV table construction step with more tracking
save_checkpoint("after_asv_table", 
               c("mergers", "seqtab", "sample.names", "seqtab_filtered", "expected_amplicon_size",
                 "is_multi_run", "file_to_run", "run_seqtabs", 
                 "asv_lengths", "modal_length", "min_allowed_len", "max_allowed_len"),
               overwrite = TRUE)
cat("Checkpoint saved after ASV table construction step\n")

# Save ASV length distribution and filtering thresholds for documentation
asv_lengths_info <- list(
  distribution = asv_lengths_df,
  modal_length = modal_length,
  expected_amplicon_size = expected_amplicon_size,
  filtering_thresholds = c(min = min_allowed_len, max = max_allowed_len),
  original_counts = c(asvs = ncol(seqtab), reads = sum(seqtab)),
  filtered_counts = c(asvs = ncol(seqtab_filtered), reads = sum(seqtab_filtered))
)
if (!dir.exists("results")) dir.create("results")
saveRDS(asv_lengths_info, "results/asv_lengths_info.rds")
cat("ASV length distribution and filtering thresholds saved to results/asv_lengths_info.rds\n")

# Create a separate file with just the filtered sequences for easier downstream use
if (!dir.exists("results")) dir.create("results")
saveRDS(colnames(seqtab_filtered), "results/filtered_asv_sequences.rds")
cat("Filtered ASV sequences saved to results/filtered_asv_sequences.rds\n")
```

# Remove Chimeras

```{r chimeras}
# Remove chimeric sequences using consensus method with optimized parallelization
cat("Removing chimeric sequences with optimized parallelization...\n")

# For very large ASV tables, use a batched approach to chimera removal
if(ncol(seqtab) > 5000) {
  cat("Using batched approach for chimera removal due to large number of ASVs...\n")
  
  # Split ASVs into batches
  batch_size <- 2000  # Process 2000 ASVs at a time
  n_batches <- ceiling(ncol(seqtab) / batch_size)
  
  # Initialize result table
  seqtab.nochim <- NULL
  
  # Process each batch
  for(i in 1:n_batches) {
    cat(sprintf("Processing chimera batch %d of %d...\n", i, n_batches))
    
    # Calculate batch indices
    start_idx <- (i-1) * batch_size + 1
    end_idx <- min(i * batch_size, ncol(seqtab))
    
    # Extract batch
    batch_seqtab <- seqtab[, start_idx:end_idx, drop=FALSE]
    
    # Remove chimeras from this batch
    batch_nochim <- removeBimeraDenovo(batch_seqtab, 
                                      method = "consensus", 
                                      multithread = TRUE, 
                                      verbose = TRUE)
    
    # Combine with previous results
    if(is.null(seqtab.nochim)) {
      seqtab.nochim <- batch_nochim
    } else {
      seqtab.nochim <- cbind(seqtab.nochim, batch_nochim)
    }
    
    # Force garbage collection
    gc()
  }
} else {
  # For smaller tables, use the standard approach
  seqtab.nochim <- removeBimeraDenovo(seqtab, 
                                     method = "consensus", 
                                     multithread = TRUE, 
                                     verbose = TRUE)
}

# Calculate the percentage of non-chimeric sequences
chimera_pct <- sum(seqtab.nochim)/sum(seqtab) * 100
cat("Percentage of non-chimeric sequences:", round(chimera_pct, 1), "%\n")

# Chimera stats
cat("ASVs before chimera removal:", ncol(seqtab), "\n")
cat("ASVs after chimera removal:", ncol(seqtab.nochim), "\n")
cat("ASVs identified as chimeric:", ncol(seqtab) - ncol(seqtab.nochim), 
    "(", round((ncol(seqtab) - ncol(seqtab.nochim))/ncol(seqtab) * 100, 1), "%)\n")

# Create a summary dataframe
chimera_summary <- data.frame(
  Category = c("Before Chimera Removal", "After Chimera Removal", "Removed as Chimeric"),
  ASVs = c(ncol(seqtab), ncol(seqtab.nochim), ncol(seqtab) - ncol(seqtab.nochim)),
  TotalReads = c(sum(seqtab), sum(seqtab.nochim), sum(seqtab) - sum(seqtab.nochim)),
  Percentage = c(100, chimera_pct, 100 - chimera_pct)
)

kable(chimera_summary, caption = "Chimera Removal Summary")

# Check if chimera removal is excessive
if (chimera_pct < 50) {
  cat("WARNING: Less than 50% of sequences remain after chimera removal.\n")
  cat("This could indicate poor read merging or excessive chimera removal.\n")
  cat("Consider revisiting the truncation lengths and quality filtering parameters.\n")
  
  # Log problematic samples with high chimera rates
  sample_chimera_rates <- data.frame(
    Sample = rownames(seqtab),
    PreChimera = rowSums(seqtab),
    PostChimera = rowSums(seqtab.nochim),
    ChimeraPercent = round((1 - rowSums(seqtab.nochim)/rowSums(seqtab)) * 100, 1)
  )
  
  # Sort by chimera percentage (highest first)
  sample_chimera_rates <- sample_chimera_rates[order(sample_chimera_rates$ChimeraPercent, decreasing = TRUE),]
  
  # Display top 10 most problematic samples
  cat("\nSamples with highest chimera rates:\n")
  print(head(sample_chimera_rates, 10))
}

# Create checkpoint after chimera removal step with additional tracking
save_checkpoint("after_chimera_removal", 
               c("seqtab", "seqtab.nochim", "sample.names", "chimera_pct",
                 "chimera_summary", "sample_chimera_rates"),
               overwrite = TRUE)
cat("Checkpoint saved after chimera removal step\n")

# Save chimera removal information for dashboard visualization
if (!dir.exists("results")) dir.create("results")
saveRDS(chimera_summary, "results/chimera_summary.rds")
write.csv(chimera_summary, "results/chimera_summary.csv", row.names = FALSE)
cat("Chimera removal summary saved to results/chimera_summary.csv\n")

# Save per-sample chimera rates if available
if (exists("sample_chimera_rates")) {
  saveRDS(sample_chimera_rates, "results/sample_chimera_rates.rds")
  write.csv(sample_chimera_rates, "results/sample_chimera_rates.csv", row.names = FALSE)
  cat("Sample-specific chimera rates saved to results/sample_chimera_rates.csv\n")
}
```

# Track Reads Through the Pipeline

```{r track}
# Track reads through the pipeline
getN <- function(x) sum(getUniques(x))

# Create a robust tracking function that handles empty or missing data
create_tracking_table <- function() {
  # First check that we have data to track
  if (length(names(derepFs)) == 0) {
    cat("ERROR: No samples available for tracking. Some processing steps may have failed.\n")
    # Return an empty tracking dataframe with the correct structure
    return(data.frame(
      Sample = character(),
      Input = numeric(),
      Filtered = numeric(),
      Denoised_F = numeric(),
      Denoised_R = numeric(),
      Merged = numeric(),
      NonChimeric = numeric()
    ))
  }
  
  # Get all possible sample names from any of the steps
  all_samples <- unique(c(
    rownames(out),
    names(derepFs),
    names(dadaFs),
    names(dadaRs),
    names(mergers)
  ))
  
  # Create a tracking data frame with NA values
  track_df <- data.frame(
    Sample = all_samples,
    Input = NA_real_,
    Filtered = NA_real_,
    Denoised_F = NA_real_,
    Denoised_R = NA_real_,
    Merged = NA_real_,
    NonChimeric = NA_real_,
    row.names = all_samples
  )
  
  # Fill in values from each step
  # Input reads
  if (nrow(out) > 0) {
    for (s in rownames(out)) {
      if (s %in% all_samples) {
        track_df[s, "Input"] <- out[s, "reads.in"]
      }
    }
  }
  
  # Filtered reads
  if (nrow(out) > 0) {
    for (s in rownames(out)) {
      if (s %in% all_samples) {
        track_df[s, "Filtered"] <- out[s, "reads.out"]
      }
    }
  }
  
  # Denoised forward reads
  for (s in names(dadaFs)) {
    if (s %in% all_samples) {
      track_df[s, "Denoised_F"] <- sum(getUniques(dadaFs[[s]]))
    }
  }
  
  # Denoised reverse reads
  for (s in names(dadaRs)) {
    if (s %in% all_samples) {
      track_df[s, "Denoised_R"] <- sum(getUniques(dadaRs[[s]]))
    }
  }
  
  # Merged reads
  for (s in names(mergers)) {
    if (s %in% all_samples) {
      track_df[s, "Merged"] <- sum(getUniques(mergers[[s]]))
    }
  }
  
  # Non-chimeric reads
  if (nrow(seqtab.nochim) > 0) {
    for (s in rownames(seqtab.nochim)) {
      if (s %in% all_samples) {
        track_df[s, "NonChimeric"] <- sum(seqtab.nochim[s, ])
      }
    }
  }
  
  # Keep only samples with complete data to avoid NA issues
  complete_samples <- rownames(track_df)[complete.cases(track_df)]
  if (length(complete_samples) < nrow(track_df)) {
    cat("Warning: ", nrow(track_df) - length(complete_samples), 
        " samples had incomplete data and were removed from tracking.\n")
    if (length(complete_samples) == 0) {
      cat("ERROR: No samples have complete data through all pipeline steps.\n")
      cat("This suggests a failure in one or more processing steps.\n")
      # Return the incomplete data anyway for diagnostics
      return(track_df)
    }
    track_df <- track_df[complete_samples, ]
  }
  
  return(track_df)
}

# Create the tracking table
track_df <- create_tracking_table()

# Check if we have data to proceed
if (nrow(track_df) == 0) {
  cat("WARNING: No samples could be tracked through the pipeline.\n")
  cat("Skipping tracking visualization.\n")
} else {
  cat("Successfully tracked", nrow(track_df), "samples through the pipeline.\n")
}

# Make sure we have the Sample column for display
if (!("Sample" %in% colnames(track_df)) && nrow(track_df) > 0) {
  track_df$Sample <- rownames(track_df)
}

# Calculate percentage retained at each step if we have data
if (nrow(track_df) > 0) {
  # Safe division function to handle division by zero
  safe_div <- function(num, denom) {
    ifelse(denom > 0, num / denom * 100, NA)
  }
  
  track_df$Percent_Filtered <- round(safe_div(track_df$Filtered, track_df$Input), 1)
  track_df$Percent_Denoised_F <- round(safe_div(track_df$Denoised_F, track_df$Filtered), 1)
  track_df$Percent_Merged <- round(safe_div(track_df$Merged, track_df$Denoised_F), 1)
  track_df$Percent_NonChimeric <- round(safe_div(track_df$NonChimeric, track_df$Merged), 1)
  track_df$Percent_Final <- round(safe_div(track_df$NonChimeric, track_df$Input), 1)
}

# Display tracking information for the first few samples
kable(head(track_df[, c("Sample", "Input", "Filtered", "Denoised_F", "Merged", "NonChimeric", "Percent_Final")], 10), 
      caption = "Read Tracking Through Pipeline (First 10 Samples)")

# Create a summary of read loss at each step if we have data
if (nrow(track_df) > 0) {
  # Safe sum function
  safe_sum <- function(x) {
    sum(x, na.rm = TRUE)
  }
  
  # Safe division function
  safe_percent <- function(numerator, denominator) {
    if (denominator > 0) {
      round(numerator / denominator * 100, 1)
    } else {
      NA
    }
  }
  
  # Calculate totals
  total_input <- safe_sum(track_df$Input)
  total_filtered <- safe_sum(track_df$Filtered)
  total_denoised <- safe_sum(track_df$Denoised_F)
  total_merged <- safe_sum(track_df$Merged)
  total_nonchim <- safe_sum(track_df$NonChimeric)
  
  # Create summary data frame
  pipeline_summary <- data.frame(
    Step = c("Raw Input", "After Filtering", "After Denoising", "After Merging", "After Chimera Removal"),
    Reads = c(total_input, total_filtered, total_denoised, total_merged, total_nonchim),
    PercentOfInput = c(
      100, 
      safe_percent(total_filtered, total_input),
      safe_percent(total_denoised, total_input),
      safe_percent(total_merged, total_input),
      safe_percent(total_nonchim, total_input)
    ),
    PercentOfPreviousStep = c(
      100,
      safe_percent(total_filtered, total_input),
      safe_percent(total_denoised, total_filtered),
      safe_percent(total_merged, total_denoised),
      safe_percent(total_nonchim, total_merged)
    )
  )
  
  kable(pipeline_summary, caption = "Summary of Reads Retained at Each Processing Step")
} else {
  cat("No tracking data available for pipeline summary.\n")
}

# Create a plot to visualize read tracking if we have data
if (nrow(track_df) > 0) {
  # Try to create visualizations with proper error handling
  tryCatch({
    # Reshape data for plotting
    track_long <- tidyr::pivot_longer(
      track_df[, c("Sample", "Input", "Filtered", "Denoised_F", "Merged", "NonChimeric")],
      cols = c("Input", "Filtered", "Denoised_F", "Merged", "NonChimeric"),
      names_to = "Step",
      values_to = "Reads"
    )
    
    # Convert Step to factor with specified order
    track_long$Step <- factor(track_long$Step, 
                              levels = c("Input", "Filtered", "Denoised_F", "Merged", "NonChimeric"))
    
    # Create the plot for all samples together
    overall_tracking_plot <- ggplot(track_long, aes(x = Step, y = Reads, group = Sample, color = Sample)) +
      geom_line(alpha = 0.5) +
      geom_point(size = 1) +
      theme_minimal() +
      labs(title = "Read Count Tracking Through DADA2 Pipeline",
           x = "Processing Step",
           y = "Number of Reads") +
      theme(legend.position = "none")  # Hide individual sample legend as it can be cluttered
    
    # Create an average tracking plot
    avg_tracking <- aggregate(Reads ~ Step, data = track_long, FUN = mean, na.rm = TRUE)
    avg_tracking$StdDev <- aggregate(Reads ~ Step, data = track_long, FUN = sd, na.rm = TRUE)$Reads
    
    avg_tracking_plot <- ggplot(avg_tracking, aes(x = Step, y = Reads, group = 1)) +
      geom_line(size = 1.5, color = "blue") +
      geom_point(size = 3, color = "blue") +
      geom_errorbar(aes(ymin = pmax(Reads - StdDev, 0), ymax = Reads + StdDev), width = 0.2, color = "blue") +
      theme_minimal() +
      labs(title = "Average Read Counts Through DADA2 Pipeline",
           subtitle = "Error bars show standard deviation across samples",
           x = "Processing Step",
           y = "Average Number of Reads")
    
    # Show both plots
    grid.arrange(overall_tracking_plot, avg_tracking_plot, ncol = 1)
  }, error = function(e) {
    cat("Error creating read tracking plots:", conditionMessage(e), "\n")
    cat("Displaying simple summary statistics instead.\n")
    
    # Display summary statistics as an alternative
    step_summary <- sapply(c("Input", "Filtered", "Denoised_F", "Merged", "NonChimeric"), function(step) {
      values <- track_df[[step]]
      c(Mean = mean(values, na.rm = TRUE),
        Min = min(values, na.rm = TRUE),
        Max = max(values, na.rm = TRUE))
    })
    
    print(step_summary)
  })
} else {
  cat("No tracking data available for visualization.\n")
}

# Save the tracking data for dashboard visualization if we have results directory and data
if (nrow(track_df) > 0) {
  if (!dir.exists("results")) dir.create("results")
  
  tryCatch({
    # Save the basic tracking data frame
    saveRDS(track_df, "results/read_tracking.rds")
    cat("Saved read tracking data to results/read_tracking.rds\n")
    
    # Save a more detailed tracking data that includes percentages
    tracking_detailed <- track_df
    if (ncol(track_df) >= 7) {
      write.csv(tracking_detailed, "results/read_tracking_detailed.csv")
      cat("Saved detailed tracking data to results/read_tracking_detailed.csv\n")
    }
    
    # Save pipeline summary info separately for easy access
    if (exists("pipeline_summary")) {
      saveRDS(pipeline_summary, "results/pipeline_summary.rds")
      write.csv(pipeline_summary, "results/pipeline_summary.csv")
      cat("Saved pipeline summary to results/pipeline_summary.csv\n")
    }
  }, error = function(e) {
    cat("Error saving tracking data:", conditionMessage(e), "\n")
  })
} else {
  cat("No tracking data available to save.\n")
}
```

# Assign Taxonomy with Multiple Methods

```{r taxonomy-setup}
# Load additional libraries for taxonomy assignment
if (!requireNamespace("DECIPHER", quietly = TRUE)) {
  cat("Installing DECIPHER package for IDTAXA method...\n")
  if (!requireNamespace("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
  }
  BiocManager::install("DECIPHER", update = FALSE)
}

if (!requireNamespace("taxize", quietly = TRUE)) {
  cat("Installing taxize package for taxonomic name resolution...\n")
  install.packages("taxize")
}

# Create directory for reference databases if it doesn't exist
if(!dir.exists("ref_db")) dir.create("ref_db")

# Function to download and manage reference databases
download_ref_db <- function(db_name) {
  cat("Setting up", db_name, "reference database...\n")
  
  # Database-specific setup
  if (db_name == "SILVA") {
    # Check if Silva files are available in the DADA2 package
    silva_train <- system.file("extdata", "silva_nr99_v138.1_train_set.fa.gz", package="dada2")
    silva_species <- system.file("extdata", "silva_species_assignment_v138.1.fa.gz", package="dada2")
    
    silva_files_in_package <- (file.exists(silva_train) && file.exists(silva_species))
    
    if(silva_files_in_package) {
      cat("Using Silva database files from DADA2 package\n")
    } else {
      cat("Silva files not found in DADA2 package, will download from Zenodo\n")
      
      # URLs for Silva files from Zenodo
      silva_url_base <- "https://zenodo.org/records/4587955/files/"
      silva_train_url <- paste0(silva_url_base, "silva_nr99_v138.1_train_set.fa.gz")
      silva_species_url <- paste0(silva_url_base, "silva_species_assignment_v138.1.fa.gz")
      
      # Local file paths for Silva files
      silva_train <- file.path("ref_db", "silva_train_set.fa.gz")
      silva_species <- file.path("ref_db", "silva_species.fa.gz")
      
      # Download training set if needed
      if(!file.exists(silva_train)) {
        cat("Downloading Silva taxonomy training file...\n")
        # Set extended timeout
        options(timeout = max(300, getOption("timeout")))
        
        tryCatch({
          download.file(silva_train_url, silva_train, method="auto", mode="wb")
          cat("Download successful!\n")
        }, error = function(e) {
          cat("Error downloading Silva taxonomy file:", conditionMessage(e), "\n")
          cat("Please try downloading it manually from:", silva_train_url, "\n")
          cat("and save it to:", silva_train, "\n")
          stop("Download failed. Please download files manually.")
        })
      }
      
      # Download species file if needed
      if(!file.exists(silva_species)) {
        cat("Downloading Silva species assignment file...\n")
        
        tryCatch({
          download.file(silva_species_url, silva_species, method="auto", mode="wb")
          cat("Download successful!\n")
        }, error = function(e) {
          cat("Error downloading Silva species file:", conditionMessage(e), "\n")
          cat("Please try downloading it manually from:", silva_species_url, "\n")
          cat("and save it to:", silva_species, "\n")
          stop("Download failed. Please download files manually.")
        })
      }
    }
    
    # Check if files exist before proceeding
    if(!file.exists(silva_train) || !file.exists(silva_species)) {
      stop("Required Silva files not available. Please download them manually as noted above.")
    }
    
    return(list(
      train = silva_train,
      species = silva_species
    ))
  }
  
  # Setup for IDTAXA/DECIPHER SILVA classifier
  else if (db_name == "IDTAXA_SILVA") {
    silva_idtaxa <- file.path("ref_db", "SILVA_SSU_r138_2019.RData")
    
    if (!file.exists(silva_idtaxa)) {
      cat("Downloading SILVA SSU r138 for IDTAXA...\n")
      
      # Create temporary directory for download
      temp_dir <- tempdir()
      download_url <- "https://zenodo.org/records/3986799/files/SILVA_SSU_r138_2019.RData"
      
      tryCatch({
        # Set extended timeout
        options(timeout = max(600, getOption("timeout")))
        download.file(download_url, silva_idtaxa, mode = "wb")
        cat("Download successful!\n")
      }, error = function(e) {
        cat("Error downloading SILVA for IDTAXA:", conditionMessage(e), "\n")
        cat("Please download it manually from:", download_url, "\n")
        cat("and save it to:", silva_idtaxa, "\n")
        stop("Download failed. Please download files manually.")
      })
    } else {
      cat("Using existing SILVA IDTAXA training set\n")
    }
    
    return(list(
      train = silva_idtaxa
    ))
  }
  
  # Setup for RDP classifier/database
  else if (db_name == "RDP") {
    rdp_train <- file.path("ref_db", "rdp_train_set_18.fa.gz")
    rdp_species <- file.path("ref_db", "rdp_species_assignment_18.fa.gz")
    
    if (!file.exists(rdp_train) || !file.exists(rdp_species)) {
      cat("Downloading RDP training files...\n")
      
      # URLs for RDP files
      rdp_base_url <- "https://zenodo.org/records/4587955/files/"
      rdp_train_url <- paste0(rdp_base_url, "rdp_train_set_18.fa.gz")
      rdp_species_url <- paste0(rdp_base_url, "rdp_species_assignment_18.fa.gz")
      
      # Download training files
      options(timeout = max(300, getOption("timeout")))
      
      if (!file.exists(rdp_train)) {
        tryCatch({
          download.file(rdp_train_url, rdp_train, method = "auto", mode = "wb")
          cat("RDP training set downloaded successfully!\n")
        }, error = function(e) {
          cat("Error downloading RDP training set:", conditionMessage(e), "\n")
          stop("Download failed. Please download files manually.")
        })
      }
      
      if (!file.exists(rdp_species)) {
        tryCatch({
          download.file(rdp_species_url, rdp_species, method = "auto", mode = "wb")
          cat("RDP species assignment file downloaded successfully!\n")
        }, error = function(e) {
          cat("Error downloading RDP species file:", conditionMessage(e), "\n")
          stop("Download failed. Please download files manually.")
        })
      }
    } else {
      cat("Using existing RDP database files\n")
    }
    
    return(list(
      train = rdp_train,
      species = rdp_species
    ))
  }
  
  # If unknown database is requested
  else {
    stop("Unknown database: ", db_name)
  }
}

# Get standard taxonomy names across databases
standardize_taxonomy <- function(taxa_df, method) {
  # Make all column names consistent
  standard_cols <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
  
  # IDTAXA specific standardization
  if (method == "IDTAXA") {
    if ("rootrank" %in% colnames(taxa_df)) {
      colnames(taxa_df)[colnames(taxa_df) == "rootrank"] <- "Kingdom"
    }
    if ("domain" %in% colnames(taxa_df)) {
      colnames(taxa_df)[colnames(taxa_df) == "domain"] <- "Kingdom"
    }
  }
  
  # RDP specific standardization
  if (method == "RDP") {
    # RDP sometimes uses Root instead of Kingdom
    if ("Root" %in% colnames(taxa_df)) {
      colnames(taxa_df)[colnames(taxa_df) == "Root"] <- "Kingdom"
    }
  }
  
  # Ensure all standard columns exist, filling with NA if missing
  for (col in standard_cols) {
    if (!col %in% colnames(taxa_df)) {
      taxa_df[[col]] <- NA
    }
  }
  
  # Select only standard columns and in the right order
  taxa_df <- taxa_df[, standard_cols]
  
  return(taxa_df)
}

# This function will be used to summarize taxonomy assignment across methods
summarize_taxonomy <- function(taxa_list) {
  # Create a data frame to store summary
  summary_df <- data.frame(
    Method = character(),
    Kingdom = numeric(),
    Phylum = numeric(),
    Class = numeric(),
    Order = numeric(),
    Family = numeric(),
    Genus = numeric(),
    Species = numeric()
  )
  
  # Calculate assignments for each method
  for (method_name in names(taxa_list)) {
    taxa_df <- taxa_list[[method_name]]
    
    # Calculate percentage assigned at each level
    method_row <- c(method_name)
    for (level in colnames(taxa_df)[-1]) { # Skip the ASV_ID column
      percent <- round(sum(!is.na(taxa_df[[level]])) / nrow(taxa_df) * 100, 1)
      method_row <- c(method_row, percent)
    }
    
    # Add to summary data frame
    summary_df <- rbind(summary_df, method_row)
  }
  
  # Set column names
  colnames(summary_df) <- c("Method", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
  
  # Convert percentage columns to numeric
  for (col in colnames(summary_df)[-1]) {
    summary_df[[col]] <- as.numeric(summary_df[[col]])
  }
  
  return(summary_df)
}

# Function to plot taxonomy assignment comparison
plot_taxonomy_comparison <- function(summary_df) {
  # Reshape data for plotting
  summary_long <- tidyr::pivot_longer(
    summary_df, 
    cols = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    names_to = "TaxonomicLevel", 
    values_to = "PercentAssigned"
  )
  
  # Create the comparison plot
  ggplot(summary_long, aes(x = TaxonomicLevel, y = PercentAssigned, fill = Method)) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
    geom_text(aes(label = paste0(PercentAssigned, "%")), 
              position = position_dodge(width = 0.9), 
              vjust = -0.5, size = 3) +
    theme_minimal() +
    labs(title = "Taxonomic Assignment Comparison Across Methods",
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_fill_brewer(palette = "Set1")
}

# Choose which taxonomic classification methods to use
# Set to TRUE/FALSE to enable/disable methods
taxonomy_methods <- list(
  SILVA_DADA2 = TRUE,      # DADA2's implementation with SILVA database
  IDTAXA_SILVA = TRUE,     # IDTAXA classifier with SILVA
  RDP_DADA2 = FALSE        # DADA2's implementation with RDP database
)

# Storage for taxonomy results
taxa_results <- list()
```

```{r silva-taxonomy}
# ===================================================================
# Enhanced Taxonomy Assignment with Confidence Scores
# ===================================================================
# 
# This section implements reference-based taxonomy confidence scoring 
# functionality to improve taxonomy assignment reliability assessment.
#
# Key enhancements include:
#
# 1. Bootstrap confidence scores for taxonomic assignments
#    - Uses DADA2's bootstrap confidence estimation
#    - Reports confidence percentages (0-100%) for each taxonomic level
#    - Provides species-level confidence for exact matches
#
# 2. Visualization of confidence scores
#    - Distribution of confidence values by taxonomic level
#    - Identification of low-confidence assignments
#    - Summary statistics of assignment confidence
#
# 3. Confidence-based taxonomy filtering
#    - Optional filtering based on confidence thresholds
#    - Customizable thresholds by taxonomic level
#    - Comparison between raw and filtered assignments
#
# 4. Data export
#    - Taxonomic assignments with confidence scores
#    - Filtered taxonomy based on confidence thresholds
#    - Visualization outputs for reports
#
# The confidence scores help researchers:
# - Assess reliability of taxonomic assignments
# - Identify potentially problematic assignments
# - Apply appropriate filtering based on research needs
# - Report assignment quality metrics in publications
#
# Confidence thresholds can be customized based on study requirements,
# with more stringent thresholds typically applied at lower taxonomic
# levels where assignment uncertainty is higher.
# ===================================================================

# DADA2 with SILVA database (default method)
if (taxonomy_methods$SILVA_DADA2) {
  cat("\n=== Method 1: DADA2 with SILVA database (with confidence scores) ===\n")
  
  # Download/locate SILVA database
  silva_files <- download_ref_db("SILVA")
  
  # Assign taxonomy at the genus level with optimized parallelization and include confidence scores
  cat("Assigning taxonomy using Silva database with confidence scores and optimized parallelization...\n")

  # For large datasets, use batched taxonomy assignment
  if(ncol(seqtab.nochim) > 3000) {
    cat("Using batched approach for taxonomy assignment due to large number of ASVs...\n")
    
    # Split ASVs into batches
    batch_size <- 1000  # Process 1000 ASVs at a time
    asv_sequences <- colnames(seqtab.nochim)
    n_batches <- ceiling(length(asv_sequences) / batch_size)
    
    # Initialize results matrices
    taxa_silva <- NULL
    taxa_silva_confidence <- NULL
    
    # Process each batch
    for(i in 1:n_batches) {
      cat(sprintf("Processing taxonomy batch %d of %d...\n", i, n_batches))
      
      # Calculate batch indices
      start_idx <- (i-1) * batch_size + 1
      end_idx <- min(i * batch_size, length(asv_sequences))
      
      # Extract batch
      batch_seqs <- asv_sequences[start_idx:end_idx]
      
      # Create a mini sequence table with just these ASVs
      batch_seqtab <- seqtab.nochim[, batch_seqs, drop=FALSE]
      
      # Assign taxonomy for this batch with confidence scores
      batch_taxa_result <- assignTaxonomy(batch_seqtab, silva_files$train, multithread=TRUE, 
                                         outputBootstraps=TRUE)
      
      # Extract taxonomic assignments and bootstrap confidence values
      batch_taxa <- batch_taxa_result$tax
      batch_confidence <- batch_taxa_result$boot
      
      # Add species assignments with confidence scores
      batch_species_result <- addSpecies(batch_taxa, silva_files$species, allowMultiple=FALSE,
                                        tryRC=TRUE, verbose=TRUE)
      
      # Prepare species-level confidence (assumed 100% for exact matches, NA for no match)
      n_asvs <- nrow(batch_taxa)
      species_confidence <- rep(NA, n_asvs)
      species_confidence[!is.na(batch_species_result[, "Species"])] <- 100
      
      # Add species confidence to the bootstrap confidence matrix
      batch_confidence <- cbind(batch_confidence, Species=species_confidence)
      
      # Combine with previous results
      if(is.null(taxa_silva)) {
        taxa_silva <- batch_species_result
        taxa_silva_confidence <- batch_confidence
      } else {
        taxa_silva <- rbind(taxa_silva, batch_species_result)
        taxa_silva_confidence <- rbind(taxa_silva_confidence, batch_confidence)
      }
      
      # Force garbage collection
      gc()
    }
  } else {
    # For smaller datasets, use the standard approach with confidence scores
    cat("Assigning taxonomy with bootstrap confidence values...\n")
    taxa_result <- assignTaxonomy(seqtab.nochim, silva_files$train, multithread=TRUE, 
                                  outputBootstraps=TRUE)
    
    # Extract taxonomic assignments and bootstrap confidence values
    taxa_silva <- taxa_result$tax
    taxa_silva_confidence <- taxa_result$boot
    
    # Add species-level assignments
    cat("Adding species-level assignments...\n")
    taxa_silva <- addSpecies(taxa_silva, silva_files$species, allowMultiple=FALSE,
                            tryRC=TRUE, verbose=TRUE)
    
    # Prepare species-level confidence (assumed 100% for exact matches, NA for no match)
    n_asvs <- nrow(taxa_silva)
    species_confidence <- rep(NA, n_asvs)
    species_confidence[!is.na(taxa_silva[, "Species"])] <- 100
    
    # Add species confidence to the bootstrap confidence matrix
    taxa_silva_confidence <- cbind(taxa_silva_confidence, Species=species_confidence)
  }
  
  # View taxonomic assignments and confidence scores
  cat("First few taxonomic assignments with confidence scores:\n")
  print(head(taxa_silva))
  cat("\nCorresponding confidence scores (bootstrap support %):\n")
  print(head(taxa_silva_confidence))
  
  # Create a combined data frame with taxa and confidence for easier visualization and export
  taxa_with_confidence <- as.data.frame(taxa_silva)
  taxa_with_confidence$ASV_ID <- rownames(taxa_with_confidence)
  
  # Create confidence data frame
  confidence_df <- as.data.frame(taxa_silva_confidence)
  confidence_df$ASV_ID <- rownames(confidence_df)
  
  # Add confidence score columns with prefix "Confidence_"
  for (level in c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")) {
    if (level %in% colnames(confidence_df)) {
      confidence_col_name <- paste0("Confidence_", level)
      taxa_with_confidence[[confidence_col_name]] <- confidence_df[[level]]
    }
  }
  
  # Save to results list
  taxa_results$SILVA_DADA2 <- taxa_silva
  taxa_results$SILVA_DADA2_confidence <- taxa_silva_confidence
  taxa_results$SILVA_DADA2_combined <- taxa_with_confidence
  
  # Export the combined taxonomy and confidence scores
  if (!dir.exists("results")) dir.create("results")
  write.csv(taxa_with_confidence, "results/taxonomy_with_confidence.csv", row.names = FALSE)
  cat("Saved taxonomy with confidence scores to results/taxonomy_with_confidence.csv\n")
  
  # Create a taxonomy summary for SILVA
  silva_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_silva[, "Kingdom"])),
      sum(!is.na(taxa_silva[, "Phylum"])),
      sum(!is.na(taxa_silva[, "Class"])),
      sum(!is.na(taxa_silva[, "Order"])),
      sum(!is.na(taxa_silva[, "Family"])),
      sum(!is.na(taxa_silva[, "Genus"])),
      sum(!is.na(taxa_silva[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_silva[, "Kingdom"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Phylum"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Class"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Order"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Family"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Genus"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Species"])) / nrow(taxa_silva) * 100, 1)
    ),
    MeanConfidence = c(
      mean(taxa_silva_confidence[, "Kingdom"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Phylum"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Class"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Order"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Family"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Genus"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Species"], na.rm = TRUE)
    )
  )
  
  # Display SILVA summary table
  kable(silva_summary, caption = "SILVA Taxonomic Assignment Summary with Confidence Scores")
  
  # Visualize confidence scores distribution by taxonomic level
  confidence_long <- tidyr::pivot_longer(
    as.data.frame(taxa_silva_confidence),
    cols = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    names_to = "TaxonomicLevel",
    values_to = "Confidence"
  )
  
  # Remove NA values
  confidence_long <- confidence_long[!is.na(confidence_long$Confidence), ]
  
  # Order taxonomic levels by assignment hierarchy
  confidence_long$TaxonomicLevel <- factor(confidence_long$TaxonomicLevel, 
                                        levels = c("Kingdom", "Phylum", "Class", 
                                                  "Order", "Family", "Genus", "Species"))
  
  # Create violin plot showing confidence score distributions
  confidence_violin_plot <- ggplot(confidence_long, aes(x = TaxonomicLevel, y = Confidence, fill = TaxonomicLevel)) +
    geom_violin(alpha = 0.7) +
    geom_boxplot(width = 0.1, fill = "white", color = "darkgray") +
    stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
    scale_y_continuous(limits = c(0, 100)) +
    labs(title = "Distribution of Taxonomy Assignment Confidence Scores",
         x = "Taxonomic Level",
         y = "Bootstrap Confidence (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "none")
  
  print(confidence_violin_plot)
  
  # Create heatmap for low confidence assignments
  # Calculate confidence threshold
  confidence_threshold <- 80
  
  # Filter for low confidence assignments
  low_confidence <- confidence_long %>%
    filter(Confidence < confidence_threshold) %>%
    group_by(TaxonomicLevel) %>%
    summarize(Count = n(),
              Percentage = round(n() / sum(!is.na(taxa_silva_confidence[, TaxonomicLevel])) * 100, 1))
  
  if (nrow(low_confidence) > 0) {
    # Plot low confidence summary
    low_confidence_plot <- ggplot(low_confidence, aes(x = TaxonomicLevel, y = Percentage, fill = TaxonomicLevel)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = paste0(Percentage, "%\n(n=", Count, ")")), vjust = -0.5) +
      labs(title = paste0("Low Confidence (<", confidence_threshold, "%) Assignments by Taxonomic Level"),
           x = "Taxonomic Level",
           y = "Percentage of Assignments") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            legend.position = "none")
    
    print(low_confidence_plot)
  } else {
    cat("No low confidence assignments found (<", confidence_threshold, "%).\n")
  }
  
  # Create a bar plot for SILVA results with confidence info
  # Order the data by percent assigned (descending)
  silva_summary$TaxonomicLevel <- factor(silva_summary$TaxonomicLevel, 
                                       levels = silva_summary$TaxonomicLevel[order(silva_summary$PercentAssigned, decreasing = TRUE)])
  
  # Create the plot with confidence overlay
  silva_plot <- ggplot(silva_summary, aes(x = TaxonomicLevel, y = PercentAssigned)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(PercentAssigned, "%")), vjust = -0.3) +
    geom_text(aes(label = paste0("(", round(MeanConfidence, 1), "% conf.)")), vjust = 1.5, color = "darkblue") +
    theme_minimal() +
    labs(title = "SILVA: Percentage of ASVs with Taxonomic Assignment",
         subtitle = "Mean confidence score shown in parentheses",
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, max(silva_summary$PercentAssigned) * 1.2))
  
  print(silva_plot)
  
  # Save visualization files
  ggsave("results/confidence_distribution.pdf", confidence_violin_plot, width = 8, height = 6)
  ggsave("results/confidence_distribution.png", confidence_violin_plot, width = 8, height = 6, dpi = 300)
  
  if (nrow(low_confidence) > 0) {
    ggsave("results/low_confidence_summary.pdf", low_confidence_plot, width = 8, height = 6)
    ggsave("results/low_confidence_summary.png", low_confidence_plot, width = 8, height = 6, dpi = 300)
  }
  
  ggsave("results/taxonomy_assignment_with_confidence.pdf", silva_plot, width = 8, height = 6)
  ggsave("results/taxonomy_assignment_with_confidence.png", silva_plot, width = 8, height = 6, dpi = 300)
}
```

```{r idtaxa-taxonomy}
# IDTAXA with SILVA database
if (taxonomy_methods$IDTAXA_SILVA) {
  cat("\n=== Method 2: IDTAXA with SILVA database ===\n")
  
  # Download/locate SILVA database for IDTAXA
  idtaxa_files <- download_ref_db("IDTAXA_SILVA")
  
  # Load the training set
  cat("Loading IDTAXA training set...\n")
  load(idtaxa_files$train)
  
  # Convert ASV sequences to DNAStringSet
  dna_seqs <- Biostrings::DNAStringSet(getSequences(seqtab.nochim))
  names(dna_seqs) <- paste0("ASV", seq_along(dna_seqs))
  
  # Run IDTAXA classification with optimized parallelization
  cat("Running IDTAXA classification with optimized parallelization...\n")
  
  # Determine optimal number of processors for IDTAXA
  num_processors <- min(parallel::detectCores() - 1, 8)
  cat(sprintf("Using %d processors for IDTAXA classification...\n", num_processors))
  
  # For large datasets, use batched approach
  if(length(dna_seqs) > 3000) {
    cat("Using batched approach for IDTAXA due to large number of sequences...\n")
    
    # Split sequences into batches
    batch_size <- 1000  # Process 1000 sequences at a time
    n_batches <- ceiling(length(dna_seqs) / batch_size)
    
    # Initialize results list
    ids <- vector("list", length(dna_seqs))
    names(ids) <- names(dna_seqs)
    
    # Process each batch
    for(i in 1:n_batches) {
      cat(sprintf("Processing IDTAXA batch %d of %d...\n", i, n_batches))
      
      # Calculate batch indices
      start_idx <- (i-1) * batch_size + 1
      end_idx <- min(i * batch_size, length(dna_seqs))
      
      # Extract batch
      batch_seqs <- dna_seqs[start_idx:end_idx]
      
      # Run IDTAXA on this batch
      batch_ids <- DECIPHER::IdTaxa(batch_seqs, trainingSet, strand = "both", 
                                   processors = num_processors, threshold = 50)
      
      # Store in the correct positions in the results list
      ids[names(batch_seqs)] <- batch_ids
      
      # Force garbage collection
      gc()
    }
  } else {
    # For smaller datasets, use standard approach
    ids <- DECIPHER::IdTaxa(dna_seqs, trainingSet, strand = "both", 
                          processors = num_processors, threshold = 50)
  }
  
  # Function to convert IDTAXA results to a taxonomy table similar to DADA2's format
  convert_idtaxa_to_table <- function(ids) {
    # Get all possible ranks
    all_ranks <- unique(unlist(lapply(ids, function(x) names(x$taxon))))
    
    # Convert to data frame
    taxa_df <- data.frame(matrix(NA, nrow = length(ids), ncol = length(all_ranks)))
    colnames(taxa_df) <- all_ranks
    rownames(taxa_df) <- names(ids)
    
    # Fill in the data frame
    for (i in seq_along(ids)) {
      id <- ids[[i]]
      for (rank in names(id$taxon)) {
        if (id$confidence[rank] >= 50) {  # Only keep assignments with ≥50% confidence
          taxa_df[i, rank] <- id$taxon[rank]
        }
      }
    }
    
    return(taxa_df)
  }
  
  # Convert IDTAXA results to taxonomy table
  taxa_idtaxa <- convert_idtaxa_to_table(ids)
  
  # Standardize taxonomy columns
  taxa_idtaxa <- standardize_taxonomy(taxa_idtaxa, "IDTAXA")
  
  # View IDTAXA taxonomic assignments
  cat("First few IDTAXA taxonomic assignments:\n")
  print(head(taxa_idtaxa))
  
  # Save to results list
  taxa_results$IDTAXA_SILVA <- taxa_idtaxa
  
  # Create a taxonomy summary for IDTAXA
  idtaxa_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_idtaxa[, "Kingdom"])),
      sum(!is.na(taxa_idtaxa[, "Phylum"])),
      sum(!is.na(taxa_idtaxa[, "Class"])),
      sum(!is.na(taxa_idtaxa[, "Order"])),
      sum(!is.na(taxa_idtaxa[, "Family"])),
      sum(!is.na(taxa_idtaxa[, "Genus"])),
      sum(!is.na(taxa_idtaxa[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_idtaxa[, "Kingdom"])) / nrow(taxa_idtaxa) * 100, 1),
      round(sum(!is.na(taxa_idtaxa[, "Phylum"])) / nrow(taxa_idtaxa) * 100, 1),
      round(sum(!is.na(taxa_idtaxa[, "Class"])) / nrow(taxa_idtaxa) * 100, 1),
      round(sum(!is.na(taxa_idtaxa[, "Order"])) / nrow(taxa_idtaxa) * 100, 1),
      round(sum(!is.na(taxa_idtaxa[, "Family"])) / nrow(taxa_idtaxa) * 100, 1),
      round(sum(!is.na(taxa_idtaxa[, "Genus"])) / nrow(taxa_idtaxa) * 100, 1),
      round(sum(!is.na(taxa_idtaxa[, "Species"])) / nrow(taxa_idtaxa) * 100, 1)
    )
  )
  
  # Display IDTAXA summary table
  kable(idtaxa_summary, caption = "IDTAXA Taxonomic Assignment Summary")
  
  # Create a bar plot for IDTAXA results
  idtaxa_summary$TaxonomicLevel <- factor(idtaxa_summary$TaxonomicLevel, 
                                       levels = idtaxa_summary$TaxonomicLevel[order(idtaxa_summary$PercentAssigned, decreasing = TRUE)])
  
  idtaxa_plot <- ggplot(idtaxa_summary, aes(x = TaxonomicLevel, y = PercentAssigned)) +
    geom_bar(stat = "identity", fill = "forestgreen") +
    geom_text(aes(label = paste0(PercentAssigned, "%")), vjust = -0.3) +
    theme_minimal() +
    labs(title = "IDTAXA: Percentage of ASVs with Taxonomic Assignment",
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, max(idtaxa_summary$PercentAssigned) * 1.1))
  
  print(idtaxa_plot)
}
```

```{r rdp-taxonomy, eval=taxonomy_methods$RDP_DADA2}
# DADA2 with RDP database (optional method)
if (taxonomy_methods$RDP_DADA2) {
  cat("\n=== Method 3: DADA2 with RDP database ===\n")
  
  # Download/locate RDP database
  rdp_files <- download_ref_db("RDP")
  
  # Assign taxonomy with RDP
  cat("Assigning taxonomy using RDP database...\n")
  taxa_rdp <- assignTaxonomy(seqtab.nochim, rdp_files$train, multithread=TRUE)
  
  # Add species-level assignments
  cat("Adding species-level assignments...\n")
  taxa_rdp <- addSpecies(taxa_rdp, rdp_files$species)
  
  # View RDP taxonomic assignments
  cat("First few RDP taxonomic assignments:\n")
  print(head(taxa_rdp))
  
  # Save to results list
  taxa_results$RDP_DADA2 <- taxa_rdp
  
  # Create a taxonomy summary for RDP
  rdp_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_rdp[, "Kingdom"])),
      sum(!is.na(taxa_rdp[, "Phylum"])),
      sum(!is.na(taxa_rdp[, "Class"])),
      sum(!is.na(taxa_rdp[, "Order"])),
      sum(!is.na(taxa_rdp[, "Family"])),
      sum(!is.na(taxa_rdp[, "Genus"])),
      sum(!is.na(taxa_rdp[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_rdp[, "Kingdom"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Phylum"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Class"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Order"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Family"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Genus"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Species"])) / nrow(taxa_rdp) * 100, 1)
    )
  )
  
  # Display RDP summary table
  kable(rdp_summary, caption = "RDP Taxonomic Assignment Summary")
}
```

```{r taxonomy-comparison}
# Compare taxonomy assignments from different methods
if (length(taxa_results) > 1) {
  cat("\n=== Comparing Taxonomy Assignments Across Methods ===\n")
  
  # Create taxonomy assignment summary across methods
  taxonomy_summary <- summarize_taxonomy(taxa_results)
  
  # Display taxonomy comparison table
  kable(taxonomy_summary, caption = "Taxonomic Assignment Comparison Across Methods")
  
  # Create a comparative visualization
  comparison_plot <- plot_taxonomy_comparison(taxonomy_summary)
  print(comparison_plot)
  
  # Create a consensus taxonomy
  cat("\nCreating consensus taxonomy from all methods...\n")
  
  # Function to create consensus taxonomy
  create_consensus_taxonomy <- function(taxa_list, min_methods = 2) {
    # Get all ASV identifiers
    all_asvs <- rownames(taxa_list[[1]])
    
    # Initialize consensus taxonomy matrix
    consensus <- matrix(NA, nrow = length(all_asvs), ncol = 7)
    colnames(consensus) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
    rownames(consensus) <- all_asvs
    
    # For each taxonomic level and ASV
    for (level in colnames(consensus)) {
      for (asv in all_asvs) {
        # Collect assignments from all methods
        assignments <- sapply(taxa_list, function(x) x[asv, level])
        
        # Remove NAs
        assignments <- assignments[!is.na(assignments)]
        
        # If we have enough assignments, find the most common one
        if (length(assignments) >= min_methods) {
          # Count occurrences of each assignment
          counts <- table(assignments)
          
          # Get the most common assignment
          most_common <- names(counts)[which.max(counts)]
          
          # Only use it if it appears in at least min_methods
          if (counts[most_common] >= min_methods) {
            consensus[asv, level] <- most_common
          }
        }
      }
    }
    
    return(as.data.frame(consensus))
  }
  
  # Create consensus taxonomy
  min_methods <- min(2, length(taxa_results))  # At least 2 methods must agree, or all if fewer than 2
  taxa_consensus <- create_consensus_taxonomy(taxa_results, min_methods)
  
  # Summarize consensus taxonomy
  consensus_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_consensus[, "Kingdom"])),
      sum(!is.na(taxa_consensus[, "Phylum"])),
      sum(!is.na(taxa_consensus[, "Class"])),
      sum(!is.na(taxa_consensus[, "Order"])),
      sum(!is.na(taxa_consensus[, "Family"])),
      sum(!is.na(taxa_consensus[, "Genus"])),
      sum(!is.na(taxa_consensus[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_consensus[, "Kingdom"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Phylum"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Class"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Order"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Family"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Genus"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Species"])) / nrow(taxa_consensus) * 100, 1)
    )
  )
  
  # Display consensus summary table
  kable(consensus_summary, caption = paste0("Consensus Taxonomy (Minimum ", min_methods, " Methods Agreement)"))
  
  # Create a bar plot for consensus results
  consensus_summary$TaxonomicLevel <- factor(consensus_summary$TaxonomicLevel, 
                                          levels = consensus_summary$TaxonomicLevel[order(consensus_summary$PercentAssigned, decreasing = TRUE)])
  
  consensus_plot <- ggplot(consensus_summary, aes(x = TaxonomicLevel, y = PercentAssigned)) +
    geom_bar(stat = "identity", fill = "purple") +
    geom_text(aes(label = paste0(PercentAssigned, "%")), vjust = -0.3) +
    theme_minimal() +
    labs(title = "Consensus: Percentage of ASVs with Taxonomic Assignment",
         subtitle = paste0("Minimum ", min_methods, " methods must agree"),
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, max(consensus_summary$PercentAssigned) * 1.1))
  
  print(consensus_plot)
  
  # Use the consensus taxonomy as the final result if it exists
  if (nrow(taxa_consensus) > 0) {
    taxa <- as.matrix(taxa_consensus)
    cat("Using consensus taxonomy for final results\n")
  } else {
    # Use the first method as backup
    taxa <- taxa_results[[1]]
    cat("Using", names(taxa_results)[1], "for final results (consensus not available)\n")
  }
} else if (length(taxa_results) == 1) {
  # If only one method was used, use its results
  taxa <- taxa_results[[1]]
  cat("Using", names(taxa_results)[1], "for final results\n")
} else {
  # Error if no taxonomy method was successful
  stop("No taxonomy assignment method was successful!")
}

# Function to filter taxonomy based on confidence thresholds
filter_taxonomy_by_confidence <- function(taxa, confidence, thresholds = NULL) {
  # Default thresholds by taxonomic level - becomes more stringent at lower levels
  if (is.null(thresholds)) {
    thresholds <- c(
      Kingdom = 50,
      Phylum = 60, 
      Class = 70,
      Order = 75,
      Family = 80,
      Genus = 85,
      Species = 95  # Very high threshold for species
    )
  }
  
  # Create a copy of the taxonomy
  filtered_taxa <- taxa
  
  # Filter each level based on confidence
  for (level in names(thresholds)) {
    if (level %in% colnames(confidence)) {
      threshold <- thresholds[level]
      mask <- !is.na(confidence[, level]) & confidence[, level] < threshold
      filtered_taxa[mask, level] <- NA
      cat(sprintf("Filtered %d %s-level assignments below %d%% confidence\n", 
                 sum(mask), level, threshold))
    }
  }
  
  return(filtered_taxa)
}

# Function to add confidence-filtered taxonomy to a phyloseq object
add_confidence_filtered_taxonomy <- function(ps, taxa_filtered) {
  # Check if input phyloseq has tax_table
  if (is.null(tax_table(ps, errorIfNULL=FALSE))) {
    stop("Input phyloseq object does not have a taxonomy table")
  }
  
  # Ensure tax_table and filtered_taxa have the same dimensions and taxa
  if (!identical(dim(tax_table(ps)), dim(taxa_filtered)) || 
      !all(taxa_names(ps) == rownames(taxa_filtered))) {
    stop("Filtered taxonomy does not match the dimensions or taxa in the phyloseq object")
  }
  
  # Create a copy of the phyloseq object
  ps_filtered <- ps
  
  # Replace tax_table with filtered version
  tax_table(ps_filtered) <- tax_table(as.matrix(taxa_filtered))
  
  return(ps_filtered)
}

# Example of using confidence filtering if confidence scores are available
if (exists("taxa_silva") && exists("taxa_silva_confidence")) {
  cat("\n=== Example: Filtering Taxonomy by Confidence Scores ===\n")
  
  # Define custom confidence thresholds (optional)
  custom_thresholds <- c(
    Kingdom = 60,
    Phylum = 70, 
    Class = 75,
    Order = 80,
    Family = 85,
    Genus = 90,
    Species = 98  # Very high threshold for species
  )
  
  # Filter taxonomy using custom thresholds
  taxa_silva_filtered <- filter_taxonomy_by_confidence(
    taxa_silva, 
    taxa_silva_confidence,
    thresholds = custom_thresholds
  )
  
  # Compare original and filtered taxonomy
  compare_df <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    Original = c(
      sum(!is.na(taxa_silva[, "Kingdom"])),
      sum(!is.na(taxa_silva[, "Phylum"])),
      sum(!is.na(taxa_silva[, "Class"])),
      sum(!is.na(taxa_silva[, "Order"])),
      sum(!is.na(taxa_silva[, "Family"])),
      sum(!is.na(taxa_silva[, "Genus"])),
      sum(!is.na(taxa_silva[, "Species"]))
    ),
    Filtered = c(
      sum(!is.na(taxa_silva_filtered[, "Kingdom"])),
      sum(!is.na(taxa_silva_filtered[, "Phylum"])),
      sum(!is.na(taxa_silva_filtered[, "Class"])),
      sum(!is.na(taxa_silva_filtered[, "Order"])),
      sum(!is.na(taxa_silva_filtered[, "Family"])),
      sum(!is.na(taxa_silva_filtered[, "Genus"])),
      sum(!is.na(taxa_silva_filtered[, "Species"]))
    )
  )
  
  # Calculate percentage removed
  compare_df$PercentRemoved <- round((compare_df$Original - compare_df$Filtered) / compare_df$Original * 100, 1)
  
  # Display comparison table
  kable(compare_df, caption = "Comparison of Taxonomic Assignments Before and After Confidence Filtering")
  
  # Plot the comparison
  compare_long <- tidyr::pivot_longer(
    compare_df,
    cols = c("Original", "Filtered"),
    names_to = "Dataset",
    values_to = "Count"
  )
  
  # Order taxonomic levels
  compare_long$TaxonomicLevel <- factor(compare_long$TaxonomicLevel, 
                                      levels = c("Kingdom", "Phylum", "Class", 
                                               "Order", "Family", "Genus", "Species"))
  
  # Create comparison plot
  comparison_plot <- ggplot(compare_long, aes(x = TaxonomicLevel, y = Count, fill = Dataset)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    geom_text(aes(label = Count), position = position_dodge(width = 0.9), vjust = -0.5) +
    labs(title = "Effect of Confidence Filtering on Taxonomic Assignments",
         subtitle = paste("Thresholds:", paste(names(custom_thresholds), custom_thresholds, sep = "=", collapse = ", ")),
         x = "Taxonomic Level",
         y = "Number of ASVs with Assignment") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(comparison_plot)
  
  # Export filtered taxonomy
  write.csv(as.data.frame(taxa_silva_filtered), "results/taxonomy_confidence_filtered.csv", row.names = TRUE)
  cat("Saved confidence-filtered taxonomy to results/taxonomy_confidence_filtered.csv\n")
  
  # Create phyloseq object with filtered taxonomy if phyloseq exists
  if (exists("ps")) {
    # Create a version with confidence-filtered taxonomy
    ps_filtered <- add_confidence_filtered_taxonomy(ps, taxa_silva_filtered)
    
    # Save the filtered phyloseq object
    saveRDS(ps_filtered, "results/phyloseq_confidence_filtered.rds")
    cat("Created and saved phyloseq object with confidence-filtered taxonomy to results/phyloseq_confidence_filtered.rds\n")
  }
}

# Save all taxonomy results
if(!dir.exists("results")) dir.create("results")
saveRDS(taxa_results, "results/all_taxonomy_results.rds")
cat("Saved all taxonomy results to results/all_taxonomy_results.rds\n")

# Save the final consensus taxonomy as CSV
write.csv(taxa, "results/taxonomy.csv")
cat("Saved final taxonomy to results/taxonomy.csv\n")

# Create checkpoint after taxonomy assignment step
save_checkpoint("after_taxonomy", 
               c("seqtab.nochim", "taxa", "taxa_results", "sample.names", "track_df"),
               overwrite = TRUE)
cat("Checkpoint saved after taxonomy assignment step\n")
```

# Create Phyloseq Object

```{r phyloseq}
# Create a directory for results if it doesn't exist
if(!dir.exists("results")) dir.create("results")

# Create sample data frame
cat("Setting up metadata handling...\n")

# Function for standardizing sample names
standardize_sample_names <- function(names, pattern = NULL) {
  # Remove common file extensions and read indicators
  clean_names <- gsub("_R[12].*|\\.fastq.*|\\.fq.*", "", names)
  
  # Apply custom regex if provided
  if (!is.null(pattern)) {
    clean_names <- gsub(pattern, "\\1", clean_names)
  }
  
  return(clean_names)
}

# Function to load and match metadata from external file
load_and_match_metadata <- function(metadata_path, seqtab, 
                                   sample_col = "SampleID",
                                   fuzzy_match = FALSE) {
  if (!file.exists(metadata_path)) {
    cat("Metadata file not found at:", metadata_path, "\n")
    cat("Falling back to sample name extraction\n")
    return(NULL)
  }
  
  cat("Loading metadata from:", metadata_path, "\n")
  metadata <- tryCatch({
    # Try to read CSV file
    read.csv(metadata_path, stringsAsFactors = FALSE)
  }, error = function(e) {
    # Try to read tab-delimited file if CSV fails
    tryCatch({
      read.delim(metadata_path, stringsAsFactors = FALSE)
    }, error = function(e2) {
      cat("Error reading metadata file:", conditionMessage(e2), "\n")
      return(NULL)
    })
  })
  
  # Check if metadata loaded successfully
  if (is.null(metadata) || nrow(metadata) == 0) {
    cat("Failed to load metadata or empty file\n")
    return(NULL)
  }
  
  # Check if sample_col exists in metadata
  if (!sample_col %in% colnames(metadata)) {
    cat("Sample column", sample_col, "not found in metadata\n")
    cat("Available columns:", paste(colnames(metadata), collapse=", "), "\n")
    return(NULL)
  }
  
  # Get standardized names from both sources
  seqtab_names <- standardize_sample_names(rownames(seqtab))
  metadata_names <- standardize_sample_names(metadata[[sample_col]])
  
  # Create mapping table
  name_map <- data.frame(
    seqtab_name = rownames(seqtab),
    seqtab_std = seqtab_names,
    stringsAsFactors = FALSE
  )
  
  # Add matched metadata names
  if (fuzzy_match) {
    # Check if stringdist package is available for fuzzy matching
    if (!requireNamespace("stringdist", quietly = TRUE)) {
      cat("stringdist package required for fuzzy matching. Installing...\n")
      install.packages("stringdist")
    }
    
    # Use string distance for fuzzy matching
    matches <- sapply(seqtab_names, function(x) {
      idx <- which.min(stringdist::stringdist(x, metadata_names))
      return(metadata[[sample_col]][idx])
    })
    name_map$metadata_match <- matches
    
    cat("Used fuzzy matching algorithm to match sample names\n")
  } else {
    # Direct matching with warning for missing samples
    matches <- match(seqtab_names, metadata_names)
    name_map$metadata_match <- metadata[[sample_col]][matches]
    if (any(is.na(matches))) {
      cat("WARNING: Missing metadata for samples:", 
          paste(seqtab_names[is.na(matches)], collapse=", "), "\n")
    }
  }
  
  # Return metadata in correct order with mapping table
  metadata_ordered <- metadata[match(name_map$metadata_match, metadata[[sample_col]]),]
  rownames(metadata_ordered) <- rownames(seqtab)
  
  # Print match summary
  cat("Metadata matching summary:\n")
  cat("- Total samples in sequence table:", nrow(seqtab), "\n")
  cat("- Total samples in metadata:", nrow(metadata), "\n")
  cat("- Successfully matched samples:", sum(!is.na(name_map$metadata_match)), "\n")
  
  return(list(metadata = metadata_ordered, mapping = name_map))
}

# First check if external metadata file exists
metadata_file <- "metadata.csv"
alt_metadata_files <- c("sample_metadata.csv", "mapping_file.csv", "mapping_file.txt", 
                      "metadata.txt", "sample_data.csv")

# Look for metadata file
if (!file.exists(metadata_file)) {
  for (alt_file in alt_metadata_files) {
    if (file.exists(alt_file)) {
      metadata_file <- alt_file
      cat("Found metadata file:", metadata_file, "\n")
      break
    }
  }
}

# Try to load and match external metadata
if (file.exists(metadata_file)) {
  metadata_result <- load_and_match_metadata(metadata_file, seqtab.nochim)
  
  if (!is.null(metadata_result)) {
    sample_metadata <- metadata_result$metadata
    cat("Successfully loaded external metadata\n")
    
    # Display metadata columns
    cat("Metadata contains the following columns:\n")
    print(colnames(sample_metadata))
  } else {
    # Fall back to extracting from filenames
    cat("Falling back to metadata extraction from sample names\n")
    sample_metadata <- data.frame(
      SampleID = sample.names,
      Group = sample.names,
      row.names = sample.names
    )
    
    # Try to extract groups from sample names
    group_pattern <- ".*?_(.*?)_.*|.*?-(.*?)-.*"
    group_matches <- regmatches(sample.names, regexec(group_pattern, sample.names))
    
    # Extract group information if the pattern matched
    valid_matches <- sapply(group_matches, length) > 1
    if (any(valid_matches)) {
      for (i in which(valid_matches)) {
        match <- group_matches[[i]]
        # Use the first capturing group that matched
        for (j in 2:length(match)) {
          if (!is.na(match[j]) && match[j] != "") {
            sample_metadata$Group[i] <- match[j]
            break
          }
        }
      }
    }
  }
} else {
  # No metadata file found, extract from sample names
  cat("No metadata file found. Extracting metadata from sample names\n")
  sample_metadata <- data.frame(
    SampleID = sample.names,
    Group = sample.names,
    row.names = sample.names
  )
  
  # Try to extract groups from sample names
  group_pattern <- ".*?_(.*?)_.*|.*?-(.*?)-.*"
  group_matches <- regmatches(sample.names, regexec(group_pattern, sample.names))
  
  # Extract group information if the pattern matched
  valid_matches <- sapply(group_matches, length) > 1
  if (any(valid_matches)) {
    for (i in which(valid_matches)) {
      match <- group_matches[[i]]
      # Use the first capturing group that matched
      for (j in 2:length(match)) {
        if (!is.na(match[j]) && match[j] != "") {
          sample_metadata$Group[i] <- match[j]
          break
        }
      }
    }
  }
}

# Count the number of samples in each group
if ("Group" %in% colnames(sample_metadata)) {
  group_counts <- table(sample_metadata$Group)
  cat("Detected sample groups:\n")
  print(group_counts)
}

# Create phyloseq object
ps <- phyloseq(
  otu_table(seqtab.nochim, taxa_are_rows = FALSE),
  sample_data(sample_metadata),
  tax_table(taxa)
)

# Add ASV sequences as reference sequences
dna <- Biostrings::DNAStringSet(colnames(seqtab.nochim))
names(dna) <- colnames(seqtab.nochim)
ps <- merge_phyloseq(ps, dna)

# Replace sequence names with ASV_1, ASV_2, etc. for easier reference
asv_ids <- paste0("ASV_", seq(ntaxa(ps)))
taxa_names(ps) <- asv_ids

# Update reference sequences
dna <- refseq(ps)
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)

# Inspect phyloseq object
cat("Created phyloseq object with:\n")
cat(" -", nsamples(ps), "samples\n")
cat(" -", ntaxa(ps), "ASVs\n")
cat(" -", sum(sample_sums(ps)), "total reads\n")
cat(" - Mean", round(mean(sample_sums(ps))), "reads per sample\n")

# Display sample read counts
ps_counts <- tibble(
  SampleID = sample_names(ps),
  Group = sample_data(ps)$Group,
  ReadCount = sample_sums(ps)
) %>%
  arrange(desc(ReadCount))

kable(head(ps_counts, 10), caption = "Sample Read Counts (Top 10)")

# Create checkpoint after phyloseq creation step
save_checkpoint("after_phyloseq", 
               c("seqtab.nochim", "taxa", "ps", "sample_metadata", "track_df"),
               overwrite = TRUE)
cat("Checkpoint saved after phyloseq creation step\n")
```

# Create Phyloseq Object

```{r phyloseq}
# Create a directory for results if it doesn't exist
if(!dir.exists("results")) dir.create("results")

# Create sample data frame
cat("Creating sample metadata...\n")

# Extract sample group information (if follows naming pattern like Sample_GroupName_...)
sample_metadata <- data.frame(
  SampleID = sample.names,
  Group = sample.names,  # Default to sample name if no pattern is found
  row.names = sample.names
)

# Add run information if in multi-run mode
if (is_multi_run && exists("file_to_run")) {
  cat("Adding sequencing run information to sample metadata\n")
  sample_metadata$Run <- file_to_run
  
  # Try to extract experimental groups without the run prefix 
  # First remove the run prefix from sample names
  clean_names <- gsub("^[^_]*_", "", sample.names)
} else {
  # When not in multi-run mode, use the original sample names
  clean_names <- sample.names
}

# Try to extract experimental groups from sample names using a common pattern
# This is just a generic example - modify based on your actual naming convention
group_pattern <- ".*?_(.*?)_.*|.*?-(.*?)-.*"
group_matches <- regmatches(clean_names, 
                          regexec(group_pattern, clean_names))

# Extract group information if the pattern matched
valid_matches <- sapply(group_matches, length) > 1
if (any(valid_matches)) {
  for (i in which(valid_matches)) {
    match <- group_matches[[i]]
    # Use the first capturing group that matched
    for (j in 2:length(match)) {
      if (!is.na(match[j]) && match[j] != "") {
        sample_metadata$Group[i] <- match[j]
        break
      }
    }
  }
}

# Count the number of samples in each group
group_counts <- table(sample_metadata$Group)
cat("Detected sample groups:\n")
print(group_counts)

# Display run summary if in multi-run mode
if (is_multi_run && exists("file_to_run")) {
  # Cross-tabulate samples by group and run
  if (length(unique(sample_metadata$Group)) > 1) {
    group_by_run <- table(sample_metadata$Group, sample_metadata$Run)
    cat("\nSample counts by group and run:\n")
    print(group_by_run)
  } else {
    run_counts <- table(sample_metadata$Run)
    cat("\nSample counts by run:\n")
    print(run_counts)
  }
}

# First create basic phyloseq components
otu <- otu_table(seqtab.nochim, taxa_are_rows = FALSE)
tax <- tax_table(taxa)
sam <- sample_data(sample_metadata)

# Add ASV sequences as reference sequences
dna <- Biostrings::DNAStringSet(colnames(seqtab.nochim))
names(dna) <- colnames(seqtab.nochim)

# Replace sequence names with ASV_1, ASV_2, etc. for easier reference
asv_ids <- paste0("ASV_", seq(ncol(otu)))
taxa_names(otu) <- asv_ids
taxa_names(tax) <- asv_ids
names(dna) <- asv_ids

# Create phyloseq object (we'll add the tree later if available)
ps <- phyloseq(otu, tax, sam, dna)

# Inspect phyloseq object
cat("Created phyloseq object with:\n")
cat(" -", nsamples(ps), "samples\n")
cat(" -", ntaxa(ps), "ASVs\n")
cat(" -", sum(sample_sums(ps)), "total reads\n")
cat(" - Mean", round(mean(sample_sums(ps))), "reads per sample\n")

# Display sample read counts
ps_counts <- tibble(
  SampleID = sample_names(ps),
  Group = sample_data(ps)$Group,
  ReadCount = sample_sums(ps)
) %>%
  arrange(desc(ReadCount))

kable(head(ps_counts, 10), caption = "Sample Read Counts (Top 10)")

# Create checkpoint after phyloseq creation step
save_checkpoint("after_phyloseq", 
               c("seqtab.nochim", "taxa", "ps", "sample_metadata", "track_df"),
               overwrite = TRUE)
cat("Checkpoint saved after phyloseq creation step\n")
```

# Phylogenetic Tree Construction

```{r build-phylogenetic-tree}
# Function to build phylogenetic tree from ASV sequences
build_phylogenetic_tree <- function(seqs, method = "ML", bootstrap = 100, cores = NULL, 
                                    progressbar = TRUE, verbose = TRUE) {
  # Check if required packages are available
  if (!requireNamespace("DECIPHER", quietly = TRUE) || !requireNamespace("phangorn", quietly = TRUE)) {
    cat("ERROR: DECIPHER and phangorn packages are required for tree building.\n")
    cat("Please install them with BiocManager::install(c('DECIPHER', 'phangorn'))\n")
    return(NULL)
  }
  
  # Set number of cores for parallel processing
  if (is.null(cores)) {
    cores <- min(parallel::detectCores() - 1, 8)
  }
  
  # Convert sequences to DNAStringSet if they aren't already
  if (!inherits(seqs, "DNAStringSet")) {
    if (is.character(seqs)) {
      # If this is a vector of sequences, convert to DNAStringSet
      seqs <- Biostrings::DNAStringSet(seqs)
    } else {
      stop("Sequences must be DNAStringSet or character vector")
    }
  }
  
  if (verbose) cat("Starting phylogenetic tree construction with method:", method, "\n")
  
  # Phase 1: Multiple sequence alignment with DECIPHER
  if (verbose) cat("Performing multiple sequence alignment...\n")
  alignment <- DECIPHER::AlignSeqs(seqs, anchor = NA, processors = cores, 
                                   verbose = progressbar)
  
  # Convert to phangorn format
  if (verbose) cat("Converting alignment to phyDat format...\n")
  phang.align <- phangorn::phyDat(as(alignment, "matrix"), type = "DNA")
  
  # Phase 2: Calculate distance matrix
  if (verbose) cat("Calculating distance matrix...\n")
  dm <- phangorn::dist.ml(phang.align)
  
  # Phase 3: Build initial tree with Neighbor-Joining
  if (verbose) cat("Building initial Neighbor-Joining tree...\n")
  nj_tree <- phangorn::NJ(dm)
  
  # Check if we should return just the NJ tree
  if (method == "NJ") {
    if (verbose) cat("NJ tree construction complete.\n")
    return(nj_tree)
  } 
  
  # Phase 4: Maximum Likelihood optimization
  if (verbose) cat("Optimizing tree with Maximum Likelihood method...\n")
  
  # Initial fit
  if (verbose) cat("  Initial tree fitting...\n")
  fit <- phangorn::pml(nj_tree, data = phang.align)
  
  # Model selection
  if (verbose) cat("  Optimizing with GTR+G+I model...\n")
  fit_gtr <- phangorn::optim.pml(fit, model = "GTR+G+I", 
                               rearrangement = "stochastic",
                               control = phangorn::pml.control(trace = progressbar))
  
  # Phase 5: Bootstrap support (if requested)
  if (bootstrap > 0) {
    if (verbose) cat("Calculating bootstrap support with", bootstrap, "replicates...\n")
    bs <- phangorn::bootstrap.pml(fit_gtr, bs = bootstrap, 
                                multicore = TRUE, 
                                mc.cores = cores)
    tree <- phangorn::plotBS(fit_gtr$tree, bs, type = "phylogram")
    if (verbose) cat("Bootstrap analysis complete.\n")
  } else {
    tree <- fit_gtr$tree
  }
  
  if (verbose) cat("Phylogenetic tree construction complete.\n")
  return(tree)
}

# Check if we can build trees
if (can_build_trees) {
  cat("Building phylogenetic tree from ASV sequences...\n")
  
  # Get ASV sequences
  asv_sequences <- colnames(seqtab.nochim)
  
  # Convert to DNAStringSet
  dna <- Biostrings::DNAStringSet(asv_sequences)
  names(dna) <- paste0("ASV_", seq_along(asv_sequences))
  
  # Check if this is a reasonably sized dataset for tree building
  if (length(dna) > 5000) {
    cat("Large number of ASVs detected (", length(dna), "). ",
        "Tree building may take a very long time.\n", sep = "")
    cat("Consider reducing the dataset before building a tree.\n")
    cat("Proceeding with NJ method only (faster)...\n")
    tree_method <- "NJ"
    bootstrap_reps <- 0
  } else if (length(dna) > 1000) {
    cat("Moderately large number of ASVs (", length(dna), "). ",
        "Using NJ method with limited bootstrap...\n", sep = "")
    tree_method <- "NJ"
    bootstrap_reps <- 100
  } else {
    cat("ASV count suitable for ML tree building (", length(dna), ").\n", sep = "")
    tree_method <- "ML"
    bootstrap_reps <- 100
  }
  
  # Time the tree construction
  start_time <- Sys.time()
  
  # Build the tree
  asv_tree <- build_phylogenetic_tree(
    dna, 
    method = tree_method,
    bootstrap = bootstrap_reps,
    verbose = TRUE
  )
  
  # Report time taken
  end_time <- Sys.time()
  time_taken <- difftime(end_time, start_time, units = "mins")
  cat("Tree construction completed in", round(time_taken, 2), "minutes\n")
  
  # Save tree to file
  if (!dir.exists("results")) dir.create("results")
  tree_file <- "results/asv_tree.rds"
  saveRDS(asv_tree, tree_file)
  cat("Phylogenetic tree saved to", tree_file, "\n")
  
  # Create checkpoint after tree building with metadata about the tree
  # Create a summary of the tree's properties
  tree_metadata <- list(
    method = tree_method,
    bootstrap = bootstrap_reps,
    n_tips = length(asv_tree$tip.label),
    build_time = as.numeric(time_taken, units = "mins"),
    timestamp = Sys.time()
  )
  
  save_checkpoint("after_tree_building", 
                 c("asv_tree", "seqtab.nochim", "taxa", "tree_metadata"),
                 overwrite = TRUE)
  cat("Checkpoint saved after tree building step\n")
  
  # Save additional metadata about the tree for dashboard and documentation
  if (!dir.exists("results")) dir.create("results")
  saveRDS(tree_metadata, "results/tree_metadata.rds")
  cat("Tree metadata saved to results/tree_metadata.rds\n")
  
  # Update phyloseq object with the tree if available
  if (!is.null(asv_tree)) {
    cat("Updating phyloseq object with phylogenetic tree...\n")
    
    # Make sure tree has the correct labels
    asv_ids <- taxa_names(ps)
    if (!identical(asv_ids, asv_tree$tip.label)) {
      cat("Adjusting tree tip labels to match ASV IDs...\n")
      # Map original sequence names to ASV IDs
      seq_to_asv <- structure(asv_ids, names = colnames(seqtab.nochim))
      asv_tree$tip.label <- seq_to_asv[match(asv_tree$tip.label, names(seq_to_asv))]
      
      # Check if any NAs in tree tips
      if (any(is.na(asv_tree$tip.label))) {
        warning("Some tree tips could not be mapped to ASV IDs")
        # Fix NAs by assigning sequential numbers
        na_tips <- which(is.na(asv_tree$tip.label))
        asv_tree$tip.label[na_tips] <- paste0("ASV_unknown_", seq_along(na_tips))
      }
    }
    
    # Add tree to phyloseq object
    phy_tree(ps) <- asv_tree
    cat("Phylogenetic tree added to phyloseq object\n")
    
    # Resave phyloseq checkpoint with tree included
    save_checkpoint("after_phyloseq_with_tree", 
                   c("ps", "asv_tree", "seqtab.nochim", "taxa", "sample_metadata"),
                   overwrite = TRUE)
  }
} else {
  cat("Skipping phylogenetic tree construction due to missing dependencies.\n")
  cat("Install DECIPHER and phangorn to enable this feature.\n")
  asv_tree <- NULL
}
```

# Save Results

# Rarefaction Analysis

```{r rarefaction-analysis}
# Check required packages for rarefaction analysis
can_do_rarefaction <- requireNamespace("vegan", quietly = TRUE)

if (can_do_rarefaction) {
  cat("Beginning rarefaction analysis...\n")
  
  # Function to calculate rarefaction curves
  calculate_rarefaction_curves <- function(ps, 
                                          step_size = 10, 
                                          label = NULL,
                                          max_depth = NULL) {
    
    cat("Calculating rarefaction curves...\n")
    
    # Get the abundance matrix
    abundances <- as.matrix(otu_table(ps))
    if(taxa_are_rows(ps)) {
      abundances <- t(abundances)
    }
    
    # Get sample data
    sample_data <- as.data.frame(sample_data(ps))
    
    # Get max depth if not specified
    if(is.null(max_depth)) {
      max_depth <- max(rowSums(abundances))
    }
    
    # Define depths at which to rarefy
    depths <- seq(1, max_depth, by = step_size)
    depths <- unique(c(depths, max_depth))
    
    # Calculate rarefaction at each depth for each sample
    rarefaction_curves <- data.frame(depth = numeric(), 
                                    richness = numeric(), 
                                    sample = character(),
                                    stringsAsFactors = FALSE)
    
    # Use parallel processing if available
    if (exists("parallel_cores") && parallel_cores > 1 && 
        requireNamespace("future.apply", quietly = TRUE) && 
        requireNamespace("future", quietly = TRUE)) {
      
      cat("Using parallel processing for rarefaction curves calculations...\n")
      
      # Set up future plan for parallel processing
      previous_plan <- future::plan(future::multisession, workers = parallel_cores)
      on.exit(future::plan(previous_plan), add = TRUE)
      
      # Process in chunks to avoid memory issues
      chunk_size <- min(10, length(depths))  # Adjust chunk size based on your data
      depth_chunks <- split(depths, ceiling(seq_along(depths) / chunk_size))
      
      for (chunk_idx in seq_along(depth_chunks)) {
        chunk <- depth_chunks[[chunk_idx]]
        cat(sprintf("Processing depths chunk %d of %d...\n", chunk_idx, length(depth_chunks)))
        
        # Process this chunk in parallel
        chunk_results <- future.apply::future_lapply(chunk, function(depth) {
          if(!requireNamespace("vegan", quietly = TRUE)) {
            return(NULL)  # Skip if vegan not available
          }
          
          # Calculate richness at this depth for each sample
          richness <- vegan::rarefy(abundances, depth)
          
          # Create results for each sample
          result_df <- data.frame(
            depth = depth,
            richness = richness,
            sample = rownames(abundances),
            stringsAsFactors = FALSE
          )
          
          # Filter out samples with insufficient reads
          result_df <- result_df[!is.na(result_df$richness), ]
          return(result_df)
        })
        
        # Combine results from this chunk
        for (result in chunk_results) {
          if (!is.null(result)) {
            rarefaction_curves <- rbind(rarefaction_curves, result)
          }
        }
        
        # Force garbage collection to free memory
        gc()
      }
    } else {
      # Non-parallel processing
      for (i in seq_along(depths)) {
        depth <- depths[i]
        
        # Calculate richness at this depth for each sample
        richness <- vegan::rarefy(abundances, depth)
        
        # Store the results
        for (j in seq_along(richness)) {
          if(!is.na(richness[j])) {  # Skip samples with insufficient reads
            rarefaction_curves <- rbind(rarefaction_curves, 
                                      data.frame(depth = depth,
                                               richness = richness[j],
                                               sample = rownames(abundances)[j],
                                               stringsAsFactors = FALSE))
          }
        }
        
        # Progress indicator
        if(i %% 10 == 0 || i == length(depths)) {
          cat(sprintf("Processed %d of %d depths...\n", i, length(depths)))
        }
      }
    }
    
    # Add sample metadata if available
    if(!is.null(sample_data) && nrow(sample_data) > 0) {
      # Ensure sample names match
      sample_data$sample_id <- rownames(sample_data)
      rarefaction_curves$sample_id <- rarefaction_curves$sample
      
      # Merge with sample data
      rarefaction_curves <- merge(rarefaction_curves, sample_data, 
                                by = "sample_id", all.x = TRUE)
    }
    
    return(rarefaction_curves)
  }
  
  # Rarefy at specific depth to create a rarefied phyloseq object
  rarefy_phyloseq <- function(ps, depth = NULL, seed = 123) {
    # Get the abundance matrix
    abundances <- as.matrix(otu_table(ps))
    if(taxa_are_rows(ps)) {
      abundances <- t(abundances)
    }
    
    # Determine rarefaction depth if not specified
    if(is.null(depth)) {
      depth <- min(rowSums(abundances))
      cat("Auto-selecting minimum sample depth:", depth, "reads\n")
    } else {
      cat("Rarefying to specified depth:", depth, "reads\n")
    }
    
    # Check if any samples have fewer reads than the rarefaction depth
    if(any(rowSums(abundances) < depth)) {
      excluded_samples <- rownames(abundances)[rowSums(abundances) < depth]
      cat("WARNING: The following samples have fewer reads than the rarefaction depth and will be excluded:\n")
      cat(paste(excluded_samples, collapse = ", "), "\n")
    }
    
    # Perform rarefaction
    ps_rare <- NULL
    tryCatch({
      ps_rare <- rarefy_even_depth(ps, sample.size = depth, 
                                  rngseed = seed, replace = FALSE, 
                                  trimOTUs = TRUE, verbose = TRUE)
      cat("Rarefaction successful!\n")
    }, error = function(e) {
      cat("Error in rarefaction:", conditionMessage(e), "\n")
      cat("Try a different rarefaction depth or check your data.\n")
    })
    
    return(ps_rare)
  }
  
  # Define an appropriate step size based on the dataset size
  # For larger datasets, we use larger steps to reduce computation time
  max_depth <- max(sample_sums(ps))
  if (max_depth > 100000) {
    step_size <- 1000  # Very large datasets
  } else if (max_depth > 50000) {
    step_size <- 500   # Large datasets
  } else if (max_depth > 10000) {
    step_size <- 200   # Medium datasets
  } else {
    step_size <- 100   # Small datasets
  }
  
  cat("Calculating rarefaction curves with step size:", step_size, "\n")
  
  # Generate rarefaction curves
  rarefaction_curves <- calculate_rarefaction_curves(ps, step_size = step_size)
  
  # Plot rarefaction curves
  if(!is.null(rarefaction_curves)) {
    # Get metadata for grouping
    sample_metadata <- as.data.frame(sample_data(ps))
    
    # Find suitable categorical variable for grouping
    metadata_cols <- colnames(sample_metadata)
    categorical_cols <- sapply(sample_metadata, function(col) {
      is.character(col) || is.factor(col) || length(unique(col)) < 10
    })
    
    group_var <- if(any(categorical_cols)) {
      metadata_cols[which(categorical_cols)[1]]  # Choose first categorical column
    } else {
      NULL  # No suitable grouping variable
    }
    
    # Basic rarefaction plot with grouping if available
    plot_title <- "Rarefaction Curves"
    
    if(!is.null(group_var) && group_var %in% colnames(rarefaction_curves)) {
      # Plot with grouping
      rare_plot <- ggplot(rarefaction_curves, 
                         aes(x = depth, y = richness, group = sample, 
                             color = .data[[group_var]])) +
        geom_line(alpha = 0.7) +
        labs(title = plot_title,
             subtitle = paste("Grouped by", group_var),
             x = "Sequencing Depth",
             y = "ASV Richness") +
        theme_minimal() +
        theme(legend.position = "right")
    } else {
      # Plot without grouping
      rare_plot <- ggplot(rarefaction_curves, 
                         aes(x = depth, y = richness, group = sample)) +
        geom_line(alpha = 0.7) +
        labs(title = plot_title,
             x = "Sequencing Depth",
             y = "ASV Richness") +
        theme_minimal()
    }
    
    # Enhanced visualization
    rare_plot <- rare_plot +
      geom_vline(xintercept = min(sample_sums(ps)), 
                linetype = "dashed", color = "blue", alpha = 0.7) +
      geom_vline(xintercept = median(sample_sums(ps)), 
                linetype = "dashed", color = "darkgreen", alpha = 0.7) +
      annotate("text", x = min(sample_sums(ps)), y = max(rarefaction_curves$richness),
              label = "Min depth", hjust = -0.1, vjust = 0, color = "blue") +
      annotate("text", x = median(sample_sums(ps)), y = max(rarefaction_curves$richness),
              label = "Median depth", hjust = -0.1, vjust = 1, color = "darkgreen")
    
    print(rare_plot)
    
    # Find suitable rarefaction depth
    # Typically, we choose a depth that retains most samples while reaching near-asymptotic richness
    sample_depths <- data.frame(
      sample = names(sample_sums(ps)),
      depth = sample_sums(ps)
    )
    
    # Recommend a rarefaction depth
    min_depth <- min(sample_depths$depth)
    median_depth <- median(sample_depths$depth)
    mean_depth <- mean(sample_depths$depth)
    
    cat("\nSample sequencing depth summary:\n")
    cat("Minimum depth:", min_depth, "reads\n")
    cat("Mean depth:", round(mean_depth, 1), "reads\n")
    cat("Median depth:", median_depth, "reads\n")
    cat("Maximum depth:", max(sample_depths$depth), "reads\n")
    
    # Create depth distribution plot
    depth_dist_plot <- ggplot(sample_depths, aes(x = depth)) +
      geom_histogram(bins = 30, fill = "steelblue", color = "black") +
      geom_vline(xintercept = min_depth, linetype = "dashed", color = "blue") +
      geom_vline(xintercept = median_depth, linetype = "dashed", color = "darkgreen") +
      labs(title = "Distribution of Sample Sequencing Depths",
           subtitle = "Dashed lines show min (blue) and median (green) depths",
           x = "Sequencing Depth (reads)",
           y = "Number of Samples") +
      theme_minimal()
    
    print(depth_dist_plot)
    
    # Analyze rarefaction curve saturation
    # To evaluate if samples have reached a plateau in their rarefaction curves
    evaluate_rarefaction_saturation <- function(curves, threshold = 0.05) {
      # Group by sample
      sample_curves <- split(curves, curves$sample)
      
      # Calculate rate of change at the end of each curve
      saturation_status <- data.frame(
        sample = character(),
        plateau_reached = logical(),
        plateau_depth = numeric(),
        final_slope = numeric(),
        stringsAsFactors = FALSE
      )
      
      for (sample_name in names(sample_curves)) {
        curve_data <- sample_curves[[sample_name]]
        # Sort by depth
        curve_data <- curve_data[order(curve_data$depth), ]
        
        # Need at least 3 points to calculate saturation
        if (nrow(curve_data) < 3) next
        
        # Calculate slopes between consecutive points
        depths <- curve_data$depth
        richness <- curve_data$richness
        slopes <- diff(richness) / diff(depths)
        
        # Check the last 20% of the curve (or at least the last 3 points)
        end_index <- max(3, round(length(slopes) * 0.2))
        end_slopes <- tail(slopes, end_index)
        
        # Calculate average slope at the end
        final_slope <- mean(end_slopes)
        
        # Normalize by maximum richness to get relative rate of change
        max_richness <- max(richness)
        relative_slope <- final_slope / max_richness
        
        # Determine if plateau has been reached (slope is below threshold)
        plateau_reached <- relative_slope < threshold
        
        # If plateau reached, find approximate depth where it started
        plateau_depth <- NA
        if (plateau_reached) {
          # Find where the curve first flattens (slope drops below threshold)
          relative_slopes <- slopes / max_richness
          plateau_idx <- which(relative_slopes < threshold)[1]
          if (!is.na(plateau_idx)) {
            plateau_depth <- depths[plateau_idx]
          }
        }
        
        # Add to results
        saturation_status <- rbind(saturation_status, data.frame(
          sample = sample_name,
          plateau_reached = plateau_reached,
          plateau_depth = plateau_depth,
          final_slope = relative_slope,
          stringsAsFactors = FALSE
        ))
      }
      
      return(saturation_status)
    }
    
    # Evaluate plateau status for each sample
    saturation_results <- evaluate_rarefaction_saturation(rarefaction_curves)
    
    # Summarize plateau results
    plateau_count <- sum(saturation_results$plateau_reached, na.rm = TRUE)
    total_samples <- nrow(saturation_results)
    
    cat("\nRarefaction curve saturation analysis:\n")
    cat(plateau_count, "out of", total_samples, "samples (", 
        round(plateau_count/total_samples*100, 1), "%) have reached a rarefaction plateau.\n", sep = "")
    
    # Suggest optimal rarefaction depth based on saturation analysis
    if (plateau_count > 0) {
      # Calculate median plateau depth for samples that reached plateau
      median_plateau <- median(saturation_results$plateau_depth, na.rm = TRUE)
      cat("Median plateau depth:", round(median_plateau), "reads\n")
      
      # Suggest a conservative depth that captures most plateaus but doesn't exclude too many samples
      suggested_depth <- min(median_plateau, median_depth)
      cat("Suggested rarefaction depth:", round(suggested_depth), "reads\n")
      
      # Count how many samples would be excluded at this depth
      excluded_count <- sum(sample_depths$depth < suggested_depth)
      if (excluded_count > 0) {
        cat("NOTE: Using this depth would exclude", excluded_count, "samples (", 
            round(excluded_count/nrow(sample_depths)*100, 1), "% of total)\n", sep = "")
      }
    } else {
      cat("No samples have clearly reached a plateau. Consider using the minimum depth for conservative rarefaction.\n")
      suggested_depth <- min_depth
    }
    
    # Create rarefied phyloseq object at suggested depth
    cat("\nCreating rarefied phyloseq object at suggested depth...\n")
    ps_rarefied <- rarefy_phyloseq(ps, depth = round(suggested_depth))
    
    # Create rarefied phyloseq object at minimum depth (most conservative approach)
    if (suggested_depth > min_depth) {
      cat("\nAlso creating a conservative rarefied phyloseq object at minimum depth...\n")
      ps_rarefied_min <- rarefy_phyloseq(ps, depth = min_depth)
    } else {
      ps_rarefied_min <- ps_rarefied  # They're the same in this case
    }
    
    # Compare alpha diversity before and after rarefaction
    if(!is.null(ps_rarefied)) {
      # Calculate alpha diversity metrics for all versions
      alpha_div_raw <- estimate_richness(ps, measures = c("Observed", "Shannon", "Simpson"))
      alpha_div_rare <- estimate_richness(ps_rarefied, measures = c("Observed", "Shannon", "Simpson"))
      
      # Add sample names as a column
      alpha_div_raw$sample <- rownames(alpha_div_raw)
      alpha_div_rare$sample <- rownames(alpha_div_rare)
      
      # Add data source column
      alpha_div_raw$data <- "Raw"
      alpha_div_rare$data <- "Rarefied"
      
      # Combine data
      alpha_div_combined <- rbind(alpha_div_raw, alpha_div_rare)
      
      # Add sample metadata
      sample_data_df <- as.data.frame(sample_data(ps))
      sample_data_df$sample <- rownames(sample_data_df)
      alpha_div_combined <- merge(alpha_div_combined, sample_data_df, by = "sample", all.x = TRUE)
      
      # Plot comparison
      p1 <- ggplot(alpha_div_combined, aes(x = data, y = Observed, fill = data)) +
        geom_boxplot() +
        labs(title = "Observed ASVs", x = "", y = "Richness") +
        theme_minimal() +
        theme(legend.position = "none")
      
      p2 <- ggplot(alpha_div_combined, aes(x = data, y = Shannon, fill = data)) +
        geom_boxplot() +
        labs(title = "Shannon Diversity", x = "", y = "Shannon Index") +
        theme_minimal() +
        theme(legend.position = "none")
      
      # Plot side by side
      grid.arrange(p1, p2, ncol = 2, top = "Alpha Diversity Before and After Rarefaction")
      
      # Correlation of diversity metrics before and after rarefaction
      common_samples <- intersect(alpha_div_raw$sample, alpha_div_rare$sample)
      
      if (length(common_samples) > 1) {  # Need at least 2 samples for correlation
        alpha_div_raw_common <- alpha_div_raw[alpha_div_raw$sample %in% common_samples, ]
        alpha_div_rare_common <- alpha_div_rare[alpha_div_rare$sample %in% common_samples, ]
        
        # Ensure same ordering
        alpha_div_rare_common <- alpha_div_rare_common[match(alpha_div_raw_common$sample, alpha_div_rare_common$sample), ]
        
        # Calculate correlations
        cor_observed <- cor(alpha_div_raw_common$Observed, alpha_div_rare_common$Observed)
        cor_shannon <- cor(alpha_div_raw_common$Shannon, alpha_div_rare_common$Shannon)
        
        cat("\nCorrelation between raw and rarefied diversity metrics:\n")
        cat("Observed ASVs: r =", round(cor_observed, 3), "\n")
        cat("Shannon Index: r =", round(cor_shannon, 3), "\n")
        
        # Create correlation plots
        par(mfrow = c(1, 2))
        plot(alpha_div_raw_common$Observed, alpha_div_rare_common$Observed,
             xlab = "Raw", ylab = "Rarefied", main = "Observed ASVs",
             pch = 19, col = alpha("darkblue", 0.7))
        abline(0, 1, lty = 2, col = "red")
        text(max(alpha_div_raw_common$Observed, na.rm = TRUE) * 0.8, 
             min(alpha_div_rare_common$Observed, na.rm = TRUE) * 1.2,
             paste("r =", round(cor_observed, 3)))
        
        plot(alpha_div_raw_common$Shannon, alpha_div_rare_common$Shannon,
             xlab = "Raw", ylab = "Rarefied", main = "Shannon Index",
             pch = 19, col = alpha("darkgreen", 0.7))
        abline(0, 1, lty = 2, col = "red")
        text(max(alpha_div_raw_common$Shannon, na.rm = TRUE) * 0.8, 
             min(alpha_div_rare_common$Shannon, na.rm = TRUE) * 1.2,
             paste("r =", round(cor_shannon, 3)))
        par(mfrow = c(1, 1))
      }
    }
    
    # Save rarefied phyloseq objects
    if(!is.null(ps_rarefied)) {
      ps_rarefied_path <- "results/phyloseq_rarefied.rds"
      saveRDS(ps_rarefied, ps_rarefied_path)
      cat("Saved rarefied phyloseq object to", ps_rarefied_path, "\n")
      
      if (exists("ps_rarefied_min") && !identical(ps_rarefied, ps_rarefied_min)) {
        ps_rarefied_min_path <- "results/phyloseq_rarefied_min.rds"
        saveRDS(ps_rarefied_min, ps_rarefied_min_path)
        cat("Saved conservative rarefied phyloseq object to", ps_rarefied_min_path, "\n")
      }
    }
    
    # Save rarefaction curves data
    rarefaction_path <- "results/rarefaction_curves.rds"
    saveRDS(rarefaction_curves, rarefaction_path)
    cat("Saved rarefaction curves data to", rarefaction_path, "\n")
    
    # Save rarefaction saturation analysis
    saturation_path <- "results/rarefaction_saturation.rds"
    saveRDS(saturation_results, saturation_path)
    cat("Saved rarefaction saturation analysis to", saturation_path, "\n")
    
    # Create checkpoint after rarefaction analysis step with additional metadata
    rarefaction_metadata <- list(
      timestamp = Sys.time(),
      step_size = step_size,
      sample_count = nsamples(ps),
      suggested_depth = if(exists("suggested_depth")) suggested_depth else NULL,
      min_depth = min_depth,
      median_depth = median_depth,
      mean_depth = mean_depth,
      plateau_percentage = if(exists("plateau_count")) round(plateau_count/total_samples*100, 1) else NULL
    )

    save_checkpoint("after_rarefaction", 
                  c("ps", "ps_rarefied", "rarefaction_curves", "saturation_results", 
                    "rarefaction_metadata", "alpha_div_combined"),
                  overwrite = TRUE)
    cat("Checkpoint saved after rarefaction analysis step\n")

    # Save rarefaction metadata separately for dashboard access
    if (!dir.exists("results")) dir.create("results")
    saveRDS(rarefaction_metadata, "results/rarefaction_metadata.rds")
    cat("Rarefaction metadata saved to results/rarefaction_metadata.rds\n")

    # Save alpha diversity comparison data if available
    if (exists("alpha_div_combined")) {
      saveRDS(alpha_div_combined, "results/alpha_diversity_comparison.rds")
      write.csv(alpha_div_combined, "results/alpha_diversity_comparison.csv", row.names = FALSE)
      cat("Alpha diversity comparison data saved to results/alpha_diversity_comparison.csv\n")
    }
  }
} else {
  cat("Skipping rarefaction analysis due to missing dependencies.\n")
  cat("Install 'vegan' package to enable this feature.\n")
}
```

# Save Results

```{r save-results}
# Create directory for results if it doesn't exist
if(!dir.exists("results")) dir.create("results")

# Save sequence table with ASV IDs
asv_tab <- as.data.frame(t(otu_table(ps)))
asv_tab$ASV_ID <- rownames(asv_tab)
write.csv(asv_tab, "results/seqtab_nochim.csv", row.names = FALSE)
cat("Saved ASV table to results/seqtab_nochim.csv\n")

# Save taxonomy table with ASV IDs
tax_tab <- as.data.frame(tax_table(ps))
tax_tab$ASV_ID <- rownames(tax_tab)
write.csv(tax_tab, "results/taxonomy.csv", row.names = FALSE)
cat("Saved taxonomy table to results/taxonomy.csv\n")

# Save phyloseq object
saveRDS(ps, "results/phyloseq_object.rds")
cat("Saved phyloseq object to results/phyloseq_object.rds\n")

# Check if the phyloseq object contains a tree and mention it in the message
if (!is.null(phy_tree(ps, errorIfNULL = FALSE))) {
  cat("Note: The saved phyloseq object includes a phylogenetic tree\n")
}

# Extract and save ASV sequences with IDs
asv_seqs <- refseq(ps)
asv_headers <- names(asv_seqs)
asv_strings <- paste(asv_seqs)

# Interleave headers and sequences for FASTA format
asv_fasta <- c(rbind(paste0(">", asv_headers), asv_strings))
write(asv_fasta, "results/ASVs.fasta")
cat("Saved ASV sequences to results/ASVs.fasta\n")

# Save parameter optimization results for future reference
param_optimization <- list(
  platform = truncation_lengths$platform,
  max_read_lengths = c(forward = truncation_lengths$max_forward_len, 
                      reverse = truncation_lengths$max_reverse_len),
  truncation_lengths = c(forward = truncation_lengths$forward, 
                        reverse = truncation_lengths$reverse),
  maxEE = c(forward = maxEE_params$maxEE[1], 
           reverse = maxEE_params$maxEE[2]),
  expected_amplicon_size = expected_amplicon_size,
  expected_overlap = truncation_lengths$expected_overlap,
  primers = primer_info
)
saveRDS(param_optimization, "results/parameter_optimization.rds")
cat("Saved parameter optimization results to results/parameter_optimization.rds\n")

# Save workflow results summary with enhanced metadata
workflow_summary <- list(
  date = Sys.Date(),
  timestamp = Sys.time(),
  num_samples = nsamples(ps),
  num_asvs = ntaxa(ps),
  total_reads = sum(sample_sums(ps)),
  mean_reads_per_sample = mean(sample_sums(ps)),
  read_tracking = track_df,
  parameters = param_optimization,
  run_info = list(
    r_version = R.version.string,
    run_time = format(Sys.time()),
    is_multi_run = is_multi_run,
    has_tree = !is.null(phy_tree(ps, errorIfNULL = FALSE))
  ),
  read_stats = list(
    min_reads = min(sample_sums(ps)),
    max_reads = max(sample_sums(ps)),
    median_reads = median(sample_sums(ps)),
    read_distribution = quantile(sample_sums(ps), probs = c(0, 0.25, 0.5, 0.75, 1))
  ),
  taxonomy_stats = list(
    kingdom = sum(!is.na(tax_table(ps)[, "Kingdom"])),
    phylum = sum(!is.na(tax_table(ps)[, "Phylum"])),
    class = sum(!is.na(tax_table(ps)[, "Class"])),
    order = sum(!is.na(tax_table(ps)[, "Order"])),
    family = sum(!is.na(tax_table(ps)[, "Family"])),
    genus = sum(!is.na(tax_table(ps)[, "Genus"])),
    species = sum(!is.na(tax_table(ps)[, "Species"]))
  )
)
saveRDS(workflow_summary, "results/workflow_summary.rds")
write.csv(data.frame(
  Metric = names(unlist(workflow_summary[c("num_samples", "num_asvs", "total_reads", "mean_reads_per_sample")])),
  Value = unlist(workflow_summary[c("num_samples", "num_asvs", "total_reads", "mean_reads_per_sample")])
), "results/workflow_summary_stats.csv", row.names = FALSE)
cat("Saved enhanced workflow summary to results/workflow_summary.rds\n")
```

# Generate Automated Report

```{r generate-report, eval=params$generate_report}
# Only execute this block if report generation is requested via parameters

# Create reports directory if it doesn't exist
if(!dir.exists(params$output_dir)) {
  dir.create(params$output_dir, recursive = TRUE)
  cat("Created reports directory:", params$output_dir, "\n")
}

# Current date for report filename
report_date <- format(Sys.Date(), "%Y%m%d")

# Create output filename
output_file <- file.path(params$output_dir, 
                         paste0(params$output_file, "_", report_date))

# Generate the report
report_result <- try({
  rmarkdown::render(
    input = knitr::current_input(),  # Use current file as input
    output_format = params$output_format,
    output_file = output_file,
    params = list(
      generate_report = FALSE  # Prevent infinite recursion
    ),
    envir = new.env(parent = globalenv())
  )
}, silent = TRUE)

if(inherits(report_result, "try-error")) {
  cat("Error generating report:", conditionMessage(report_result), "\n")
} else {
  cat("Report successfully generated:", output_file, "\n")
}
```

# Conclusion

This document outlines an optimized DADA2 workflow for processing 16S rRNA gene amplicon data. The workflow includes automatic parameter optimization based on your sequencing data, quality filtering, denoising, merging paired reads, chimera removal, and taxonomic assignment to generate a table of amplicon sequence variants (ASVs).

Key enhancements in this optimized workflow:

1. **Automatic platform detection** and parameter optimization based on read length and quality
2. **Adaptive quality filtering** with optimized truncation lengths and expected error thresholds
3. **Primer detection** for expected amplicon size estimation
4. **Read merging optimization** to ensure sufficient overlap
5. **Phylogenetic tree construction** with multiple alignment and maximum likelihood optimization
6. **Enhanced quality visualization** with percentile distributions and quality interpretation zones
7. **Extensive quality control** throughout the pipeline
8. **Detailed visualization** of results at each step
9. **Comprehensive results export** for downstream analysis
10. **Automated HTML/PDF report generation**

## Checkpointing System

This workflow includes a robust checkpointing system that saves intermediate results after each computationally intensive step. Checkpoints provide several benefits:

1. **Workflow Resilience**: If a long-running analysis is interrupted, you can restart from the last successful checkpoint instead of beginning again
2. **Parameter Experimentation**: Run parts of the workflow with different parameters without rerunning earlier steps
3. **Saving Time**: Skip time-consuming steps when you're just making changes to later parts of the analysis
4. **Memory Management**: Break a large analysis into manageable chunks that fit within available memory

Checkpoints are saved in the `checkpoints/` directory and can be loaded at the start of a workflow run. To work with checkpoints:

- At the beginning of a workflow, you will be prompted to load an existing checkpoint if any are available
- To manually load a checkpoint, use `load_checkpoint("checkpoint_name")`
- To list all available checkpoints, use `list_checkpoints()`

This optimized workflow creates checkpoints after each major processing step:
- After filtering and trimming
- After error learning
- After dereplication
- After sample inference
- After read merging
- After ASV table construction
- After chimera removal
- After taxonomy assignment
- After phyloseq object creation
- After phylogenetic tree construction

## Automated Reporting

This workflow supports automated report generation in both HTML and PDF formats. To generate a report, use the following command-line options with the `run_dashboard.R` script:

```bash
# Generate HTML report (default)
Rscript run_dashboard.R --generate-report

# Generate PDF report
Rscript run_dashboard.R --generate-report --format pdf_document

# Customize output location and filename
Rscript run_dashboard.R --generate-report --output-dir custom/path --output-file custom_name
```

The report will include all the plots, tables, and analysis results with the code optionally hidden (toggled via the code_folding parameter). Reports are automatically timestamped and saved in the specified output directory.

To visualize and explore your results interactively, you can use the companion dashboard:

```r
# Run the dashboard
source("run_dashboard.R")
```

The output files in the `results/` directory can be used for further microbiome analyses in R (using phyloseq, microbiome, vegan, etc.) or exported to other platforms.
