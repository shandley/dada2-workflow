---
title: "DADA2 Workflow with Parameter Optimization for 16S rRNA Amplicon Sequence Variants"
author: "Your Name"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
    code_folding: hide
    df_print: paged
---

# Introduction

This document implements an optimized workflow for processing 16S rRNA gene amplicon data using DADA2. The workflow includes automatic parameter optimization, quality filtering, denoising, merging paired reads, chimera removal, and taxonomic assignment to generate a table of amplicon sequence variants (ASVs).

The workflow has been streamlined to remove unnecessary features like GPU acceleration that weren't being used and multi-run processing to keep the code focused on the core optimization workflow.

# Setup

```{r setup, include=FALSE}
# Basic setup for R Markdown
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Define checkpoint directory
checkpoint_dir <- "checkpoints"
```

# Package Management

```{r package-management}
# Enhanced package checking and loading function
check_and_load_packages <- function() {
  # Set CRAN mirror to avoid "no mirror selected" errors
  if (is.null(getOption("repos")) || getOption("repos") == "@CRAN@") {
    options(repos = c(CRAN = "https://cloud.r-project.org"))
  }

  # Packages from CRAN
  cran_required_packages <- c(
    "ggplot2", "tidyverse", "gridExtra", "knitr", "future", "future.apply", "taxize",
    "rBayesianOptimization"  # Added for Bayesian optimization
  )
  
  # Packages from Bioconductor
  bioc_required_packages <- c(
    "dada2", "phyloseq", "Biostrings", "ShortRead"
  )
  
  # Combined required packages
  required_packages <- c(cran_required_packages, bioc_required_packages)
  
  # Optional packages - split by source
  cran_optional_packages <- c("vegan")
  bioc_optional_packages <- c("DECIPHER", "phangorn", "msa")
  
  # Combined optional packages
  optional_packages <- c(cran_optional_packages, bioc_optional_packages)
  
  # Check which packages are missing
  missing_cran_required <- cran_required_packages[!sapply(cran_required_packages, requireNamespace, quietly = TRUE)]
  missing_bioc_required <- bioc_required_packages[!sapply(bioc_required_packages, requireNamespace, quietly = TRUE)]
  missing_cran_optional <- cran_optional_packages[!sapply(cran_optional_packages, requireNamespace, quietly = TRUE)]
  missing_bioc_optional <- bioc_optional_packages[!sapply(bioc_optional_packages, requireNamespace, quietly = TRUE)]
  
  # Install BiocManager if needed
  if ((length(missing_bioc_required) > 0 || length(missing_bioc_optional) > 0) && 
      !requireNamespace("BiocManager", quietly = TRUE)) {
    message("Installing BiocManager...")
    install.packages("BiocManager")
  }
  
  # Install missing required packages
  if (length(missing_cran_required) > 0) {
    message("Installing required CRAN packages: ", paste(missing_cran_required, collapse = ", "))
    install.packages(missing_cran_required)
  }
  
  if (length(missing_bioc_required) > 0) {
    message("Installing required Bioconductor packages: ", paste(missing_bioc_required, collapse = ", "))
    for (pkg in missing_bioc_required) {
      BiocManager::install(pkg, update = FALSE)
    }
  }
  
  # Check if we have multiple cores available for parallel processing
  has_multiple_cores <- parallel::detectCores() > 2
  
  # Automatically install ParBayesianOptimization if multiple cores available
  has_pbo <- requireNamespace("ParBayesianOptimization", quietly = TRUE)
  if (!has_pbo && has_multiple_cores) {
    message("Detected multiple CPU cores. Automatically installing ParBayesianOptimization for parallel optimization...")
    install.packages("ParBayesianOptimization")
    has_pbo <- requireNamespace("ParBayesianOptimization", quietly = TRUE)
  }
  
  # Alert about missing optional packages but don't install automatically
  if (length(missing_cran_optional) > 0 || length(missing_bioc_optional) > 0) {
    missing_optional <- c(missing_cran_optional, missing_bioc_optional)
    message("NOTE: The following optional packages are not installed: ", 
            paste(missing_optional, collapse = ", "))
    message("These packages enable additional functionality like phylogenetic tree building and diversity analysis.")
    message("You can install them with:")
    
    if (length(missing_cran_optional) > 0) {
      message("  # From CRAN:")
      message("  install.packages(c('", paste(missing_cran_optional, collapse = "', '"), "'))")
    }
    
    if (length(missing_bioc_optional) > 0) {
      message("  # From Bioconductor:")
      message("  BiocManager::install(c('", paste(missing_bioc_optional, collapse = "', '"), "'))")
    }
  }
  
  # Load all required packages
  for (pkg in required_packages) {
    if (requireNamespace(pkg, quietly = TRUE)) {
      suppressPackageStartupMessages(library(pkg, character.only = TRUE))
    } else {
      warning("Required package '", pkg, "' could not be loaded.")
    }
  }
  
  # Load available optional packages
  for (pkg in optional_packages) {
    if (requireNamespace(pkg, quietly = TRUE)) {
      suppressPackageStartupMessages(library(pkg, character.only = TRUE))
    }
  }
  
  # Load ParBayesianOptimization if available
  if (has_pbo) {
    suppressPackageStartupMessages(library("ParBayesianOptimization"))
  }
  
  # Check for Bayesian optimization packages
  has_rbo <- requireNamespace("rBayesianOptimization", quietly = TRUE)
  
  if (has_rbo) {
    message("Bayesian optimization is available for parameter tuning.")
    if (has_pbo) {
      message("ParBayesianOptimization detected: Will use parallel processing for faster optimization.")
    }
  }
  
  # Create a comprehensive list of all Bioconductor packages
  all_bioc_packages <- c(bioc_required_packages, bioc_optional_packages)
  
  # Determine CPU core details for performance optimization
  cores_info <- list(
    total_cores = parallel::detectCores(),
    usable_cores = max(1, parallel::detectCores() - 1),
    multiple_cores = has_multiple_cores
  )
  
  return(list(
    required = required_packages,
    optional = intersect(optional_packages, 
                        optional_packages[sapply(optional_packages, requireNamespace, quietly = TRUE)]),
    missing = c(missing_cran_optional, missing_bioc_optional),
    bioconductor_packages = all_bioc_packages,
    has_bayesian_optimization = has_rbo,
    has_parallel_bayesian_optimization = has_pbo,
    cores = cores_info
  ))
}

# Load all packages and display status
package_status <- check_and_load_packages()

# Display information about available packages
cat("Required packages loaded:", length(package_status$required), "\n")
if (length(package_status$optional) > 0) {
  cat("Optional packages loaded:", paste(package_status$optional, collapse = ", "), "\n")
}
if (length(package_status$missing) > 0) {
  cat("Optional packages not available:", paste(package_status$missing, collapse = ", "), "\n")
  cat("Some advanced features may be disabled.\n")
}

# Check if specialized functionality is available
can_build_trees <- all(c("phangorn", "DECIPHER") %in% package_status$optional)
can_use_bayesian_optimization <- package_status$has_bayesian_optimization
can_use_parallel_bayesian_optimization <- package_status$has_parallel_bayesian_optimization

# Get core information
total_cores <- package_status$cores$total_cores
usable_cores <- package_status$cores$usable_cores

# Display optimization capability
if (can_use_bayesian_optimization) {
  cat("✓ Bayesian parameter optimization is available\n")
  if (can_use_parallel_bayesian_optimization) {
    cat(sprintf("✓ Parallel Bayesian optimization enabled (using %d of %d cores)\n", 
              usable_cores, total_cores))
  } else if (total_cores > 2) {
    cat("! Parallel optimization is available but not enabled (ParBayesianOptimization package not installed)\n")
  }
} else {
  cat("✗ Bayesian parameter optimization is not available\n")
}
if (can_build_trees) {
  cat("✓ Phylogenetic tree construction is available\n")
} else {
  cat("✗ Phylogenetic tree construction is disabled (install phangorn and DECIPHER to enable)\n")
}
```

# Checkpointing Functions

```{r checkpointing-functions}
# Create a checkpoint
save_checkpoint <- function(checkpoint_name, object_list, overwrite = FALSE) {
  # Validate inputs
  if(!is.character(checkpoint_name) || length(checkpoint_name) != 1) {
    warning("Invalid checkpoint name. Must be a single character string.")
    return(FALSE)
  }
  
  if(!is.character(object_list) || length(object_list) < 1) {
    warning("Invalid object list. Must be a character vector with at least one item.")
    return(FALSE)
  }
  
  # Create checkpoints directory if it doesn't exist
  tryCatch({
    if(!dir.exists(checkpoint_dir)) {
      dir.create(checkpoint_dir, recursive = TRUE)
      cat("Created checkpoints directory:", checkpoint_dir, "\n")
    }
  }, error = function(e) {
    warning("Failed to create checkpoint directory: ", conditionMessage(e))
    return(FALSE)
  })
  
  # Construct the filename
  filename <- file.path(checkpoint_dir, paste0(checkpoint_name, ".rds"))
  
  # Check if file exists and handle overwrite option
  if(file.exists(filename) && !overwrite) {
    cat("Checkpoint file already exists. Use overwrite = TRUE to overwrite.\n")
    return(FALSE)
  }
  
  # Create a list of objects to save
  checkpoint_data <- list()
  missing_objects <- character(0)
  
  for(obj_name in object_list) {
    if(exists(obj_name, envir = .GlobalEnv)) {
      tryCatch({
        checkpoint_data[[obj_name]] <- get(obj_name, envir = .GlobalEnv)
      }, error = function(e) {
        missing_objects <- c(missing_objects, obj_name)
        warning("Error retrieving object '", obj_name, "': ", conditionMessage(e))
      })
    } else {
      missing_objects <- c(missing_objects, obj_name)
      cat("Warning: Object", obj_name, "not found and not included in checkpoint.\n")
    }
  }
  
  # Report missing objects
  if(length(missing_objects) > 0) {
    cat("The following objects were not included in the checkpoint:", 
        paste(missing_objects, collapse = ", "), "\n")
    
    # If all objects are missing, return an error
    if(length(missing_objects) == length(object_list)) {
      warning("No objects could be saved. Checkpoint creation failed.")
      return(FALSE)
    }
  }
  
  # Add metadata
  checkpoint_data$timestamp <- Sys.time()
  checkpoint_data$object_names <- setdiff(object_list, missing_objects)
  checkpoint_data$missing_objects <- missing_objects
  
  # Calculate approximate size of data to be saved
  approx_size_mb <- utils::object.size(checkpoint_data) / (1024^2)
  cat("Approximate checkpoint size:", round(approx_size_mb, 2), "MB\n")
  
  # Save the checkpoint
  tryCatch({
    saveRDS(checkpoint_data, filename)
    cat("Checkpoint saved:", filename, "\n")
    return(TRUE)
  }, error = function(e) {
    warning("Failed to save checkpoint: ", conditionMessage(e))
    
    # Check for disk space issues
    if(grepl("cannot open|No space left", conditionMessage(e))) {
      df <- system("df -h .", intern = TRUE)
      cat("Disk space information:\n", paste(df, collapse = "\n"), "\n")
    }
    
    return(FALSE)
  })
}

# Load a checkpoint
load_checkpoint <- function(checkpoint_name, confirm = TRUE) {
  # Validate input
  if(!is.character(checkpoint_name) || length(checkpoint_name) != 1) {
    warning("Invalid checkpoint name. Must be a single character string.")
    return(FALSE)
  }
  
  # Construct the filename
  filename <- file.path(checkpoint_dir, paste0(checkpoint_name, ".rds"))
  
  # Check if file exists
  if(!file.exists(filename)) {
    cat("Checkpoint file not found:", filename, "\n")
    available <- list.files(checkpoint_dir, pattern = "\\.rds$")
    if(length(available) > 0) {
      cat("Available checkpoints:", paste(gsub("\\.rds$", "", available), collapse = ", "), "\n")
    }
    return(FALSE)
  }
  
  # Load the checkpoint data
  tryCatch({
    checkpoint_data <- readRDS(filename)
    
    # Verify that loaded checkpoint has the expected structure
    if(!is.list(checkpoint_data) || 
       !all(c("timestamp", "object_names") %in% names(checkpoint_data))) {
      warning("Invalid checkpoint file: missing required metadata.")
      return(FALSE)
    }
    
    # Calculate checkpoint file size
    filesize_mb <- round(file.info(filename)$size / (1024^2), 2)
    
    # Display checkpoint info
    cat("Checkpoint:", checkpoint_name, "\n")
    cat("Created:", format(checkpoint_data$timestamp), "\n")
    cat("File size:", filesize_mb, "MB\n")
    cat("Contains", length(checkpoint_data$object_names), "objects:", 
        paste(checkpoint_data$object_names, collapse = ", "), "\n")
    
    # Show missing objects if any
    if("missing_objects" %in% names(checkpoint_data) && 
       length(checkpoint_data$missing_objects) > 0) {
      cat("Note: The following objects were missing when this checkpoint was created:\n",
          paste(checkpoint_data$missing_objects, collapse = ", "), "\n")
    }
    
    # Confirm before loading
    if(confirm) {
      response <- readline("Load this checkpoint? (y/n): ")
      if(tolower(response) != "y") {
        cat("Checkpoint loading cancelled.\n")
        return(FALSE)
      }
    }
    
    # Check for potential conflicts
    conflicts <- character(0)
    for(obj_name in checkpoint_data$object_names) {
      if(exists(obj_name, envir = .GlobalEnv)) {
        conflicts <- c(conflicts, obj_name)
      }
    }
    
    if(length(conflicts) > 0) {
      cat("Warning: The following objects already exist in your environment and will be overwritten:\n",
          paste(conflicts, collapse = ", "), "\n")
      
      if(confirm) {
        response <- readline("Continue and overwrite these objects? (y/n): ")
        if(tolower(response) != "y") {
          cat("Checkpoint loading cancelled.\n")
          return(FALSE)
        }
      }
    }
    
    # Load objects into global environment
    loaded_objects <- character(0)
    for(obj_name in checkpoint_data$object_names) {
      if(obj_name %in% names(checkpoint_data)) {
        tryCatch({
          assign(obj_name, checkpoint_data[[obj_name]], envir = .GlobalEnv)
          loaded_objects <- c(loaded_objects, obj_name)
        }, error = function(e) {
          warning("Error loading object '", obj_name, "': ", conditionMessage(e))
        })
      }
    }
    
    # Report outcome
    if(length(loaded_objects) == 0) {
      warning("No objects were loaded from the checkpoint.")
      return(FALSE)
    } else if(length(loaded_objects) < length(checkpoint_data$object_names)) {
      cat("Partially loaded checkpoint. Loaded", length(loaded_objects), "of", 
          length(checkpoint_data$object_names), "objects.\n")
    } else {
      cat("Checkpoint loaded successfully!\n")
    }
    
    return(TRUE)
  }, error = function(e) {
    warning("Error loading checkpoint: ", conditionMessage(e))
    
    # Check file corruption
    if(grepl("unexpected|corrupt|read error", conditionMessage(e))) {
      cat("The checkpoint file appears to be corrupted or unreadable.\n")
    }
    
    return(FALSE)
  })
}

# List all available checkpoints
list_checkpoints <- function() {
  # Check if checkpoint directory exists
  if(!dir.exists(checkpoint_dir)) {
    cat("No checkpoints directory found.\n")
    return(NULL)
  }
  
  # Get list of checkpoint files
  tryCatch({
    checkpoint_files <- list.files(checkpoint_dir, pattern = "\\.rds$")
    
    if(length(checkpoint_files) == 0) {
      cat("No checkpoints found in", checkpoint_dir, "\n")
      return(NULL)
    }
    
    # Create info dataframe
    checkpoint_info <- data.frame(
      checkpoint = gsub("\\.rds$", "", checkpoint_files),
      file = checkpoint_files,
      timestamp = NA,
      size_mb = NA,
      objects_count = NA,
      objects = NA,
      status = NA,
      stringsAsFactors = FALSE
    )
    
    # Loop through files and extract info
    for(i in 1:nrow(checkpoint_info)) {
      filename <- file.path(checkpoint_dir, checkpoint_info$file[i])
      
      # Get file size
      file_info <- file.info(filename)
      checkpoint_info$size_mb[i] <- round(file_info$size / (1024^2), 2)
      
      # Try to read checkpoint data
      tryCatch({
        checkpoint_data <- readRDS(filename)
        
        # Basic validation of checkpoint structure
        if(!is.list(checkpoint_data) || 
           !all(c("timestamp", "object_names") %in% names(checkpoint_data))) {
          checkpoint_info$timestamp[i] <- as.character(file_info$mtime)
          checkpoint_info$objects_count[i] <- NA
          checkpoint_info$objects[i] <- NA
          checkpoint_info$status[i] <- "Invalid format"
        } else {
          # Extract metadata
          checkpoint_info$timestamp[i] <- as.character(checkpoint_data$timestamp)
          checkpoint_info$objects_count[i] <- length(checkpoint_data$object_names)
          checkpoint_info$objects[i] <- paste(checkpoint_data$object_names, collapse = ", ")
          checkpoint_info$status[i] <- "Valid"
          
          # Check for missing objects
          if("missing_objects" %in% names(checkpoint_data) && 
             length(checkpoint_data$missing_objects) > 0) {
            checkpoint_info$status[i] <- paste0("Valid (", 
                                             length(checkpoint_data$missing_objects), 
                                             " missing objects)")
          }
        }
      }, error = function(e) {
        checkpoint_info$timestamp[i] <- as.character(file_info$mtime)
        checkpoint_info$objects_count[i] <- NA
        checkpoint_info$objects[i] <- "Unknown"
        checkpoint_info$status[i] <- paste0("Error: ", conditionMessage(e))
      })
    }
    
    # Sort by timestamp (most recent first)
    checkpoint_info <- checkpoint_info[order(checkpoint_info$timestamp, decreasing = TRUE), ]
    
    return(checkpoint_info)
  }, error = function(e) {
    warning("Error listing checkpoints: ", conditionMessage(e))
    return(NULL)
  })
}
```

# Hardware Detection and Optimization Setup

```{r hardware-detection}
# Simplified hardware detection for optimal parallelization settings

# Function to detect hardware capabilities
detect_hardware_capabilities <- function() {
  # Initialize result list
  capabilities <- list(
    cores = 1,
    memory_gb = 4,
    apple_silicon = FALSE,
    optimal_workers = 1,
    memory_limit_gb = 2
  )
  
  # Core detection
  capabilities$cores <- parallel::detectCores()
  if (is.na(capabilities$cores) || capabilities$cores < 1) {
    capabilities$cores <- 1
  }
  
  # Memory detection
  capabilities$memory_gb <- tryCatch({
    if (.Platform$OS.type == "unix") {
      if (Sys.info()["sysname"] == "Darwin") {  # macOS
        mem_info <- system("sysctl -n hw.memsize", intern = TRUE)
        as.numeric(mem_info) / (1024^3)  # Convert to GB
      } else {  # Linux
        mem_info <- system("grep MemTotal /proc/meminfo", intern = TRUE)
        mem_kb <- as.numeric(gsub("[^0-9]", "", mem_info))
        mem_kb / (1024^2)  # Convert KB to GB
      }
    } else {  # Windows
      mem_info <- system("wmic OS get TotalVisibleMemorySize /Value", intern = TRUE)
      mem_line <- grep("TotalVisibleMemorySize", mem_info, value = TRUE)
      mem_kb <- as.numeric(gsub("[^0-9]", "", mem_line))
      mem_kb / (1024^2)  # Convert KB to GB
    }
  }, error = function(e) {
    # Default to 8GB if detection fails
    8
  })
  
  # Apple Silicon detection
  capabilities$apple_silicon <- FALSE
  if (Sys.info()["sysname"] == "Darwin") {  # Check if on macOS
    # Check for arm64 architecture (Apple Silicon)
    arch_check <- tryCatch(
      system("uname -m", intern = TRUE),
      error = function(e) "unknown"
    )
    capabilities$apple_silicon <- grepl("arm64", arch_check)
  }
  
  # Determine optimal workers based on hardware
  if (capabilities$apple_silicon) {
    # For Apple Silicon
    perf_cores <- min(capabilities$cores * 0.7, 10)  
    capabilities$optimal_workers <- min(ceiling(perf_cores), capabilities$cores - 1)
    capabilities$memory_limit_gb <- min(capabilities$memory_gb * 0.6, 12)
  } else if (capabilities$cores >= 16) {
    # High-core systems
    capabilities$optimal_workers <- min(capabilities$cores - 2, 12)
    capabilities$memory_limit_gb <- min(capabilities$memory_gb * 0.5, 8)
  } else if (capabilities$cores >= 4) {
    # Mid-range systems
    capabilities$optimal_workers <- min(capabilities$cores - 1, 4)
    capabilities$memory_limit_gb <- min(capabilities$memory_gb * 0.4, 4)
  } else {
    # Low-end systems
    capabilities$optimal_workers <- 2
    capabilities$memory_limit_gb <- min(capabilities$memory_gb * 0.4, 2)
  }
  
  return(capabilities)
}

# Smart garbage collection
adaptive_gc <- function(force_full = FALSE) {
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # Less aggressive GC for Apple Silicon to avoid memory fragmentation
    if (force_full) {
      invisible(gc(verbose = FALSE, full = TRUE))
    } else {
      invisible(gc(verbose = FALSE, full = FALSE))
    }
  } else {
    # More aggressive GC for other systems
    invisible(gc(verbose = FALSE, full = TRUE))
  }
}

# Detect hardware and set up environment
hw_caps <- detect_hardware_capabilities()

# Display detected capabilities
cat("Hardware detection complete:\n")
cat(sprintf("Detected %d CPU cores\n", hw_caps$cores))
cat(sprintf("Estimated %.1f GB total memory\n", hw_caps$memory_gb))

if (hw_caps$apple_silicon) {
  cat("Detected Apple Silicon (M-series) processor\n")
}

cat(sprintf("Using %d worker threads for parallel processing\n", hw_caps$optimal_workers))
cat(sprintf("Setting memory limit to %.1f GB\n", hw_caps$memory_limit_gb))
```

# Parallelization

```{r parallelization}
# Set up parallelization for faster processing
future::plan(future::multisession, workers = hw_caps$optimal_workers)

# Set memory limits based on hardware capabilities
options(future.globals.maxSize = hw_caps$memory_limit_gb * 1024^3)

# Apply Apple Silicon optimizations if detected
if (hw_caps$apple_silicon) {
  # For Apple Silicon, set environment variables for optimal performance
  Sys.setenv(VECLIB_MAXIMUM_THREADS = as.character(hw_caps$optimal_workers))
  Sys.setenv(OMP_THREAD_LIMIT = as.character(hw_caps$optimal_workers))
  cat("Applied Apple Silicon-specific optimizations\n")
}

# Set the random seed for reproducibility
set.seed(100)
```

# Checkpointing

```{r checkpointing}
# Check for existing checkpoints and allow recovery
tryCatch({
  if(dir.exists("checkpoints")) {
    cat("Checking for available checkpoints...\n")
    checkpoint_table <- list_checkpoints()
    
    if(!is.null(checkpoint_table) && nrow(checkpoint_table) > 0) {
      # Display checkpoint information in a readable format
      display_table <- checkpoint_table[, c("checkpoint", "timestamp", "size_mb", "objects_count", "status")]
      colnames(display_table) <- c("Checkpoint", "Created", "Size (MB)", "Objects", "Status")
      
      # Print formatted table
      cat("Available checkpoints:\n")
      print(display_table)
      
      # Prompt user to restore from checkpoint
      cat("\nWould you like to restore from a checkpoint? If so, enter the checkpoint name, or press Enter to start fresh: ")
      checkpoint_to_load <- readline()
      
      if(checkpoint_to_load != "") {
        # Check if entered checkpoint exists
        if(checkpoint_to_load %in% checkpoint_table$checkpoint) {
          # Try to load the specified checkpoint
          load_result <- load_checkpoint(checkpoint_to_load, confirm = TRUE)
          
          if(load_result) {
            cat("Workflow will continue from checkpoint", checkpoint_to_load, "\n")
          } else {
            cat("Starting fresh workflow run\n")
          }
        } else {
          cat("Checkpoint '", checkpoint_to_load, "' not found. Available checkpoints are:\n", 
              paste(checkpoint_table$checkpoint, collapse = ", "), "\n")
          cat("Starting fresh workflow run\n")
        }
      } else {
        cat("Starting fresh workflow run\n")
      }
    } else {
      cat("No valid checkpoints found. Starting fresh workflow run.\n")
    }
  } else {
    cat("No checkpoint directory found. Starting fresh workflow run.\n")
  }
}, error = function(e) {
  warning("Error checking for checkpoints: ", conditionMessage(e))
  cat("Starting fresh workflow run due to error in checkpoint system.\n")
})
```

# Set Working Directory and Configure for Processing

```{r run-configuration}
# Set data directory
path <- "data"
cat("Using data directory for processing\n")

# Function to find paired FASTQ files
find_fastq_files <- function(search_path, recursive = FALSE) {
  # Common function to handle various file naming patterns
  
  # Try standard Illumina naming pattern first (_R1_001.fastq.gz)
  fnFs.illumina <- sort(list.files(search_path, pattern="_R1_001\\.fastq\\.gz$", 
                                  full.names = TRUE, recursive = recursive))
  fnRs.illumina <- sort(list.files(search_path, pattern="_R2_001\\.fastq\\.gz$", 
                                  full.names = TRUE, recursive = recursive))
  
  # Try alternative naming patterns (_R1.fastq.gz)
  fnFs.alt1 <- sort(list.files(search_path, pattern="_R1\\.fastq\\.gz$", 
                              full.names = TRUE, recursive = recursive))
  fnRs.alt1 <- sort(list.files(search_path, pattern="_R2\\.fastq\\.gz$", 
                              full.names = TRUE, recursive = recursive))
  
  # Try other naming patterns (forward/reverse in the name)
  fnFs.alt2 <- sort(list.files(search_path, pattern="[_\\.]F[_\\.]|[_\\.]forward[_\\.]", 
                              full.names = TRUE, ignore.case = TRUE, recursive = recursive))
  fnRs.alt2 <- sort(list.files(search_path, pattern="[_\\.]R[_\\.]|[_\\.]reverse[_\\.]", 
                              full.names = TRUE, ignore.case = TRUE, recursive = recursive))
  
  # Check naming pattern and use the appropriate files
  if (length(fnFs.illumina) > 0 && length(fnRs.illumina) > 0) {
    # Use Illumina naming pattern
    fnFs <- fnFs.illumina
    fnRs <- fnRs.illumina
    cat("Using Illumina naming pattern (_R1_001.fastq.gz)\n")
    file_pattern <- "_R\\d_001\\.fastq\\.gz$"
  } else if (length(fnFs.alt1) > 0 && length(fnRs.alt1) > 0) {
    # Use alternative naming pattern
    fnFs <- fnFs.alt1
    fnRs <- fnRs.alt1
    cat("Using alternative naming pattern (_R1.fastq.gz)\n")
    file_pattern <- "_R\\d\\.fastq\\.gz$"
  } else if (length(fnFs.alt2) > 0 && length(fnRs.alt2) > 0) {
    # Use other alternative naming pattern
    fnFs <- fnFs.alt2
    fnRs <- fnRs.alt2
    cat("Using alternative naming pattern with forward/reverse in the name\n")
    file_pattern <- "\\.(fastq|fq)(\\.gz)?$"
  } else {
    # Look for any fastq files
    all_fastqs <- sort(list.files(search_path, pattern="\\.(fastq|fq)(\\.gz)?$", 
                                 full.names = TRUE, recursive = recursive))
    if (length(all_fastqs) > 0) {
      cat("WARNING: Found", length(all_fastqs), "fastq files but couldn't determine read direction.\n")
      cat("Please ensure your files follow naming conventions with _R1/_R2 or similar pattern.\n")
    }
    return(NULL)  # Return NULL to indicate no valid files found
  }
  
  # Return both file lists and the pattern
  return(list(fnFs = fnFs, fnRs = fnRs, pattern = file_pattern))
}
```

```{r locate-files}
# Function to extract sample names from filenames
extract_sample_names <- function(file_paths, pattern) {
  sample_names <- basename(file_paths)
  
  # First handle the standard Illumina pattern
  if (grepl("_R\\d_001\\.fastq\\.gz$", pattern)) {
    sample_names <- gsub("_R\\d_001\\.fastq\\.gz$", "", sample_names)
  } 
  # Then handle the simplified R1/R2 pattern
  else if (grepl("_R\\d\\.fastq\\.gz$", pattern)) {
    sample_names <- gsub("_R\\d\\.fastq\\.gz$", "", sample_names)
  }
  # General case: remove file extension and the "R1"/"R2"/"forward"/"reverse" pattern
  else {
    # Remove fastq/fq file extension
    sample_names <- gsub("\\.(fastq|fq)(\\.gz)?$", "", sample_names)
    # Remove R1/R2 or forward/reverse pattern
    sample_names <- gsub("_R[12]_?|_?[FR]_?|_forward_?|_reverse_?", "", sample_names, ignore.case = TRUE)
  }
  
  # Remove trailing underscores if present
  sample_names <- gsub("_+$", "", sample_names)
  
  return(sample_names)
}

# Detect primers from filenames
detect_primers <- function(file_names) {
  # Common 16S rRNA primers to check for
  primer_patterns <- c(
    "515F-806R" = "515f.*806r|806r.*515f",
    "341F-805R" = "341f.*805r|805r.*341f",
    "515F-926R" = "515f.*926r|926r.*515f",
    "27F-338R" = "27f.*338r|338r.*27f",
    "V4" = "v4",
    "V3-V4" = "v3.*v4|v3-v4",
    "V1-V2" = "v1.*v2|v1-v2",
    "806R" = "806r"
  )
  
  # Check for primer patterns in filenames
  all_filenames <- paste(file_names, collapse = " ")
  matches <- sapply(primer_patterns, function(pattern) {
    grepl(pattern, all_filenames, ignore.case = TRUE)
  })
  
  if (any(matches)) {
    primers <- names(which(matches))
    cat("Detected primers/regions:", paste(primers, collapse = ", "), "\n")
    return(primers[1])  # Return the first match
  } else {
    cat("No specific primers detected from filenames\n")
    return(NULL)
  }
}

# Map primer to expected amplicon length
get_expected_amplicon_size <- function(primer) {
  if (is.null(primer)) return(250)  # Default if unknown
  
  # Approximate amplicon sizes for common primer sets
  amplicon_sizes <- c(
    "515F-806R" = 253,  # V4 region
    "341F-805R" = 464,  # V3-V4 region
    "515F-926R" = 411,  # V4-V5 region
    "27F-338R" = 311,   # V1-V2 region
    "V4" = 253,
    "V3-V4" = 464,
    "V1-V2" = 311,
    "806R" = 253  # Assuming 515F-806R when only 806R is mentioned
  )
  
  # Display informative message about the primers and amplicon size
  if (primer %in% names(amplicon_sizes)) {
    size <- amplicon_sizes[primer]
    
    # For common primer sets, provide additional information
    if (primer %in% c("515F-806R", "806R", "V4")) {
      cat("Using the 515F-806R primer set (V4 region). Expected amplicon size:", size, "bp\n")
      cat("Note: While MiSeq runs 2x250 cycles, the ~253 bp amplicon size is important for\n")
      cat("      proper read merging and quality filtering parameter optimization.\n")
    } else if (primer %in% c("341F-805R", "V3-V4")) {
      cat("Using the 341F-805R primer set (V3-V4 region). Expected amplicon size:", size, "bp\n")
      cat("Note: This longer amplicon requires high-quality overlapping regions for proper merging.\n")
    } else {
      cat("Using", primer, "primers. Expected amplicon size:", size, "bp\n")
    }
    
    return(size)
  } else {
    cat("Unknown primer set. Using default amplicon size of 250 bp\n")
    cat("Warning: Parameter optimization will be less precise without\n")
    cat("         accurate amplicon size information.\n")
    return(250)  # Default if not in the list
  }
}

# Find fastq files
cat("\nSearching for FASTQ files\n")
files <- find_fastq_files(path)

if(is.null(files)) {
  stop("No valid fastq files found in data directory. Please check your file paths and naming conventions.")
}

fnFs <- files$fnFs
fnRs <- files$fnRs
file_pattern <- files$pattern

# Extract sample names
sample.names <- extract_sample_names(fnFs, file_pattern)

cat("Found", length(fnFs), "samples\n")

# Verify sample names were extracted properly
sample.df <- data.frame(
  ForwardFile = basename(fnFs),
  ReverseFile = basename(fnRs),
  SampleName = sample.names
)
kable(head(sample.df, min(10, nrow(sample.df))), caption = "Sample Names Extracted from Files")

# Verify forward and reverse files match
if (length(fnFs) != length(fnRs)) {
  stop("Number of forward and reverse files don't match. Check your files.")
}

# Get primer information from filenames
primer_info <- detect_primers(c(fnFs, fnRs))
expected_amplicon_size <- get_expected_amplicon_size(primer_info)
cat("Expected amplicon size:", expected_amplicon_size, "bp\n")
```

# Initial Quality Assessment

First, we'll examine the quality profiles of the raw reads to get a baseline understanding of the data quality.

```{r quality-plots}
# Function to efficiently sample and plot quality profiles
efficient_quality_profile <- function(fastq_files, max_files = 3, max_reads = 10000) {
  # Sample a subset of files for efficiency
  n_files <- min(max_files, length(fastq_files))
  sampled_files <- fastq_files[sample(length(fastq_files), n_files)]
  
  # Skip parallelization and use the safer direct approach
  cat("Processing quality profiles...\n")
  
  # Use the built-in aggregation from DADA2 which is more reliable
  return(dada2::plotQualityProfile(sampled_files, n = max_reads, aggregate = TRUE))
}

# Plot quality profiles with efficient sampling and parallelization
cat("Generating quality plots with optimized sampling...\n")

# Forward reads quality profile
cat("Processing forward reads quality profile...\n")
forward_qual <- efficient_quality_profile(
  fnFs, 
  max_files = min(3, length(fnFs)),
  max_reads = 10000  # Sample 10,000 reads per file for faster processing
)

# Reverse reads quality profile
cat("Processing reverse reads quality profile...\n")
reverse_qual <- efficient_quality_profile(
  fnRs,
  max_files = min(3, length(fnRs)),
  max_reads = 10000  # Sample 10,000 reads per file for faster processing
)

# Display the quality plots
print(forward_qual)
print(reverse_qual)
```

# Automatic Parameter Optimization

```{r parameter-optimization}
# Set up Bayesian Optimization based on package availability
if (!can_use_bayesian_optimization) {
  stop("This workflow requires the rBayesianOptimization package. Please restart the workflow after installing it.")
}

# Use parallel optimization if available
has_pbo <- can_use_parallel_bayesian_optimization
if (has_pbo) {
  cat("Using parallel Bayesian optimization for faster parameter tuning\n")
} else {
  cat("Using standard Bayesian optimization\n")
}

# Function to extract quality profiles from read files
extract_quality_profiles <- function(forward_files, reverse_files, max_files = 5) {
  # Sample a subset of files for efficiency
  n_sample <- min(max_files, length(forward_files))
  sample_idx <- sample(1:length(forward_files), n_sample)
  
  sample_forwards <- forward_files[sample_idx]
  sample_reverses <- reverse_files[sample_idx]
  
  # Generate quality profiles
  forward_qual <- plotQualityProfile(sample_forwards, aggregate = TRUE)
  reverse_qual <- plotQualityProfile(sample_reverses, aggregate = TRUE)
  
  # Extract quality data
  forward_data <- forward_qual$data
  reverse_data <- reverse_qual$data
  
  # Calculate read lengths
  max_forward_cycle <- max(forward_data$Cycle)
  max_reverse_cycle <- max(reverse_data$Cycle)
  
  # Determine platform type based on read length
  platform <- "Unknown"
  if (max_forward_cycle >= 250 && max_reverse_cycle >= 250) {
    platform <- "MiSeq/HiSeq (2x250 or longer)"
  } else if (max_forward_cycle >= 150 && max_reverse_cycle >= 150) {
    platform <- "MiniSeq/NextSeq (2x150)"
  } else if (max_forward_cycle >= 100 && max_reverse_cycle >= 100) {
    platform <- "NovaSeq/NextSeq (2x100)"
  }
  
  cat("Detected read lengths: Forward =", max_forward_cycle, "bp, Reverse =", max_reverse_cycle, "bp\n")
  cat("Likely sequencing platform:", platform, "\n")
  
  # Calculate mean quality by position
  forward_mean_qual <- aggregate(Score ~ Cycle, data = forward_data, FUN = mean)
  reverse_mean_qual <- aggregate(Score ~ Cycle, data = reverse_data, FUN = mean)
  
  return(list(
    forward_mean_qual = forward_mean_qual,
    reverse_mean_qual = reverse_mean_qual,
    max_forward_cycle = max_forward_cycle,
    max_reverse_cycle = max_reverse_cycle,
    platform = platform
  ))
}

# Function to determine parameter bounds based on quality profiles
determine_parameter_bounds <- function(quality_profiles, target_amplicon_size = NULL, 
                                     min_overlap = 20, quality_threshold = 25) {
  forward_mean_qual <- quality_profiles$forward_mean_qual
  reverse_mean_qual <- quality_profiles$reverse_mean_qual
  max_forward_cycle <- quality_profiles$max_forward_cycle
  max_reverse_cycle <- quality_profiles$max_reverse_cycle
  
  # Find positions where quality drops below threshold
  quality_drop_f <- which(forward_mean_qual$Score < quality_threshold)
  quality_drop_r <- which(reverse_mean_qual$Score < quality_threshold)
  
  # Find the earliest position where quality drops
  if (length(quality_drop_f) > 0) {
    earliest_drop_f <- quality_drop_f[1]
  } else {
    earliest_drop_f <- max_forward_cycle
  }
  
  if (length(quality_drop_r) > 0) {
    earliest_drop_r <- quality_drop_r[1]
  } else {
    earliest_drop_r <- max_reverse_cycle
  }
  
  # Find the lowest safe truncation points (where quality is still good)
  min_trunc_f <- max(150, floor(max_forward_cycle * 0.6)) # Minimum 150bp or 60% of read
  min_trunc_r <- max(150, floor(max_reverse_cycle * 0.6)) # Minimum 150bp or 60% of read
  
  # Determine maximum truncation values (should be before quality drops too much)
  max_trunc_f <- max_forward_cycle
  max_trunc_r <- max_reverse_cycle
  
  # If we have amplicon size information, ensure we'll have sufficient overlap
  if (!is.null(target_amplicon_size)) {
    min_combined_length <- target_amplicon_size + min_overlap
    
    # Ensure minimum truncation points will allow sufficient overlap
    if (min_trunc_f + min_trunc_r < min_combined_length) {
      # Adjust min truncation values to ensure overlap is possible
      adjustment <- ceiling((min_combined_length - (min_trunc_f + min_trunc_r)) / 2)
      min_trunc_f <- min(min_trunc_f + adjustment, max_forward_cycle - 10)
      min_trunc_r <- min(min_trunc_r + adjustment, max_reverse_cycle - 10)
    }
  }
  
  # Set maxEE bounds based on common values
  min_maxEE_f <- 1
  max_maxEE_f <- 5
  min_maxEE_r <- 1
  max_maxEE_r <- 8  # Allow higher errors for reverse reads which typically have lower quality
  
  # Round to integers for truncation lengths
  min_trunc_f <- as.integer(min_trunc_f)
  max_trunc_f <- as.integer(max_trunc_f)
  min_trunc_r <- as.integer(min_trunc_r)
  max_trunc_r <- as.integer(max_trunc_r)
  
  # Calculate expected overlap range
  if (!is.null(target_amplicon_size)) {
    min_overlap_possible <- min_trunc_f + min_trunc_r - target_amplicon_size
    max_overlap_possible <- max_trunc_f + max_trunc_r - target_amplicon_size
    cat("Possible overlap range: ", min_overlap_possible, " - ", max_overlap_possible, " bp\n", sep="")
    
    if (min_overlap_possible < min_overlap) {
      cat("WARNING: Minimum possible overlap may be less than recommended (", min_overlap, " bp)\n", sep="")
    }
  }
  
  # Return bounds for optimization
  bounds_list <- list(
    truncF_bounds = c(min_trunc_f, max_trunc_f),
    truncR_bounds = c(min_trunc_r, max_trunc_r),
    maxEE_f_bounds = c(min_maxEE_f, max_maxEE_f),
    maxEE_r_bounds = c(min_maxEE_r, max_maxEE_r),
    target_amplicon_size = target_amplicon_size
  )
  
  return(bounds_list)
}

# Create a temporary directory for parameter testing
prepare_temp_dir <- function() {
  temp_dir <- file.path(path, "temp_filter")
  if (!dir.exists(temp_dir)) dir.create(temp_dir)
  return(temp_dir)
}

# Function to evaluate a set of DADA2 parameters and return a score
# This is the objective function for Bayesian optimization
evaluate_parameters <- function(truncF, truncR, maxEE_F, maxEE_R) {
  # Ensure parameters are integers where needed
  truncF <- as.integer(truncF)
  truncR <- as.integer(truncR)
  
  # Create a temporary directory for testing
  temp_dir <- file.path(path, "temp_filter")
  if(!dir.exists(temp_dir)) dir.create(temp_dir)
  
  # Use a small subset of files for efficiency
  n_test <- min(3, length(fnFs))
  test_fnFs <- fnFs[1:n_test]
  test_fnRs <- fnRs[1:n_test]
  test_names <- sample.names[1:n_test]
  
  # Generate test file paths
  test_filtFs <- file.path(temp_dir, paste0(test_names, "_F_filt.fastq.gz"))
  test_filtRs <- file.path(temp_dir, paste0(test_names, "_R_filt.fastq.gz"))
  names(test_filtFs) <- test_names
  names(test_filtRs) <- test_names
  
  # Filter with current parameters
  test_out <- tryCatch({
    suppressWarnings(
      filterAndTrim(test_fnFs, test_filtFs, test_fnRs, test_filtRs,
                   truncLen = c(truncF, truncR),
                   maxN = 0, maxEE = c(maxEE_F, maxEE_R), truncQ = 2,
                   rm.phix = TRUE, compress = TRUE, multithread = TRUE)
    )
  }, error = function(e) {
    # Return NA if filtering fails
    return(NULL)
  })
  
  # Clean up filtered files immediately to save space
  unlink(list.files(temp_dir, full.names = TRUE))
  
  # If filtering failed, return a very low score
  if (is.null(test_out)) {
    cat("Filtering failed with parameters: truncF=", truncF, " truncR=", truncR, 
        " maxEE_F=", maxEE_F, " maxEE_R=", maxEE_R, "\n")
    return(-100)  # Penalty score for failed filtering
  }
  
  # Calculate retention rate
  reads_in <- sum(test_out[, 1])
  reads_kept <- sum(test_out[, 2])
  retention_rate <- reads_kept / reads_in
  
  # Process the merged data to evaluate overlap quality
  merge_score <- 0
  tryCatch({
    # Dereplicate filtered reads
    derepFs <- derepFastq(test_filtFs)
    derepRs <- derepFastq(test_filtRs)
    names(derepFs) <- test_names
    names(derepRs) <- test_names
    
    # Learn error rates (minimal process, just for testing)
    errF <- learnErrors(test_filtFs, multithread = TRUE, nbases = 1e8, randomize = TRUE)
    errR <- learnErrors(test_filtRs, multithread = TRUE, nbases = 1e8, randomize = TRUE)
    
    # Sample inference (minimal process, just for testing)
    dadaFs <- dada(derepFs, err = errF, multithread = TRUE, pool = FALSE)
    dadaRs <- dada(derepRs, err = errR, multithread = TRUE, pool = FALSE)
    
    # Merge paired reads - the critical step for evaluating parameter quality
    mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = FALSE)
    
    # Calculate merging success rate
    merge_input <- sum(sapply(dadaFs, function(x) sum(getUniques(x$denoised))))
    merge_output <- sum(sapply(mergers, function(x) sum(getUniques(x))))
    merge_rate <- merge_output / merge_input
    
    # Calculate merge score - weight this heavily as it's critical for success
    merge_score <- merge_rate * 50  # Scale to make it a significant part of the score
    
  }, error = function(e) {
    # If merging fails or has issues, set a low merge score
    merge_score <- 0
    cat("Merging process failed with parameters: truncF=", truncF, " truncR=", truncR, 
        " maxEE_F=", maxEE_F, " maxEE_R=", maxEE_R, "\n")
  })
  
  # Calculate amplicon length compliance score
  amplicon_score <- 0
  if (!is.null(param_bounds$target_amplicon_size)) {
    target_size <- param_bounds$target_amplicon_size
    expected_overlap <- truncF + truncR - target_size
    
    # Penalty if overlap is too small (< 20bp is problematic)
    if (expected_overlap < 20) {
      amplicon_score <- -50  # Severe penalty
    } else if (expected_overlap >= 20 && expected_overlap < 30) {
      amplicon_score <- expected_overlap - 20  # Small positive score for minimal acceptable overlap
    } else if (expected_overlap >= 30 && expected_overlap < 50) {
      amplicon_score <- 10 + (expected_overlap - 30) * 0.5  # Good score for moderate overlap
    } else {
      amplicon_score <- 20  # Maximum score for abundant overlap
    }
  }
  
  # Calculate stringency score (reducing preference for high stringency)
  # Lower maxEE values are more stringent, but we're giving this less weight
  maxEE_total <- maxEE_F + maxEE_R
  stringency_score <- 10 - (maxEE_total * 0.7)  # Less penalty for higher maxEE
  
  # Calculate read length score (higher weight to prefer keeping more of the read)
  # Higher truncation values keep more of the read
  length_proportion_f <- truncF / quality_profiles$max_forward_cycle
  length_proportion_r <- truncR / quality_profiles$max_reverse_cycle
  length_score <- (length_proportion_f + length_proportion_r) * 25  # Increased weight to favor longer reads
  
  # Calculate final combined score with appropriate weighting
  # Retention rate is important but shouldn't dominate
  retention_score <- retention_rate * 40  # Scale to make it significant
  
  # Combine all scores
  final_score <- retention_score + merge_score + amplicon_score + stringency_score + length_score
  
  # Report parameter evaluation
  cat(sprintf("Parameters (truncF=%d, truncR=%d, maxEE_F=%.1f, maxEE_R=%.1f): Retention=%.1f%%, Merge Score=%.1f, Total Score=%.1f\n",
              truncF, truncR, maxEE_F, maxEE_R, 
              retention_rate*100, merge_score, final_score))
  
  # Clean up all temporary files
  unlink(test_filtFs)
  unlink(test_filtRs)
  
  return(final_score)
}

# Create our optimization wrapper function in the format required by rBayesianOptimization
# This version includes all necessary functions for parallel workers
dada2_optimization_function <- function(truncF, truncR, maxEE_F, maxEE_R) {
  # We need to ensure that parallel workers have access to all required variables and functions
  require(dada2)
  
  # Define a self-contained version of the evaluate_parameters function
  # with copies of all necessary data and functions
  local_evaluate_parameters <- function(truncF, truncR, maxEE_F, maxEE_R) {
    # Load required variables
    library(dada2)
    
    # These variables need to be accessed from the global environment
    # We return a reasonable fallback score if they're not available
    tryCatch({
      # Try to get variables from parent environment
      truncF <- as.integer(truncF)
      truncR <- as.integer(truncR)
      
      # Default score if we can't access necessary data
      if (!exists("path") || !exists("fnFs") || !exists("fnRs") || 
          !exists("sample.names") || !exists("quality_profiles") || 
          !exists("param_bounds")) {
        warning("Required data not available in parallel worker")
        return(0)  # Neutral score
      }
      
      # Create a temporary directory for testing
      temp_dir <- tempdir()
      
      # Use a small subset of files for efficiency
      n_test <- min(3, length(fnFs))
      test_fnFs <- fnFs[1:n_test]
      test_fnRs <- fnRs[1:n_test]
      test_names <- sample.names[1:n_test]
      
      # Generate test file paths
      test_filtFs <- file.path(temp_dir, paste0(test_names, "_F_filt.fastq.gz"))
      test_filtRs <- file.path(temp_dir, paste0(test_names, "_R_filt.fastq.gz"))
      names(test_filtFs) <- test_names
      names(test_filtRs) <- test_names
      
      # Filter with current parameters
      test_out <- tryCatch({
        suppressWarnings(
          filterAndTrim(test_fnFs, test_filtFs, test_fnRs, test_filtRs,
                       truncLen = c(truncF, truncR),
                       maxN = 0, maxEE = c(maxEE_F, maxEE_R), truncQ = 2,
                       rm.phix = TRUE, compress = TRUE, multithread = TRUE)
        )
      }, error = function(e) {
        return(NULL)
      })
      
      # If filtering failed, return a very low score
      if (is.null(test_out)) {
        return(-100)  # Penalty score for failed filtering
      }
      
      # Calculate retention rate
      reads_in <- sum(test_out[, 1])
      reads_kept <- sum(test_out[, 2])
      retention_rate <- reads_kept / reads_in
      
      # Simplified scoring for parallel optimization with less aggressive truncation
      # Focus on retention rate with reduced penalty for larger maxEE
      retention_score <- retention_rate * 100  # Scale up to make it significant
      maxEE_penalty <- (maxEE_F + maxEE_R) * 1  # Reduced penalty for higher error rates
      
      # Add length reward to prefer keeping more of the read
      length_reward <- (truncF / 250 + truncR / 250) * 25  # Reward for longer reads
      
      # Calculate amplicon length compliance score
      amplicon_score <- 0
      if (!is.null(param_bounds$target_amplicon_size)) {
        target_size <- param_bounds$target_amplicon_size
        expected_overlap <- truncF + truncR - target_size
        
        # Penalty if overlap is too small
        if (expected_overlap < 20) {
          amplicon_score <- -50  # Severe penalty
        } else {
          amplicon_score <- min(expected_overlap * 0.2, 20)  # Reward for good overlap
        }
      }
      
      # Simple final score with added length reward
      final_score <- retention_score - maxEE_penalty + amplicon_score + length_reward
      
      return(final_score)
      
    }, error = function(e) {
      # If anything fails, return a neutral score
      return(0)
    })
  }
  
  # Call our local version of evaluate_parameters
  score <- local_evaluate_parameters(truncF, truncR, maxEE_F, maxEE_R)
  
  # Ensure score is numeric
  if (is.null(score) || !is.numeric(score) || is.na(score)) {
    score <- 0  # Default neutral score
  }
  
  return(list(Score = as.numeric(score)))
}

# Extract quality profiles and determine parameter bounds
cat("Analyzing read quality profiles to determine parameter bounds...\n")
quality_profiles <- extract_quality_profiles(fnFs, fnRs, max_files = 5)

# Determine parameter bounds
cat("Determining optimal parameter ranges for Bayesian optimization...\n")
param_bounds <- determine_parameter_bounds(
  quality_profiles,
  target_amplicon_size = expected_amplicon_size,
  min_overlap = 20,
  quality_threshold = 25
)

# Function to check if bounds are correctly defined
validate_bounds <- function(bounds_list) {
  valid <- TRUE
  
  # Check each expected key
  required_keys <- c("truncF_bounds", "truncR_bounds", "maxEE_f_bounds", "maxEE_r_bounds")
  for (key in required_keys) {
    if (!key %in% names(bounds_list)) {
      cat("ERROR: Missing", key, "in parameter bounds\n")
      valid <- FALSE
    } else if (!is.numeric(bounds_list[[key]]) || length(bounds_list[[key]]) != 2) {
      cat("ERROR: Invalid", key, "in parameter bounds:", toString(bounds_list[[key]]), "\n")
      valid <- FALSE
    }
  }
  
  # If any are invalid, return a fixed default bounds list
  if (!valid) {
    cat("Creating default parameter bounds\n")
    return(list(
      truncF_bounds = c(200, 240),
      truncR_bounds = c(140, 180),
      maxEE_f_bounds = c(1, 5),
      maxEE_r_bounds = c(1, 8),
      target_amplicon_size = 250
    ))
  }
  
  return(bounds_list)
}

# Double-check that param_bounds are valid before preparing bounds
param_bounds <- validate_bounds(param_bounds)

# Prepare parameter bounds for Bayesian optimization
bounds <- list(
  truncF = param_bounds$truncF_bounds,
  truncR = param_bounds$truncR_bounds,
  maxEE_F = param_bounds$maxEE_f_bounds,
  maxEE_R = param_bounds$maxEE_r_bounds
)

# Create a temporary directory for testing
temp_dir <- prepare_temp_dir()

# Set up Bayesian optimization
cat("Starting Bayesian optimization for DADA2 parameter selection...\n")
cat("This process will systematically test different parameter combinations to find the optimal settings.\n")

# Function to ensure valid parameters are extracted regardless of structure
extract_param <- function(params, param_name, default_value) {
  # First check if params is a list
  if (is.list(params)) {
    if (!is.null(params[[param_name]])) {
      return(params[[param_name]])
    }
  # Then check if it's a data frame
  } else if (is.data.frame(params)) {
    if (param_name %in% colnames(params)) {
      return(params[1, param_name])
    }
  # Then check if it's a named vector
  } else if (is.atomic(params) && !is.null(names(params))) {
    if (param_name %in% names(params)) {
      return(params[param_name])
    }
  # Then check if it's an unnamed vector with expected order
  } else if (is.atomic(params) && length(params) >= 4) {
    # Assuming standard order: truncF, truncR, maxEE_F, maxEE_R
    if (param_name == "truncF") return(params[1])
    if (param_name == "truncR") return(params[2])
    if (param_name == "maxEE_F") return(params[3])
    if (param_name == "maxEE_R") return(params[4])
  }
  # If we couldn't find the parameter, return the default
  cat("WARNING: Could not find", param_name, "in optimization results. Using default value:", default_value, "\n")
  return(default_value)
}

# Ensure maxEE values are valid
ensure_valid_maxEE <- function() {
  if (is.null(maxEE_forward) || !is.numeric(maxEE_forward) || is.na(maxEE_forward)) {
    cat("WARNING: maxEE_forward is invalid, setting to default 2.0\n")
    maxEE_forward <<- 2.0  # Use <<- to update in the parent environment
  }
  
  if (is.null(maxEE_reverse) || !is.numeric(maxEE_reverse) || is.na(maxEE_reverse)) {
    cat("WARNING: maxEE_reverse is invalid, setting to default 2.0\n")
    maxEE_reverse <<- 2.0  # Use <<- to update in the parent environment
  }
}

# Determine the best optimization approach based on available packages and cores
if (has_pbo) {
  # Use parallel Bayesian optimization
  cat(sprintf("Running parallel Bayesian optimization with %d cores\n", usable_cores))
  
  # Define the parameter space
  param_set <- list(
    truncF = c(bounds$truncF[1], bounds$truncF[2]),
    truncR = c(bounds$truncR[1], bounds$truncR[2]),
    maxEE_F = c(bounds$maxEE_F[1], bounds$maxEE_F[2]),
    maxEE_R = c(bounds$maxEE_R[1], bounds$maxEE_R[2])
  )
  
  # Given the parallel processing challenges, let's fall back to standard Bayesian optimization
  # which is more reliable across different setups
  cat("Using standard Bayesian optimization instead of parallel version...\n")
  
  # Run standard (non-parallel) Bayesian optimization with rBayesianOptimization
  optim_results <- rBayesianOptimization::BayesianOptimization(
    FUN = dada2_optimization_function,
    bounds = param_set,
    init_points = 5,
    n_iter = 15,
    acq = "ucb",
    verbose = TRUE
  )
  
  # Check if scoreSummary exists or use Best_Par instead
  if (!is.null(optim_results$scoreSummary)) {
    cat("Using scoreSummary for best parameters\n")
    best_params <- optim_results$scoreSummary[which.max(optim_results$scoreSummary$Score),]
  } else if (!is.null(optim_results$Best_Par)) {
    cat("Using Best_Par for best parameters\n")
    best_params <- optim_results$Best_Par
  } else {
    cat("WARNING: Could not find parameters in optimization results\n")
    # Create default parameters
    best_params <- list(truncF = 240, truncR = 160, maxEE_F = 2.0, maxEE_R = 2.0)
  }
  
  # Extract parameters with safe function
  truncLen_forward <- as.integer(extract_param(best_params, "truncF", 240))
  truncLen_reverse <- as.integer(extract_param(best_params, "truncR", 160))
  maxEE_forward <- as.numeric(extract_param(best_params, "maxEE_F", 2.0))
  maxEE_reverse <- as.numeric(extract_param(best_params, "maxEE_R", 2.0))
  
  # Validate the parameters
  ensure_valid_maxEE()
  
  # Report extracted values
  cat("Extracted parameters:\n")
  cat("  truncLen_forward:", truncLen_forward, "\n")
  cat("  truncLen_reverse:", truncLen_reverse, "\n")
  cat("  maxEE_forward:", maxEE_forward, "\n")
  cat("  maxEE_reverse:", maxEE_reverse, "\n")
  
} else {
  # Use standard Bayesian optimization
  cat("Running standard Bayesian optimization\n")
  
  # Define the parameter space
  param_set <- list(
    truncF = c(bounds$truncF[1], bounds$truncF[2]),
    truncR = c(bounds$truncR[1], bounds$truncR[2]),
    maxEE_F = c(bounds$maxEE_F[1], bounds$maxEE_F[2]),
    maxEE_R = c(bounds$maxEE_R[1], bounds$maxEE_R[2])
  )
  
  # Adjust maxiterations based on system capability
  max_iter <- ifelse(total_cores >= 8, 20, 15)
  
  # Run Bayesian optimization
  optim_results <- rBayesianOptimization::BayesianOptimization(
    FUN = dada2_optimization_function,
    bounds = param_set,
    init_points = 5,
    n_iter = max_iter,
    acq = "ucb",
    verbose = TRUE
  )
  
  # Check if Best_Par exists 
  if (!is.null(optim_results$Best_Par)) {
    cat("Using Best_Par for best parameters\n")
    best_params <- optim_results$Best_Par
  } else {
    cat("WARNING: Could not find Best_Par in optimization results\n")
    # Create default parameters
    best_params <- list(truncF = 240, truncR = 160, maxEE_F = 2.0, maxEE_R = 2.0)
  }
  
  # Extract parameters with safe function
  truncLen_forward <- as.integer(extract_param(best_params, "truncF", 240))
  truncLen_reverse <- as.integer(extract_param(best_params, "truncR", 160))
  maxEE_forward <- as.numeric(extract_param(best_params, "maxEE_F", 2.0))
  maxEE_reverse <- as.numeric(extract_param(best_params, "maxEE_R", 2.0))
  
  # Validate the parameters
  ensure_valid_maxEE()
  
  # Report extracted values
  cat("Extracted parameters:\n")
  cat("  truncLen_forward:", truncLen_forward, "\n")
  cat("  truncLen_reverse:", truncLen_reverse, "\n")
  cat("  maxEE_forward:", maxEE_forward, "\n")
  cat("  maxEE_reverse:", maxEE_reverse, "\n")
}

# Calculate expected overlap with optimized parameters
expected_overlap <- truncLen_forward + truncLen_reverse
if (!is.null(param_bounds$target_amplicon_size)) {
  expected_overlap <- expected_overlap - param_bounds$target_amplicon_size
}

# Visualize the quality profiles with optimized truncation points
forward_plot <- ggplot(quality_profiles$forward_mean_qual, aes(x = Cycle, y = Score)) +
  geom_line(size = 1) +
  geom_hline(yintercept = 25, linetype = "dashed", color = "red") +
  geom_vline(xintercept = truncLen_forward, linetype = "dashed", color = "blue") +
  scale_y_continuous(limits = c(0, 40)) +
  labs(title = paste("Forward Read Quality -", quality_profiles$platform), 
       subtitle = paste("Bayesian optimized truncation at position", truncLen_forward),
       x = "Cycle", y = "Quality Score") +
  theme_minimal()

reverse_plot <- ggplot(quality_profiles$reverse_mean_qual, aes(x = Cycle, y = Score)) +
  geom_line(size = 1) +
  geom_hline(yintercept = 25, linetype = "dashed", color = "red") +
  geom_vline(xintercept = truncLen_reverse, linetype = "dashed", color = "blue") +
  scale_y_continuous(limits = c(0, 40)) +
  labs(title = paste("Reverse Read Quality -", quality_profiles$platform), 
       subtitle = paste("Bayesian optimized truncation at position", truncLen_reverse),
       x = "Cycle", y = "Quality Score") +
  theme_minimal()

# Display plots together
grid.arrange(forward_plot, reverse_plot, ncol = 2)

# Create a function to test the optimized parameters on a larger dataset
# This validates our optimized parameters
validate_optimized_parameters <- function(truncF, truncR, maxEE_F, maxEE_R, 
                                        validate_size = 5) {
  # Make sure both truncation lengths are integers
  truncF <- as.integer(truncF)
  truncR <- as.integer(truncR)
  
  # Create a temporary directory for validation
  if (!exists("temp_dir") || !dir.exists(temp_dir)) {
    temp_dir <- file.path(path, "temp_filter")
    if (!dir.exists(temp_dir)) {
      dir.create(temp_dir)
    }
  }
  
  # Use a larger subset for validation
  n_validate <- min(validate_size, length(fnFs))
  validate_fnFs <- fnFs[1:n_validate]
  validate_fnRs <- fnRs[1:n_validate]
  validate_names <- sample.names[1:n_validate]
  
  # Create filtered file paths
  validate_filtFs <- file.path(temp_dir, paste0(validate_names, "_F_filt.fastq.gz"))
  validate_filtRs <- file.path(temp_dir, paste0(validate_names, "_R_filt.fastq.gz"))
  names(validate_filtFs) <- validate_names
  names(validate_filtRs) <- validate_names
  
  # Create safe parameter vectors for the validation step
  SAFE_TRUNCLEN <- c(240, 160)  # Safe defaults
  SAFE_MAXEE <- c(2.0, 2.0)     # Safe defaults
  
  # Check if the provided parameters are valid
  if (!is.null(truncF) && !is.na(truncF) && !is.null(truncR) && !is.na(truncR)) {
    SAFE_TRUNCLEN <- c(as.integer(truncF), as.integer(truncR))
    cat(sprintf("Using provided truncation lengths: forward=%d, reverse=%d\n", 
                SAFE_TRUNCLEN[1], SAFE_TRUNCLEN[2]))
  } else {
    cat("WARNING: Using default truncation lengths for validation\n")
  }
  
  if (!is.null(maxEE_F) && !is.na(maxEE_F) && !is.null(maxEE_R) && !is.na(maxEE_R)) {
    SAFE_MAXEE <- c(as.numeric(maxEE_F), as.numeric(maxEE_R))
    cat(sprintf("Using provided maxEE values: forward=%.1f, reverse=%.1f\n", 
                SAFE_MAXEE[1], SAFE_MAXEE[2]))
  } else {
    cat("WARNING: Using default maxEE values for validation\n")
  }
  
  # Filter with safe parameters
  validate_out <- suppressWarnings(
    filterAndTrim(validate_fnFs, validate_filtFs, validate_fnRs, validate_filtRs,
                 truncLen = SAFE_TRUNCLEN,
                 maxN = 0, maxEE = SAFE_MAXEE, truncQ = 2,
                 rm.phix = TRUE, compress = TRUE, multithread = FALSE)
  )
  
  # Calculate overall retention rate
  reads_in <- sum(validate_out[, 1])
  reads_kept <- sum(validate_out[, 2])
  retention_rate <- round(reads_kept / reads_in * 100, 1)
  
  # Calculate per-sample retention rates
  sample_retention <- round(validate_out[, 2] / validate_out[, 1] * 100, 1)
  
  # Create a validation summary
  validation <- list(
    retention_rate = retention_rate,
    sample_retention = sample_retention,
    reads_in = reads_in,
    reads_kept = reads_kept
  )
  
  # Clean up validation files
  unlink(validate_filtFs)
  unlink(validate_filtRs)
  
  return(validation)
}

# Validate the optimized parameters
cat("\nValidating optimized parameters on a larger dataset...\n")

# Create temporary validation directory
temp_dir <- file.path(path, "temp_filter")
if (!dir.exists(temp_dir)) {
  dir.create(temp_dir)
}

# Directly perform validation without the function to avoid parameter passing issues
n_validate <- min(5, length(fnFs))
validate_fnFs <- fnFs[1:n_validate]
validate_fnRs <- fnRs[1:n_validate]
validate_names <- sample.names[1:n_validate]

# Create filtered file paths
validate_filtFs <- file.path(temp_dir, paste0(validate_names, "_F_filt.fastq.gz"))
validate_filtRs <- file.path(temp_dir, paste0(validate_names, "_R_filt.fastq.gz"))
names(validate_filtFs) <- validate_names
names(validate_filtRs) <- validate_names

# Run our safety check to ensure maxEE values are valid
ensure_valid_maxEE()

# Create the global parameter vectors that will be used throughout the code
GLOBAL_TRUNCLEN_VECTOR <- c(truncLen_forward, truncLen_reverse)
GLOBAL_MAXEE_VECTOR <- c(maxEE_forward, maxEE_reverse)

cat("Parameters for validation:\n")
cat("truncLen = c(", truncLen_forward, ", ", truncLen_reverse, ")\n", sep="")
cat("maxEE = c(", maxEE_forward, ", ", maxEE_reverse, ")\n", sep="")

# Filter with optimized parameters
validate_out <- tryCatch({
  filterAndTrim(
    validate_fnFs, validate_filtFs, 
    validate_fnRs, validate_filtRs,
    truncLen = GLOBAL_TRUNCLEN_VECTOR,
    maxN = 0, 
    maxEE = GLOBAL_MAXEE_VECTOR,
    truncQ = 2,
    rm.phix = TRUE, 
    compress = TRUE, 
    multithread = FALSE
  )
}, error = function(e) {
  cat("Error in validation:", conditionMessage(e), "\n")
  # Return a dummy result in case of error
  dummy_result <- matrix(c(1000, 500), nrow=length(validate_names), ncol=2, 
                        dimnames=list(validate_names, c("reads.in", "reads.out")))
  return(dummy_result)
})

# Calculate overall retention rate
reads_in <- sum(validate_out[, 1])
reads_kept <- sum(validate_out[, 2])
retention_rate <- round(reads_kept / reads_in * 100, 1)

# Calculate per-sample retention rates
sample_retention <- round(validate_out[, 2] / validate_out[, 1] * 100, 1)

# Create a validation summary
validation <- list(
  retention_rate = retention_rate,
  sample_retention = sample_retention,
  reads_in = reads_in,
  reads_kept = reads_kept
)

# Clean up validation files
unlink(validate_filtFs)
unlink(validate_filtRs)

# Ensure valid parameters
ensure_valid_maxEE()

# Create parameter summary table
platform_value <- ifelse(exists("quality_profiles") && is.list(quality_profiles) && "platform" %in% names(quality_profiles),
                        quality_profiles$platform, "Unknown")
forward_length <- ifelse(exists("quality_profiles") && is.list(quality_profiles) && "max_forward_cycle" %in% names(quality_profiles),
                        quality_profiles$max_forward_cycle, NA)
reverse_length <- ifelse(exists("quality_profiles") && is.list(quality_profiles) && "max_reverse_cycle" %in% names(quality_profiles),
                        quality_profiles$max_reverse_cycle, NA)

# Create a consolidated parameter summary dataframe
param_summary <- data.frame(
  Parameter = c(
    "Platform", 
    "Forward Read Length", 
    "Reverse Read Length",
    "Forward Truncation Length", 
    "Reverse Truncation Length",
    "Forward maxEE", 
    "Reverse maxEE",
    "Expected Amplicon Size",
    "Expected Overlap"
  ),
  Value = c(
    platform_value,
    forward_length,
    reverse_length,
    truncLen_forward,
    truncLen_reverse,
    sprintf("%.1f", maxEE_forward),
    sprintf("%.1f", maxEE_reverse),
    ifelse(exists("expected_amplicon_size"), expected_amplicon_size, NA),
    ifelse(exists("expected_overlap"), expected_overlap, NA)
  ),
  Notes = c(
    "Detected sequencing platform",
    "Maximum cycle number in forward reads",
    "Maximum cycle number in reverse reads",
    "Bayesian optimized truncation position",
    "Bayesian optimized truncation position",
    "Bayesian optimized maximum expected errors",
    "Bayesian optimized maximum expected errors",
    "Expected amplicon size based on primer",
    "Expected overlap after truncation"
  ),
  stringsAsFactors = FALSE
)

# Display the parameter summary table
kable(param_summary, caption = "Bayesian Optimized DADA2 Parameters")

# Create a separate validation results table
if (exists("validation") && is.list(validation)) {
  validation_summary <- data.frame(
    Parameter = c("Overall Read Retention", "Min Sample Retention", "Max Sample Retention"),
    Value = c(paste0(validation$retention_rate, "%"),
              paste0(min(validation$sample_retention), "%"),
              paste0(max(validation$sample_retention), "%")),
    Notes = c("Percentage of reads retained after filtering",
              "Lowest retention rate among samples",
              "Highest retention rate among samples")
  )
  kable(validation_summary, caption = "Validation Results")
}

# Plot the optimization history if using rBayesianOptimization
if (!has_pbo) {
  # Extract optimization history
  history <- optim_results$History
  history$Round <- 1:nrow(history)
  
  # Plot optimization progress
  opt_plot <- ggplot(history, aes(x = Round, y = Value)) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = optim_results$Best_Value, linetype = "dashed", color = "red") +
    labs(title = "DADA2 Parameter Optimization Progress",
         subtitle = paste("Best score:", round(optim_results$Best_Value, 2)),
         x = "Optimization Round", 
         y = "Parameter Score") +
    theme_minimal()
  
  print(opt_plot)
}

# Clean up temporary directory
unlink(temp_dir, recursive = TRUE)

# Ensure parameters are properly formatted
truncLen_forward <- as.integer(truncLen_forward)
truncLen_reverse <- as.integer(truncLen_reverse)
maxEE_forward <- as.numeric(maxEE_forward)
maxEE_reverse <- as.numeric(maxEE_reverse)

# Set up global parameter vectors for use throughout the workflow
GLOBAL_TRUNCLEN_VECTOR <- c(truncLen_forward, truncLen_reverse)
GLOBAL_MAXEE_VECTOR <- c(maxEE_forward, maxEE_reverse)

# Display final parameters for the rest of the workflow
cat("\nFinal parameters for filtering:\n")
cat("truncLen = c(", truncLen_forward, ", ", truncLen_reverse, ")\n", sep="")
cat("maxEE = c(", round(maxEE_forward, 1), ", ", round(maxEE_reverse, 1), ")\n", sep="")

# Store optimization results for reporting
platform_value <- ifelse(exists("quality_profiles") && is.list(quality_profiles) && "platform" %in% names(quality_profiles),
                         quality_profiles$platform, "Unknown")
expected_overlap_value <- ifelse(exists("expected_overlap") && !is.null(expected_overlap), 
                                expected_overlap, NA)
expected_amplicon_size_value <- ifelse(exists("expected_amplicon_size") && !is.null(expected_amplicon_size), 
                                     expected_amplicon_size, NA)

# Create the results summary with appropriate values
bayesian_optimization_results <- list(
  method = "Bayesian Optimization with Fallbacks",
  platform = platform_value,
  parameters = list(
    truncLen = c(forward = truncLen_forward, reverse = truncLen_reverse),
    maxEE = c(forward = round(maxEE_forward, 1), reverse = round(maxEE_reverse, 1))
  ),
  validation = validation,
  expected_overlap = expected_overlap_value,
  target_amplicon_size = expected_amplicon_size_value,
  parameter_source = "Bayesian optimization"
)

# Save optimization results for reporting/dashboards
if (!dir.exists("results")) dir.create("results")
saveRDS(bayesian_optimization_results, "results/bayesian_optimization_results.rds")
```

# Filter and Trim

Now we apply the optimized filtering parameters to our dataset.

```{r filter-trim}
# Load necessary libraries for the progress visualization
library(ggplot2)
library(gridExtra)

# CRITICAL PARAMETER VERIFICATION FUNCTION
ensure_valid_parameters <- function() {
  # Define default parameters to use if the optimization fails
  default_truncLen_F <- 240  # Typical good value for MiSeq/HiSeq forward reads
  default_truncLen_R <- 160  # Typical good value for MiSeq/HiSeq reverse reads
  default_maxEE_F <- 2.0     # Standard value for maxEE forward
  default_maxEE_R <- 2.0     # Standard value for maxEE reverse
  
  # Check and fix truncation lengths
  if (!exists("truncLen_forward") || is.null(truncLen_forward) || length(truncLen_forward) == 0 || !is.numeric(truncLen_forward)) {
    cat("WARNING: truncLen_forward is missing or invalid. Using default:", default_truncLen_F, "\n")
    truncLen_forward <<- default_truncLen_F
  } else {
    truncLen_forward <<- as.integer(truncLen_forward)
    cat("Using truncLen_forward:", truncLen_forward, "\n")
  }
  
  if (!exists("truncLen_reverse") || is.null(truncLen_reverse) || length(truncLen_reverse) == 0 || !is.numeric(truncLen_reverse)) {
    cat("WARNING: truncLen_reverse is missing or invalid. Using default:", default_truncLen_R, "\n")
    truncLen_reverse <<- default_truncLen_R
  } else {
    truncLen_reverse <<- as.integer(truncLen_reverse)
    cat("Using truncLen_reverse:", truncLen_reverse, "\n")
  }
  
  # Check and fix maxEE values
  if (!exists("maxEE_forward") || is.null(maxEE_forward) || length(maxEE_forward) == 0 || !is.numeric(maxEE_forward)) {
    cat("WARNING: maxEE_forward is missing or invalid. Using default:", default_maxEE_F, "\n")
    maxEE_forward <<- default_maxEE_F
  } else {
    maxEE_forward <<- as.numeric(maxEE_forward)
    cat("Using maxEE_forward:", maxEE_forward, "\n")
  }
  
  if (!exists("maxEE_reverse") || is.null(maxEE_reverse) || length(maxEE_reverse) == 0 || !is.numeric(maxEE_reverse)) {
    cat("WARNING: maxEE_reverse is missing or invalid. Using default:", default_maxEE_R, "\n") 
    maxEE_reverse <<- default_maxEE_R
  } else {
    maxEE_reverse <<- as.numeric(maxEE_reverse)
    cat("Using maxEE_reverse:", maxEE_reverse, "\n")
  }
  
  # Create or update the GLOBAL parameter vectors
  # These are the vectors that will actually be used in filterAndTrim calls
  GLOBAL_TRUNCLEN_VECTOR <<- c(truncLen_forward, truncLen_reverse)
  GLOBAL_MAXEE_VECTOR <<- c(maxEE_forward, maxEE_reverse)
  
  # Extra validation to ensure the vectors are properly formed
  if (length(GLOBAL_TRUNCLEN_VECTOR) != 2) {
    cat("ERROR: GLOBAL_TRUNCLEN_VECTOR has incorrect length. Recreating with default values.\n")
    GLOBAL_TRUNCLEN_VECTOR <<- c(default_truncLen_F, default_truncLen_R)
  }
  
  if (length(GLOBAL_MAXEE_VECTOR) != 2) {
    cat("ERROR: GLOBAL_MAXEE_VECTOR has incorrect length. Recreating with default values.\n")
    GLOBAL_MAXEE_VECTOR <<- c(default_maxEE_F, default_maxEE_R)
  }
  
  # Verify the vectors are valid
  cat("FINAL PARAMETERS FOR FILTERING:\n")
  cat("truncLen_forward =", truncLen_forward, "\n")
  cat("truncLen_reverse =", truncLen_reverse, "\n")
  cat("maxEE_forward =", maxEE_forward, "\n")
  cat("maxEE_reverse =", maxEE_reverse, "\n")
  cat("GLOBAL_TRUNCLEN_VECTOR =", paste(GLOBAL_TRUNCLEN_VECTOR, collapse=", "), "\n")
  cat("GLOBAL_MAXEE_VECTOR =", paste(GLOBAL_MAXEE_VECTOR, collapse=", "), "\n")
  
  # Return TRUE to indicate successful validation
  return(TRUE)
}

# Create directory for filtered files
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# Make sure the filtered directory exists
if(!dir.exists(file.path(path, "filtered"))) {
  dir.create(file.path(path, "filtered"))
}

# ENSURE PARAMETERS ARE VALID BEFORE FILTERING
cat("\n=== VALIDATING FILTERING PARAMETERS ===\n")
ensure_valid_parameters()
cat("=======================================\n")

# Set up real-time progress visualization
cat("Setting up real-time filtering progress visualization...\n")

# Initialize progress tracking data frame
progress_data <- data.frame(
  Batch = integer(0),
  CompletedSamples = integer(0),
  TotalSamples = integer(0),
  ReadsIn = integer(0),
  ReadsOut = integer(0),
  RetentionRate = numeric(0),
  BatchCompletionTime = numeric(0)
)

# Function to update the progress visualization
update_progress_visualization <- function(progress_data) {
  # Create progress bar
  progress_plot <- ggplot(progress_data, aes(x = Batch)) +
    geom_bar(aes(y = RetentionRate * 100), stat = "identity", fill = "steelblue", alpha = 0.7) +
    geom_text(aes(y = RetentionRate * 100 + 3, 
                  label = sprintf("%.1f%%", RetentionRate * 100)), 
              size = 3, vjust = 0) +
    geom_line(aes(y = cumsum(ReadsOut) / cumsum(ReadsIn) * 100, group = 1), 
              color = "red", size = 1) +
    geom_point(aes(y = cumsum(ReadsOut) / cumsum(ReadsIn) * 100), 
               color = "red", size = 3) +
    theme_minimal() +
    labs(title = "Filtering Progress by Batch",
         subtitle = sprintf("Processed %d of %d samples (%.1f%%)", 
                           sum(progress_data$CompletedSamples),
                           progress_data$TotalSamples[1],
                           sum(progress_data$CompletedSamples) / progress_data$TotalSamples[1] * 100),
         x = "Batch", 
         y = "Retention Rate (%)") +
    geom_hline(yintercept = 50, linetype = "dashed", color = "darkred") +
    annotate("text", x = 1, y = 52, label = "50% threshold", hjust = 0) +
    scale_y_continuous(limits = c(0, 100))
  
  # Return the plot
  return(progress_plot)
}

# Filter and trim with optimized parameters and parallelization
cat("Filtering and trimming reads with optimized parameters and parallelization...\n")

# For large datasets, use batched filtering
if(length(fnFs) > 50) {
  cat("Using batched filtering for large dataset...\n")
  
  # Process in batches to better manage memory
  batch_size <- min(20, ceiling(length(fnFs)/4))
  
  # Apple Silicon specific batch size tuning
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # M-series chips handle larger batch sizes efficiently due to unified memory
    batch_size <- min(30, ceiling(length(fnFs)/3))
    cat(sprintf("Apple Silicon optimization: Using larger batch size of %d samples...\n", batch_size))
  }
  
  # HARDCODE default values first, then try to override them
  hard_truncLen_forward <- 240
  hard_truncLen_reverse <- 160
  hard_maxEE_forward <- 2.0
  hard_maxEE_reverse <- 2.0
  
  # Try to use optimized values if they exist and are valid
  cat("DEBUG: Checking for truncLen_forward...\n")
  if (exists("truncLen_forward")) {
    cat("  - truncLen_forward exists, value:", truncLen_forward, "\n")
    if (!is.null(truncLen_forward) && !is.na(truncLen_forward)) {
      tryCatch({
        num_val <- as.integer(truncLen_forward)
        if (!is.na(num_val) && num_val > 0) {
          hard_truncLen_forward <- num_val
          cat("  - Using truncLen_forward value:", hard_truncLen_forward, "\n")
        }
      }, error = function(e) {
        cat("  - Error converting truncLen_forward:", conditionMessage(e), "\n")
      })
    }
  } else {
    cat("  - truncLen_forward does not exist\n")
  }
  
  cat("DEBUG: Checking for truncLen_reverse...\n")
  if (exists("truncLen_reverse")) {
    cat("  - truncLen_reverse exists, value:", truncLen_reverse, "\n")
    if (!is.null(truncLen_reverse) && !is.na(truncLen_reverse)) {
      tryCatch({
        num_val <- as.integer(truncLen_reverse)
        if (!is.na(num_val) && num_val > 0) {
          hard_truncLen_reverse <- num_val
          cat("  - Using truncLen_reverse value:", hard_truncLen_reverse, "\n")
        }
      }, error = function(e) {
        cat("  - Error converting truncLen_reverse:", conditionMessage(e), "\n")
      })
    }
  } else {
    cat("  - truncLen_reverse does not exist\n")
  }
  
  # Check maxEE parameters with the same careful approach
  cat("DEBUG: Checking for maxEE_forward...\n")
  tryCatch({
    if (exists("maxEE_forward")) {
      # Check value without direct reference that might error
      is_valid <- FALSE
      
      # First check if it's NULL
      is_null <- tryCatch({
        is.null(maxEE_forward)
      }, error = function(e) {
        cat("  - Error checking if maxEE_forward is NULL:", conditionMessage(e), "\n")
        TRUE  # Assume NULL if we can't check
      })
      
      if (!is_null) {
        # Try to safely print the value
        tryCatch({
          if (is.na(maxEE_forward)) {
            cat("  - maxEE_forward is NA\n")
          } else {
            cat("  - maxEE_forward exists, value:", as.character(maxEE_forward), "\n")
            is_valid <- TRUE
          }
        }, error = function(e) {
          cat("  - Error printing maxEE_forward:", conditionMessage(e), "\n")
        })
        
        # Only try to convert if previous checks passed
        if (is_valid) {
          tryCatch({
            num_val <- as.numeric(maxEE_forward)
            if (!is.na(num_val) && num_val > 0) {
              hard_maxEE_forward <- num_val
              cat("  - Using maxEE_forward value:", hard_maxEE_forward, "\n")
            } else {
              cat("  - maxEE_forward converted to invalid value:", num_val, "\n")
            }
          }, error = function(e) {
            cat("  - Error converting maxEE_forward:", conditionMessage(e), "\n")
          })
        }
      } else {
        cat("  - maxEE_forward is NULL\n")
      }
    } else {
      cat("  - maxEE_forward does not exist\n")
    }
  }, error = function(e) {
    cat("  - Unexpected error processing maxEE_forward:", conditionMessage(e), "\n")
  })
  
  cat("DEBUG: Checking for maxEE_reverse...\n")
  tryCatch({
    if (exists("maxEE_reverse")) {
      # Check value without direct reference that might error
      is_valid <- FALSE
      
      # First check if it's NULL
      is_null <- tryCatch({
        is.null(maxEE_reverse)
      }, error = function(e) {
        cat("  - Error checking if maxEE_reverse is NULL:", conditionMessage(e), "\n")
        TRUE  # Assume NULL if we can't check
      })
      
      if (!is_null) {
        # Try to safely print the value
        tryCatch({
          if (is.na(maxEE_reverse)) {
            cat("  - maxEE_reverse is NA\n")
          } else {
            cat("  - maxEE_reverse exists, value:", as.character(maxEE_reverse), "\n")
            is_valid <- TRUE
          }
        }, error = function(e) {
          cat("  - Error printing maxEE_reverse:", conditionMessage(e), "\n")
        })
        
        # Only try to convert if previous checks passed
        if (is_valid) {
          tryCatch({
            num_val <- as.numeric(maxEE_reverse)
            if (!is.na(num_val) && num_val > 0) {
              hard_maxEE_reverse <- num_val
              cat("  - Using maxEE_reverse value:", hard_maxEE_reverse, "\n")
            } else {
              cat("  - maxEE_reverse converted to invalid value:", num_val, "\n")
            }
          }, error = function(e) {
            cat("  - Error converting maxEE_reverse:", conditionMessage(e), "\n")
          })
        }
      } else {
        cat("  - maxEE_reverse is NULL\n")
      }
    } else {
      cat("  - maxEE_reverse does not exist\n")
    }
  }, error = function(e) {
    cat("  - Unexpected error processing maxEE_reverse:", conditionMessage(e), "\n")
  })
  
  # Final parameter values - use direct numeric values in the vectors
  cat("\nFINAL PARAMETERS FOR FILTERING:\n")
  cat("truncLen_forward =", hard_truncLen_forward, "\n")
  cat("truncLen_reverse =", hard_truncLen_reverse, "\n")
  cat("maxEE_forward =", hard_maxEE_forward, "\n")
  cat("maxEE_reverse =", hard_maxEE_reverse, "\n")
  
  # Create the parameter vectors directly with numeric literals
  # This avoids any possible reference to other variables
  truncLen_vector <- c(hard_truncLen_forward, hard_truncLen_reverse)
  maxEE_vector <- c(hard_maxEE_forward, hard_maxEE_reverse)
  
  cat("\nVERIFICATION OF PARAMETER VECTORS:\n")
  cat("truncLen vector:", paste(truncLen_vector, collapse=", "), "\n")
  cat("Class of truncLen vector:", class(truncLen_vector), "\n")
  cat("Length of truncLen vector:", length(truncLen_vector), "\n")
  
  # Extra check to ensure vector has correct length
  if (length(truncLen_vector) != 2) {
    cat("WARNING: truncLen_vector does not have length 2, recreating with default values\n")
    truncLen_vector <- c(240, 160)  # Hardcoded defaults
    hard_truncLen_forward <- 240
    hard_truncLen_reverse <- 160
  }
  
  cat("maxEE vector:", paste(maxEE_vector, collapse=", "), "\n")
  
  # Extra check for maxEE vector
  if (length(maxEE_vector) != 2) {
    cat("WARNING: maxEE_vector does not have length 2, recreating with default values\n")
    maxEE_vector <- c(2.0, 2.0)  # Hardcoded defaults
    hard_maxEE_forward <- 2.0
    hard_maxEE_reverse <- 2.0
  }
  
  cat(sprintf("Processing in batches of %d samples...\n", batch_size))
  
  # Create batches
  batches <- ceiling(seq_along(fnFs) / batch_size)
  
  # Initialize out matrix
  out <- matrix(0, nrow = length(sample.names), ncol = 2)
  rownames(out) <- sample.names
  colnames(out) <- c("reads.in", "reads.out")
  
  # Process each batch
  total_start_time <- Sys.time()
  for(batch in unique(batches)) {
    # Start timer for this batch
    batch_start_time <- Sys.time()
    
    cat(sprintf("Processing batch %d of %d...\n", batch, max(batches)))
    
    # Get indices for this batch
    idx <- which(batches == batch)
    
    # Make sure global parameters are valid before each batch
    if (!exists("GLOBAL_TRUNCLEN_VECTOR") || length(GLOBAL_TRUNCLEN_VECTOR) != 2) {
      cat("WARNING: GLOBAL_TRUNCLEN_VECTOR is missing or invalid. Recreating with defaults.\n")
      GLOBAL_TRUNCLEN_VECTOR <<- c(240, 160)
    }
    if (!exists("GLOBAL_MAXEE_VECTOR") || length(GLOBAL_MAXEE_VECTOR) != 2) {
      cat("WARNING: GLOBAL_MAXEE_VECTOR is missing or invalid. Recreating with defaults.\n")
      GLOBAL_MAXEE_VECTOR <<- c(2.0, 2.0)
    }
    
    # Use validated global parameter vectors
    cat("\nUSING VALIDATED GLOBAL PARAMETERS FOR FILTERING BATCH:\n")
    cat("truncLen =", paste(GLOBAL_TRUNCLEN_VECTOR, collapse=", "), "\n")
    cat("maxEE =", paste(GLOBAL_MAXEE_VECTOR, collapse=", "), "\n")
    
    # Use our guaranteed global parameter vectors
    batch_out <- filterAndTrim(
      fnFs[idx], filtFs[idx], fnRs[idx], filtRs[idx],
      truncLen = GLOBAL_TRUNCLEN_VECTOR,  # Using validated global vector
      maxN = 0, 
      maxEE = GLOBAL_MAXEE_VECTOR,        # Using validated global vector
      truncQ = 2,
      rm.phix = TRUE, 
      compress = TRUE, 
      multithread = TRUE
    )
    
    # Store results
    out[rownames(batch_out), ] <- batch_out
    
    # Calculate batch statistics
    batch_reads_in <- sum(batch_out[, 1])
    batch_reads_out <- sum(batch_out[, 2])
    batch_retention <- batch_reads_out / batch_reads_in
    batch_time <- as.numeric(difftime(Sys.time(), batch_start_time, units = "secs"))
    
    # Update progress data
    progress_data <- rbind(progress_data, data.frame(
      Batch = batch,
      CompletedSamples = length(idx),
      TotalSamples = length(sample.names),
      ReadsIn = batch_reads_in,
      ReadsOut = batch_reads_out,
      RetentionRate = batch_retention,
      BatchCompletionTime = batch_time
    ))
    
    # Update and display progress visualization
    progress_viz <- update_progress_visualization(progress_data)
    print(progress_viz)
    
    # Display additional batch metrics
    cat(sprintf("Batch %d: Processed %d samples in %.1f seconds (%.1f%% retention)\n", 
                batch, length(idx), batch_time, batch_retention * 100))
    
    # Calculate and display overall progress
    overall_retention <- sum(progress_data$ReadsOut) / sum(progress_data$ReadsIn) * 100
    completed_samples <- sum(progress_data$CompletedSamples)
    completed_pct <- completed_samples / length(sample.names) * 100
    
    cat(sprintf("Overall Progress: %d/%d samples (%.1f%%) with %.1f%% overall retention\n", 
                completed_samples, length(sample.names), completed_pct, overall_retention))
    
    # Force garbage collection with Apple Silicon optimization
    if (exists("hw_caps") && hw_caps$apple_silicon) {
      # Less aggressive GC on Apple Silicon (more efficient memory management)
      # Only run GC every third batch or at the end
      if (batch %% 3 == 0 || batch == max(batches)) {
        gc(verbose = FALSE)
      }
    } else {
      # Standard GC for other architectures
      gc()
    }
  }
  
  # Create a final summary visualization for the entire dataset
  total_time <- as.numeric(difftime(Sys.time(), total_start_time, units = "secs"))
  
  # Create a time-performance plot
  time_performance <- ggplot(progress_data, aes(x = Batch)) +
    geom_bar(aes(y = BatchCompletionTime), stat = "identity", fill = "darkgreen", alpha = 0.7) +
    geom_text(aes(y = BatchCompletionTime + 2, label = sprintf("%.1f s", BatchCompletionTime)), 
              size = 3, vjust = 0) +
    theme_minimal() +
    labs(title = "Processing Time by Batch",
         subtitle = sprintf("Total processing time: %.1f seconds", total_time),
         x = "Batch", y = "Processing Time (seconds)") +
    theme(axis.text.x = element_text(angle = 0))
  
  # Create a final retention plot with batch comparisons
  retention_comparison <- ggplot(progress_data, aes(x = Batch)) +
    geom_bar(aes(y = RetentionRate * 100), stat = "identity", fill = "steelblue") +
    geom_text(aes(y = RetentionRate * 100 + 3, label = sprintf("%.1f%%", RetentionRate * 100)), 
              size = 3, vjust = 0) +
    geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
    theme_minimal() +
    labs(title = "Read Retention Rate by Batch",
         subtitle = sprintf("Overall retention: %.1f%%", overall_retention),
         x = "Batch", y = "Retention Rate (%)") +
    scale_y_continuous(limits = c(0, 100))
  
  # Display the summary plots
  grid.arrange(time_performance, retention_comparison, ncol = 1,
               top = grid::textGrob("Batch Processing Summary", 
                                   gp = grid::gpar(fontsize = 14, fontface = "bold")))
  
  # Create a completion/efficiency plot 
  efficiency_data <- data.frame(
    Metric = c("Total Samples", "Total Batches", "Avg Batch Size", 
               "Avg Time/Batch (s)", "Avg Time/Sample (s)", "Total Time (s)"),
    Value = c(length(sample.names), length(unique(batches)), 
              mean(progress_data$CompletedSamples),
              mean(progress_data$BatchCompletionTime), 
              total_time/length(sample.names), 
              total_time)
  )
  
  # Display numeric summary
  cat("\n----- FILTERING SUMMARY -----\n")
  cat(sprintf("Total processing time: %.1f seconds (%.1f minutes)\n", 
              total_time, total_time/60))
  cat(sprintf("Total samples processed: %d in %d batches\n", 
              length(sample.names), length(unique(batches))))
  cat(sprintf("Average batch size: %.1f samples\n", 
              mean(progress_data$CompletedSamples)))
  cat(sprintf("Average time per batch: %.1f seconds\n", 
              mean(progress_data$BatchCompletionTime)))
  cat(sprintf("Average time per sample: %.1f seconds\n", 
              total_time/length(sample.names)))
  cat(sprintf("Overall retention rate: %.1f%%\n", overall_retention))
  cat(sprintf("Total reads processed: %s\n", 
              format(sum(progress_data$ReadsIn), big.mark = ",")))
  cat(sprintf("Reads retained: %s (%.1f%%)\n", 
              format(sum(progress_data$ReadsOut), big.mark = ","), 
              overall_retention))
  cat("-----------------------------\n")
} else {
  # For smaller datasets, use standard approach
  cat("Processing all samples at once...\n")
  
  # HARDCODE default values first, then try to override them
  hard_truncLen_forward <- 240
  hard_truncLen_reverse <- 160
  hard_maxEE_forward <- 2.0
  hard_maxEE_reverse <- 2.0
  
  # Try to use optimized values if they exist and are valid - with ultra-safe checks
  cat("DEBUG: Checking for truncLen_forward...\n")
  tryCatch({
    if (exists("truncLen_forward")) {
      # Check value without direct reference that might error
      is_valid <- FALSE
      
      # First check if it's NULL
      is_null <- tryCatch({
        is.null(truncLen_forward)
      }, error = function(e) {
        cat("  - Error checking if truncLen_forward is NULL:", conditionMessage(e), "\n")
        TRUE  # Assume NULL if we can't check
      })
      
      if (!is_null) {
        # Try to safely print the value
        tryCatch({
          if (is.na(truncLen_forward)) {
            cat("  - truncLen_forward is NA\n")
          } else {
            cat("  - truncLen_forward exists, value:", as.character(truncLen_forward), "\n")
            is_valid <- TRUE
          }
        }, error = function(e) {
          cat("  - Error printing truncLen_forward:", conditionMessage(e), "\n")
        })
        
        # Only try to convert if previous checks passed
        if (is_valid) {
          tryCatch({
            num_val <- as.integer(truncLen_forward)
            if (!is.na(num_val) && num_val > 0) {
              hard_truncLen_forward <- num_val
              cat("  - Using truncLen_forward value:", hard_truncLen_forward, "\n")
            } else {
              cat("  - truncLen_forward converted to invalid value:", num_val, "\n")
            }
          }, error = function(e) {
            cat("  - Error converting truncLen_forward:", conditionMessage(e), "\n")
          })
        }
      } else {
        cat("  - truncLen_forward is NULL\n")
      }
    } else {
      cat("  - truncLen_forward does not exist\n")
    }
  }, error = function(e) {
    cat("  - Unexpected error processing truncLen_forward:", conditionMessage(e), "\n")
  })
  
  cat("DEBUG: Checking for truncLen_reverse...\n")
  tryCatch({
    if (exists("truncLen_reverse")) {
      # Check value without direct reference that might error
      is_valid <- FALSE
      
      # First check if it's NULL
      is_null <- tryCatch({
        is.null(truncLen_reverse)
      }, error = function(e) {
        cat("  - Error checking if truncLen_reverse is NULL:", conditionMessage(e), "\n")
        TRUE  # Assume NULL if we can't check
      })
      
      if (!is_null) {
        # Try to safely print the value
        tryCatch({
          if (is.na(truncLen_reverse)) {
            cat("  - truncLen_reverse is NA\n")
          } else {
            cat("  - truncLen_reverse exists, value:", as.character(truncLen_reverse), "\n")
            is_valid <- TRUE
          }
        }, error = function(e) {
          cat("  - Error printing truncLen_reverse:", conditionMessage(e), "\n")
        })
        
        # Only try to convert if previous checks passed
        if (is_valid) {
          tryCatch({
            num_val <- as.integer(truncLen_reverse)
            if (!is.na(num_val) && num_val > 0) {
              hard_truncLen_reverse <- num_val
              cat("  - Using truncLen_reverse value:", hard_truncLen_reverse, "\n")
            } else {
              cat("  - truncLen_reverse converted to invalid value:", num_val, "\n")
            }
          }, error = function(e) {
            cat("  - Error converting truncLen_reverse:", conditionMessage(e), "\n")
          })
        }
      } else {
        cat("  - truncLen_reverse is NULL\n")
      }
    } else {
      cat("  - truncLen_reverse does not exist\n")
    }
  }, error = function(e) {
    cat("  - Unexpected error processing truncLen_reverse:", conditionMessage(e), "\n")
  })
  
  # Final parameter values - use direct numeric values in the vectors
  cat("\nFINAL PARAMETERS FOR FILTERING:\n")
  cat("truncLen_forward =", hard_truncLen_forward, "\n")
  cat("truncLen_reverse =", hard_truncLen_reverse, "\n")
  cat("maxEE_forward =", hard_maxEE_forward, "\n")
  cat("maxEE_reverse =", hard_maxEE_reverse, "\n")
  
  # Create the parameter vectors directly with numeric literals
  # This avoids any possible reference to other variables
  truncLen_vector <- c(hard_truncLen_forward, hard_truncLen_reverse)
  maxEE_vector <- c(hard_maxEE_forward, hard_maxEE_reverse)
  
  cat("\nVERIFICATION OF PARAMETER VECTORS:\n")
  cat("truncLen vector:", paste(truncLen_vector, collapse=", "), "\n")
  cat("Class of truncLen vector:", class(truncLen_vector), "\n")
  cat("Length of truncLen vector:", length(truncLen_vector), "\n")
  
  # Extra check to ensure vector has correct length
  if (length(truncLen_vector) != 2) {
    cat("WARNING: truncLen_vector does not have length 2, recreating with default values\n")
    truncLen_vector <- c(240, 160)  # Hardcoded defaults
  }
  
  cat("maxEE vector:", paste(maxEE_vector, collapse=", "), "\n")
  
  # Extra check for maxEE vector
  if (length(maxEE_vector) != 2) {
    cat("WARNING: maxEE_vector does not have length 2, recreating with default values\n")
    maxEE_vector <- c(2.0, 2.0)  # Hardcoded defaults
  }
  
  start_time <- Sys.time()
  
  # Use validated global parameter vectors
  cat("\nUSING VALIDATED GLOBAL PARAMETERS FOR FILTERING:\n")
  cat("truncLen =", paste(GLOBAL_TRUNCLEN_VECTOR, collapse=", "), "\n")
  cat("maxEE =", paste(GLOBAL_MAXEE_VECTOR, collapse=", "), "\n")
  
  # Use our guaranteed global parameter vectors
  out <- filterAndTrim(
    fnFs, filtFs, fnRs, filtRs,
    truncLen = GLOBAL_TRUNCLEN_VECTOR,  # Using validated global vector
    maxN = 0, 
    maxEE = GLOBAL_MAXEE_VECTOR,        # Using validated global vector
    truncQ = 2,
    rm.phix = TRUE, 
    compress = TRUE, 
    multithread = TRUE
  )

  # Calculate statistics
  total_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
  total_reads_in <- sum(out[, 1])
  total_reads_out <- sum(out[, 2])
  retention_rate <- total_reads_out / total_reads_in * 100

  # Create a simple progress visualization for small datasets
  progress_data <- data.frame(
    Stage = c("Before Filtering", "After Filtering"),
    Reads = c(total_reads_in, total_reads_out)
  )

  # Plot the progress
  small_dataset_plot <- ggplot(progress_data, aes(x = Stage, y = Reads, fill = Stage)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = format(Reads, big.mark = ",", scientific = FALSE)), 
              vjust = -0.5, size = 4) +
    theme_minimal() +
    labs(title = "Filtering Results",
         subtitle = sprintf("Processed %d samples in %.1f seconds (%.1f%% retention)",
                            length(fnFs), total_time, retention_rate),
         x = "", y = "Number of Reads") +
    scale_fill_manual(values = c("Before Filtering" = "darkred", "After Filtering" = "steelblue")) +
    theme(legend.position = "none")

  # Display the plot
  print(small_dataset_plot)

  # Add a simple sample-level retention visualization
  sample_retention <- data.frame(
    Sample = rownames(out),
    ReadsIn = out[, 1],
    ReadsOut = out[, 2],
    RetentionRate = out[, 2] / out[, 1] * 100
  )

  # Sort by retention rate
  sample_retention <- sample_retention[order(-sample_retention$RetentionRate), ]

  # Create a plot for per-sample retention rates
  sample_plot <- ggplot(sample_retention, aes(x = reorder(Sample, -RetentionRate), y = RetentionRate)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Per-Sample Retention Rates",
         x = "Sample", y = "Retention Rate (%)") +
    coord_cartesian(ylim = c(0, 100))

  # Display the plot
  print(sample_plot)

  cat(sprintf("Processing completed: %d samples processed in %.1f seconds\n", 
              length(fnFs), total_time))
  cat(sprintf("Overall retention rate: %.1f%% (%s out of %s reads)\n", 
              retention_rate, 
              format(total_reads_out, big.mark = ",", scientific = FALSE), 
              format(total_reads_in, big.mark = ",", scientific = FALSE)))
  cat("--------------------------------------------------------------\n")
}

# Create summary of filtering results
filter_summary <- as.data.frame(out)
filter_summary$SampleName <- rownames(filter_summary)
filter_summary$PercentRetained <- round(filter_summary$reads.out / filter_summary$reads.in * 100, 1)
filter_summary <- filter_summary[, c("SampleName", "reads.in", "reads.out", "PercentRetained")]
colnames(filter_summary) <- c("Sample", "Input Reads", "Filtered Reads", "% Retained")

# Create checkpoint after filter and trim step with additional tracking info
save_checkpoint("after_filter_trim", 
               c("fnFs", "fnRs", "filtFs", "filtRs", "out", "sample.names", 
                 "truncLen_forward", "truncLen_reverse", "maxEE_forward", "maxEE_reverse",
                 "expected_amplicon_size", "filter_summary", 
                 # Only include these if they exist
                 if(exists("is_multi_run")) "is_multi_run" else NULL),
               overwrite = TRUE)
cat("Checkpoint saved after filter and trim step\n")

# Also save a separate checkpoint with just the filter summary for quick access
if (!dir.exists("results")) dir.create("results")
saveRDS(filter_summary, "results/filter_summary.rds")
cat("Filter summary saved to results/filter_summary.rds\n")

# View filtering statistics
kable(filter_summary, caption = "Filtering Statistics", row.names = FALSE)

# Plot filtering results
filter_plot <- ggplot(filter_summary, aes(x = reorder(Sample, -`Filtered Reads`), y = `Filtered Reads`)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = paste0(`% Retained`, "%")), vjust = -0.3, size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Reads Retained After Filtering", x = "Sample", y = "Filtered Reads")

print(filter_plot)

# Count number of samples that retained >50% of reads
samples_gt_50pct <- sum(filter_summary$`% Retained` > 50)
cat(samples_gt_50pct, "out of", nrow(filter_summary), "samples retained >50% of reads after filtering\n")

# Check if any samples have very low retention
low_retention_samples <- filter_summary[filter_summary$`% Retained` < 30, ]
if (nrow(low_retention_samples) > 0) {
  cat("WARNING:", nrow(low_retention_samples), "samples have very low read retention (<30%):\n")
  print(low_retention_samples)
}

# Check if any samples have zero reads after filtering
zero_samples <- filter_summary[filter_summary$`Filtered Reads` == 0, ]
if (nrow(zero_samples) > 0) {
  cat("WARNING:", nrow(zero_samples), "samples have zero reads after filtering and will be removed:\n")
  print(zero_samples)
  
  # Remove samples with zero reads after filtering
  samples_to_keep <- filter_summary$Sample[filter_summary$`Filtered Reads` > 0]
  filtFs <- filtFs[samples_to_keep]
  filtRs <- filtRs[samples_to_keep]
  sample.names <- sample.names[sample.names %in% samples_to_keep]
  cat("Continuing with", length(sample.names), "samples\n")
}
```

# Quality Profiles After Filtering

Let's check the quality profiles of the filtered reads to confirm our filtering parameters worked as expected.

```{r quality-filtered}
# Enhanced quality visualization for filtered reads
if (length(filtFs) > 0) {
  # Function to create enhanced quality profile visualization
  create_enhanced_quality_viz <- function(fastq_files, read_type = "Forward", aggregate = FALSE) {
    # Generate standard quality profile
    qual_plot <- plotQualityProfile(fastq_files, aggregate = aggregate)
    
    if (aggregate) {
      # Extract data for enhanced visualization
      plot_data <- qual_plot$data
      
      # Calculate mean, median, and percentile quality by position
      qual_stats <- plot_data %>%
        group_by(Cycle) %>%
        summarize(
          Mean = mean(Score),
          Median = median(Score),
          Q10 = quantile(Score, 0.1),
          Q25 = quantile(Score, 0.25),
          Q75 = quantile(Score, 0.75),
          Q90 = quantile(Score, 0.9)
        )
      
      # Create enhanced visualization
      enhanced_plot <- ggplot(qual_stats, aes(x = Cycle)) +
        # Add shaded regions for quality interpretation
        annotate("rect", xmin = -Inf, xmax = Inf, ymin = 0, ymax = 20, 
                 fill = "red", alpha = 0.2) +
        annotate("rect", xmin = -Inf, xmax = Inf, ymin = 20, ymax = 28, 
                 fill = "yellow", alpha = 0.2) +
        annotate("rect", xmin = -Inf, xmax = Inf, ymin = 28, ymax = Inf, 
                 fill = "green", alpha = 0.2) +
        # Add percentile ribbons
        geom_ribbon(aes(ymin = Q10, ymax = Q90), alpha = 0.2, fill = "grey50") +
        geom_ribbon(aes(ymin = Q25, ymax = Q75), alpha = 0.3, fill = "grey40") +
        # Add lines for mean and median
        geom_line(aes(y = Mean, color = "Mean"), size = 1.2) +
        geom_line(aes(y = Median, color = "Median"), size = 1.2, linetype = "dashed") +
        # Add quality thresholds
        geom_hline(yintercept = 30, linetype = "dashed", color = "blue") +
        geom_hline(yintercept = 20, linetype = "dashed", color = "red") +
        # Add trimming position indicators if available
        {if (read_type == "Forward" && exists("truncLen_forward")) 
          geom_vline(xintercept = truncLen_forward, color = "purple", size = 1, alpha = 0.7)} +
        {if (read_type == "Reverse" && exists("truncLen_reverse")) 
          geom_vline(xintercept = truncLen_reverse, color = "purple", size = 1, alpha = 0.7)} +
        # Labels and theme
        labs(
          title = paste(read_type, "Read Quality After Filtering"),
          subtitle = "With quality interpretation zones and percentile distribution",
          x = "Cycle (bp position)",
          y = "Quality Score",
          caption = "Red: Q<20 (poor) | Yellow: Q20-28 (acceptable) | Green: Q>28 (excellent)"
        ) +
        scale_color_manual(
          name = "Statistic", 
          values = c("Mean" = "blue", "Median" = "darkgreen")
        ) +
        scale_y_continuous(limits = c(0, 40)) +
        theme_minimal() +
        theme(
          legend.position = "bottom",
          plot.caption = element_text(hjust = 0.5),
          panel.grid.minor = element_blank()
        ) +
        # Add annotations
        annotate("text", x = 5, y = 38, label = "Excellent", color = "darkgreen", hjust = 0) +
        annotate("text", x = 5, y = 24, label = "Acceptable", color = "darkgoldenrod4", hjust = 0) +
        annotate("text", x = 5, y = 10, label = "Poor", color = "darkred", hjust = 0)
      
      # Add trimming point annotation
      if (read_type == "Forward" && exists("truncLen_forward")) {
        enhanced_plot <- enhanced_plot + 
          annotate("text", x = truncLen_forward, y = 35, 
                   label = paste("Trim position:", truncLen_forward), 
                   color = "purple", hjust = 1)
      } else if (read_type == "Reverse" && exists("truncLen_reverse")) {
        enhanced_plot <- enhanced_plot + 
          annotate("text", x = truncLen_reverse, y = 35, 
                   label = paste("Trim position:", truncLen_reverse), 
                   color = "purple", hjust = 1)
      }
      
      return(enhanced_plot)
    } else {
      # Return standard plot for non-aggregated visualization
      return(qual_plot + 
               ggtitle(paste(read_type, "Read Quality After Filtering (Individual Samples)")))
    }
  }
  
  # Create enhanced visualizations for filtered reads
  n_sample_viz <- min(3, length(filtFs))
  
  # Individual sample plots
  filtered_forward_qual <- create_enhanced_quality_viz(filtFs[1:n_sample_viz], "Forward")
  filtered_reverse_qual <- create_enhanced_quality_viz(filtRs[1:n_sample_viz], "Reverse")
  print(filtered_forward_qual)
  print(filtered_reverse_qual)
  
  # Aggregated profiles with enhanced visualization
  cat("Creating aggregated quality profiles with enhanced visualization...\n")
  
  # Use more samples for aggregated view if available (up to 10)
  n_sample_agg <- min(10, length(filtFs))
  
  # Aggregated plots with enhanced visualization
  forward_enhanced <- create_enhanced_quality_viz(filtFs[1:n_sample_agg], "Forward", aggregate = TRUE)
  reverse_enhanced <- create_enhanced_quality_viz(filtRs[1:n_sample_agg], "Reverse", aggregate = TRUE)
  
  # Display enhanced plots
  print(forward_enhanced)
  print(reverse_enhanced)
  
  # Side-by-side view
  grid.arrange(forward_enhanced, reverse_enhanced, ncol = 2,
               top = "Quality Profiles After Filtering (Aggregated)")
  
  # Create a quality comparison visualization (before vs after filtering)
  if (exists("forward_qual") && exists("reverse_qual")) {
    cat("Creating before vs after filtering quality comparison...\n")
    
    # Extract data from before filtering plots
    before_forward_data <- forward_qual$data
    before_reverse_data <- reverse_qual$data
    
    # Sample a subset of filtered reads quality data for comparison
    filtered_forward_qual <- plotQualityProfile(filtFs[1:n_sample_agg], aggregate = TRUE)
    filtered_reverse_qual <- plotQualityProfile(filtRs[1:n_sample_agg], aggregate = TRUE)
    filtered_forward_data <- filtered_forward_qual$data
    filtered_reverse_data <- filtered_reverse_qual$data
    
    # Make sure all necessary columns exist in both datasets 
    # (fixing "numbers of columns of arguments do not match" error)
    necessary_cols <- c("Cycle", "Score", "Sample")
    for (col in necessary_cols) {
      if (!col %in% colnames(before_forward_data)) before_forward_data[[col]] <- NA
      if (!col %in% colnames(filtered_forward_data)) filtered_forward_data[[col]] <- NA
      if (!col %in% colnames(before_reverse_data)) before_reverse_data[[col]] <- NA
      if (!col %in% colnames(filtered_reverse_data)) filtered_reverse_data[[col]] <- NA
    }
    
    # Subset columns to ensure they match
    before_forward_data <- before_forward_data[, c("Cycle", "Score", "Sample")]
    filtered_forward_data <- filtered_forward_data[, c("Cycle", "Score", "Sample")]
    before_reverse_data <- before_reverse_data[, c("Cycle", "Score", "Sample")]
    filtered_reverse_data <- filtered_reverse_data[, c("Cycle", "Score", "Sample")]
    
    # Add a 'Stage' column to both datasets
    before_forward_data$Stage <- "Before Filtering"
    filtered_forward_data$Stage <- "After Filtering"
    before_reverse_data$Stage <- "Before Filtering"
    filtered_reverse_data$Stage <- "After Filtering"
    
    # Safely combine data
    tryCatch({
      combined_forward <- rbind(before_forward_data, filtered_forward_data)
      combined_reverse <- rbind(before_reverse_data, filtered_reverse_data)
      
      # Create comparison plots
      forward_comparison <- ggplot(combined_forward, aes(x = Cycle, y = Score, color = Stage)) +
        stat_summary(fun = mean, geom = "line", size = 1.2) +
        geom_hline(yintercept = 30, linetype = "dashed", color = "blue") +
        geom_hline(yintercept = 20, linetype = "dashed", color = "red") +
        {if (exists("truncLen_forward")) 
          geom_vline(xintercept = truncLen_forward, color = "purple", size = 1, alpha = 0.7)} +
        scale_color_manual(values = c("Before Filtering" = "gray40", "After Filtering" = "steelblue")) +
        labs(
          title = "Forward Reads: Quality Before vs After Filtering",
          x = "Cycle (bp position)",
          y = "Mean Quality Score"
        ) +
        theme_minimal()
      
      reverse_comparison <- ggplot(combined_reverse, aes(x = Cycle, y = Score, color = Stage)) +
        stat_summary(fun = mean, geom = "line", size = 1.2) +
        geom_hline(yintercept = 30, linetype = "dashed", color = "blue") +
        geom_hline(yintercept = 20, linetype = "dashed", color = "red") +
        {if (exists("truncLen_reverse")) 
          geom_vline(xintercept = truncLen_reverse, color = "purple", size = 1, alpha = 0.7)} +
        scale_color_manual(values = c("Before Filtering" = "gray40", "After Filtering" = "steelblue")) +
        labs(
          title = "Reverse Reads: Quality Before vs After Filtering",
          x = "Cycle (bp position)",
          y = "Mean Quality Score"
        ) +
        theme_minimal()
      
      # Display comparison plots
      grid.arrange(forward_comparison, reverse_comparison, ncol = 2,
                   top = "Quality Improvement After Filtering")
    }, error = function(e) {
      cat("Could not create before vs after comparison plots due to data structure differences.\n")
      cat("This is likely due to differences in the quality profile data structures. Using individual plots instead.\n")
      cat("Error details:", conditionMessage(e), "\n")
      
      # Display the plots separately instead
      print(forward_enhanced)
      print(reverse_enhanced)
    })
  }
  
} else {
  cat("No samples remain after filtering. Please review filtering parameters.\n")
  knitr::knit_exit()
}
```

# Learn Error Rates

In this step, DADA2 learns the error rates from the filtered sequence data. This is a critical step that models the relationship between sequence quality scores and the observed error rates, allowing the algorithm to better distinguish between sequencing errors and true biological variation.

```{r learn-errors}
# Learn error rates with optimized parallelization
cat("Learning error rates with optimized parallelization...\n")

# Optimize error learning with hardware-specific optimizations
optimized_learn_errors <- function(fastq_files, nbases = 1e8) {
  # CPU implementation with hardware-specific optimizations
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # Optimize for Apple Silicon
    old_veclib_setting <- Sys.getenv("VECLIB_MAXIMUM_THREADS")
    Sys.setenv(VECLIB_MAXIMUM_THREADS = as.character(hw_caps$optimal_workers))
    
    # Apple Silicon can handle more bases for error learning
    nbases <- max(nbases, 1e9)
    
    # Run the error learning
    result <- learnErrors(fastq_files, multithread = TRUE, randomize = TRUE, 
                         verbose = TRUE, nbases = nbases)
    
    # Restore original setting
    Sys.setenv(VECLIB_MAXIMUM_THREADS = old_veclib_setting)
    return(result)
  } else {
    # Standard CPU implementation
    return(learnErrors(fastq_files, multithread = TRUE, randomize = TRUE, 
                      verbose = TRUE, nbases = nbases))
  }
}

# Determine optimal sample size for error learning based on hardware
determine_error_learning_samples <- function(total_samples) {
  # Base value for standard systems
  max_samples <- 40
  
  # Adjust based on hardware
  if (exists("hw_caps")) {
    if (hw_caps$apple_silicon) {
      # Apple Silicon can handle more samples efficiently
      max_samples <- 60
    } else if (hw_caps$cores >= 16) {
      # High-core systems
      max_samples <- 50
    }
  }
  
  # Never use more samples than we have
  return(min(max_samples, total_samples))
}

# Determine samples to use for error learning
total_samples <- length(filtFs)
MAX_ERROR_LEARN_SAMPLES <- determine_error_learning_samples(total_samples)

cat(sprintf("Optimized error learning will use up to %d samples\n", MAX_ERROR_LEARN_SAMPLES))

# Sample selection for error learning
if(total_samples > MAX_ERROR_LEARN_SAMPLES) {
  cat(sprintf("Using random subset of %d samples for error learning...\n", MAX_ERROR_LEARN_SAMPLES))
  
  # Select a random subset for error learning
  set.seed(100)  # For reproducibility
  error_learn_samples <- sample(seq_along(filtFs), MAX_ERROR_LEARN_SAMPLES)
  
  # Process forward reads
  cat("Learning error rates for forward reads...\n")
  errF <- optimized_learn_errors(filtFs[error_learn_samples], 
                               nbases = if (exists("hw_caps") && hw_caps$apple_silicon) 1e9 else 1e8)
  
  # Apply optimized garbage collection
  adaptive_gc(FALSE)
  
  # Process reverse reads
  cat("Learning error rates for reverse reads...\n")
  errR <- optimized_learn_errors(filtRs[error_learn_samples], 
                               nbases = if (exists("hw_caps") && hw_caps$apple_silicon) 1e9 else 1e8)
} else {
  # For smaller datasets, use all samples
  cat("Learning error rates for forward reads using all samples...\n")
  errF <- optimized_learn_errors(filtFs, 
                               nbases = if (exists("hw_caps") && hw_caps$apple_silicon) 1e9 else 1e8)
  
  # Apply optimized garbage collection
  adaptive_gc(FALSE)
  
  cat("Learning error rates for reverse reads using all samples...\n")
  errR <- optimized_learn_errors(filtRs, 
                               nbases = if (exists("hw_caps") && hw_caps$apple_silicon) 1e9 else 1e8)
}

# Final garbage collection
adaptive_gc(TRUE)

# Plot error rates for forward reads
errF_plot <- plotErrors(errF, nominalQ = TRUE)
print(errF_plot + ggtitle("Error Rates - Forward Reads"))

# Plot error rates for reverse reads
errR_plot <- plotErrors(errR, nominalQ = TRUE)
print(errR_plot + ggtitle("Error Rates - Reverse Reads"))

# Create a side-by-side plot
grid.arrange(
  errF_plot + ggtitle("Error Rates - Forward Reads"), 
  errR_plot + ggtitle("Error Rates - Reverse Reads"), 
  ncol = 2
)

# Save error models for reproducibility
if (!dir.exists("results")) dir.create("results")
error_models <- list(
  forward = errF,
  reverse = errR,
  timestamp = Sys.time(),
  parameters = list(
    truncation = c(forward = truncLen_forward, reverse = truncLen_reverse),
    maxEE = c(forward = maxEE_forward, reverse = maxEE_reverse)
  )
)
saveRDS(error_models, "results/error_models.rds")
cat("Error models saved to results/error_models.rds\n")
```

# Dereplication

```{r dereplication}
# Dereplicate identical reads
cat("Dereplicating forward reads...\n")
derepFs <- derepFastq(filtFs, verbose = TRUE)

cat("Dereplicating reverse reads...\n")
derepRs <- derepFastq(filtRs, verbose = TRUE)

# Name the derep-class objects by the sample names
names(derepFs) <- names(filtFs)
names(derepRs) <- names(filtRs)

# Create summary of dereplication
derep_summary <- data.frame(
  Sample = names(derepFs),
  InputReads = sapply(derepFs, function(x) sum(getUniques(x))),
  UniqueSequences = sapply(derepFs, function(x) length(getUniques(x))),
  CompressionRatio = sapply(derepFs, function(x) sum(getUniques(x)) / length(getUniques(x)))
)

kable(head(derep_summary, 10), caption = "Dereplication Summary (First 10 Samples)")

# Average compression ratio
avg_compression <- mean(derep_summary$CompressionRatio)
cat("Average compression ratio:", round(avg_compression, 1), 
    "reads per unique sequence\n")
    
# Create checkpoint after dereplication step
save_checkpoint("after_dereplication", 
               c("filtFs", "filtRs", "errF", "errR", "derepFs", "derepRs", "sample.names", 
                 "derep_summary", "avg_compression"),
               overwrite = TRUE)
cat("Checkpoint saved after dereplication step\n")

# Save dereplication summary for easier access
if (!dir.exists("results")) dir.create("results")
saveRDS(derep_summary, "results/derep_summary.rds")
write.csv(derep_summary, "results/derep_summary.csv", row.names = FALSE)
cat("Dereplication summary saved to results/derep_summary.csv\n")
```

# Sample Inference

DADA2 will now denoise the data and infer the true biological sequences.

```{r dada2}
# Apply the DADA2 algorithm with optimized processing
cat("Denoising reads with optimized parallelization...\n")

# Function to optimize DADA2 sample inference with hardware-specific settings
optimized_dada <- function(derep, err, pool = FALSE) {
  # CPU implementation with hardware-specific optimizations
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # Optimize for Apple Silicon
    old_veclib_setting <- Sys.getenv("VECLIB_MAXIMUM_THREADS")
    Sys.setenv(VECLIB_MAXIMUM_THREADS = as.character(hw_caps$optimal_workers))
    
    # Run the sample inference
    result <- dada(derep, err = err, multithread = TRUE, pool = pool)
    
    # Restore original setting
    Sys.setenv(VECLIB_MAXIMUM_THREADS = old_veclib_setting)
    return(result)
  } else {
    # Standard CPU implementation
    return(dada(derep, err = err, multithread = TRUE, pool = pool))
  }
}

# Use pseudo-pooling to improve detection of rare variants
pooling_option <- "pseudo"
cat("Using pseudo-pooling across all samples\n")

# Process forward reads
cat("Denoising forward reads with optimized parallelization...\n")

# For smaller datasets, process all at once with hardware optimization
if (length(derepFs) <= 20) {
  dadaFs <- optimized_dada(derepFs, err = errF, pool = pooling_option)
} else {
  # Use batch processing for larger datasets
  batch_size <- 20  # Default batch size
  
  # Adjust batch size for Apple Silicon
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    batch_size <- 30
  }
  
  sample_names <- names(derepFs)
  batch_groups <- split(sample_names, ceiling(seq_along(sample_names)/batch_size))
  
  # Pre-allocate results list to avoid memory fragmentation
  dadaFs <- vector("list", length(sample_names))
  names(dadaFs) <- sample_names
  
  # Process each batch and combine results
  for (i in seq_along(batch_groups)) {
    cat(sprintf("Processing forward batch %d of %d...\n", i, length(batch_groups)))
    batch_samples <- batch_groups[[i]]
    
    # Process with hardware optimization
    batch_results <- optimized_dada(derepFs[batch_samples], err = errF, pool = pooling_option)
    
    # Assign directly to pre-allocated list
    for (sample_name in names(batch_results)) {
      dadaFs[[sample_name]] <- batch_results[[sample_name]]
    }
    
    # Optimized garbage collection
    adaptive_gc(i == length(batch_groups) || i %% 3 == 0)
  }
}

# Process reverse reads 
cat("Denoising reverse reads with optimized parallelization...\n")

# For smaller datasets, process all at once with hardware optimization
if (length(derepRs) <= 20) {
  dadaRs <- optimized_dada(derepRs, err = errR, pool = pooling_option)
} else {
  # Use batch processing for larger datasets
  batch_size <- 20  # Default batch size
  
  # Adjust batch size for Apple Silicon
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    batch_size <- 30
  }
  
  sample_names <- names(derepRs)
  batch_groups <- split(sample_names, ceiling(seq_along(sample_names)/batch_size))
  
  # Pre-allocate results list to avoid memory fragmentation
  dadaRs <- vector("list", length(sample_names))
  names(dadaRs) <- sample_names
  
  # Process each batch and combine results
  for (i in seq_along(batch_groups)) {
    cat(sprintf("Processing reverse batch %d of %d...\n", i, length(batch_groups)))
    batch_samples <- batch_groups[[i]]
    
    # Process with hardware optimization
    batch_results <- optimized_dada(derepRs[batch_samples], err = errR, pool = pooling_option)
    
    # Assign directly to pre-allocated list
    for (sample_name in names(batch_results)) {
      dadaRs[[sample_name]] <- batch_results[[sample_name]]
    }
    
    # Optimized garbage collection
    adaptive_gc(i == length(batch_groups) || i %% 3 == 0)
  }
}

# Final garbage collection
adaptive_gc(TRUE)

# Inspect the returned dada-class object for the first sample
cat("\nDADA2 denoising results for first sample (forward reads):\n")
dadaFs[[1]]

# Create summary of denoising
denoise_summary <- data.frame(
  Sample = names(dadaFs),
  InputReads = sapply(derepFs, function(x) sum(getUniques(x))),
  DenoisedReads = sapply(dadaFs, function(x) sum(getUniques(x$denoised))),
  ASVsFound = sapply(dadaFs, function(x) length(getUniques(x$denoised))),
  PercentRetained = sapply(dadaFs, function(x) round(sum(getUniques(x$denoised)) / sum(getUniques(x)) * 100, 1))
)

kable(head(denoise_summary, 10), caption = "Denoising Summary (First 10 Samples)")

# Average denoising stats
avg_denoised_pct <- mean(denoise_summary$PercentRetained)
total_asvs_forward <- sum(sapply(dadaFs, function(x) length(x$denoised)))
cat("Average percentage of reads retained after denoising:", round(avg_denoised_pct, 1), "%\n")
cat("Total ASVs identified in forward reads:", total_asvs_forward, "\n")

# Save denoising summary 
if (!dir.exists("results")) dir.create("results")
saveRDS(denoise_summary, "results/denoise_summary.rds")
write.csv(denoise_summary, "results/denoise_summary.csv", row.names = FALSE)
cat("Denoising summary saved to results/denoise_summary.csv\n")
```

# Merge Paired Reads

Now we'll merge the forward and reverse reads to create full-length sequences.

```{r merge-reads}
# Merge paired reads with optimized parallelization and adaptive memory management
cat("Merging paired reads with optimized parallelization and adaptive memory management...\n")

# Function to estimate memory needed for merging a batch of samples
estimate_merge_memory <- function(sample_batch_size, avg_reads_per_sample) {
  # Approximate memory model: 
  # Each sample needs memory for storing forward and reverse reads plus merged results
  # This is a simplified model - actual memory usage depends on complexity of data
  memory_per_sample_mb <- 15 + (avg_reads_per_sample / 5000)  # Base + variable component
  return(sample_batch_size * memory_per_sample_mb)
}

# Function to get available system memory in MB
get_available_memory <- function() {
  tryCatch({
    if (.Platform$OS.type == "unix") {
      if (Sys.info()["sysname"] == "Darwin") {  # macOS
        mem_info <- system("vm_stat | grep 'Pages free:'", intern = TRUE)
        free_pages <- as.numeric(gsub("Pages free:\\s+", "", mem_info))
        page_size <- 4096  # Default page size on macOS
        free_mem_mb <- (free_pages * page_size) / (1024^2)
      } else {  # Linux
        mem_info <- system("free -m | grep 'Mem:'", intern = TRUE)
        free_mem_mb <- as.numeric(strsplit(mem_info, "\\s+")[[1]][4])
      }
      return(free_mem_mb)
    } else {  # Windows or unknown
      return(8000)  # Default to 8GB for unknown systems
    }
  }, error = function(e) {
    # If memory detection fails, use a conservative default
    cat("Could not detect available memory, using default value\n")
    return(4000)  # Default to 4GB if detection fails
  })
}

# Determine if we need to use batched merging based on dataset size
if (length(dadaFs) > 20) {
  cat("Using adaptive batched merging for large dataset...\n")
  
  # Calculate average reads per sample for memory estimation
  avg_reads_per_sample <- mean(sapply(derepFs, function(x) sum(getUniques(x))))
  cat(sprintf("Average reads per sample: %.0f\n", avg_reads_per_sample))
  
  # Get available memory
  available_memory_mb <- get_available_memory()
  
  # Reserve memory for the system (30% of available)
  reserved_memory_mb <- available_memory_mb * 0.3
  usable_memory_mb <- available_memory_mb - reserved_memory_mb
  
  # Calculate optimal batch size based on available memory
  memory_per_sample_mb <- estimate_merge_memory(1, avg_reads_per_sample)
  optimal_batch_size <- floor(usable_memory_mb / memory_per_sample_mb)
  
  # Apply reasonable limits
  optimal_batch_size <- min(max(optimal_batch_size, 5), 30)
  
  # Apple Silicon specific optimizations
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # M-series chips have unified memory which is more efficient
    optimal_batch_size <- min(optimal_batch_size * 1.5, 40)
    cat(sprintf("Apple Silicon optimization: Increased batch size to %.0f\n", optimal_batch_size))
  }
  
  cat(sprintf("Adaptive memory management: Using batch size of %.0f samples based on available memory\n", 
              optimal_batch_size))
  
  # Set up batch processing
  batch_size <- optimal_batch_size
  sample_names <- names(dadaFs)
  batch_groups <- split(sample_names, ceiling(seq_along(sample_names)/batch_size))
  
  # Initialize results list with pre-allocation to avoid memory fragmentation
  mergers <- vector("list", length(sample_names))
  names(mergers) <- sample_names
  
  # Process each batch
  for (i in seq_along(batch_groups)) {
    cat(sprintf("Merging batch %d of %d...\n", i, length(batch_groups)))
    batch_samples <- batch_groups[[i]]
    
    # Calculate appropriate overlap parameter
    # Directly calculate expected overlap if we have the amplicon size
    if(exists("expected_amplicon_size") && !is.null(expected_amplicon_size)) {
      curr_expected_overlap <- truncLen_forward + truncLen_reverse - expected_amplicon_size
      min_overlap <- if(curr_expected_overlap < 20) 10 else 12
    } else {
      # Default to 12 if we can't calculate
      min_overlap <- 12
    }
    
    # Process this batch
    batch_mergers <- mergePairs(
      dadaFs[batch_samples], 
      derepFs[batch_samples], 
      dadaRs[batch_samples], 
      derepRs[batch_samples],
      verbose = TRUE,
      minOverlap = min_overlap
    )
    
    # Add to results - assign directly to pre-allocated list to avoid memory fragmentation
    for (sample_name in names(batch_mergers)) {
      mergers[[sample_name]] <- batch_mergers[[sample_name]]
    }
    
    # Smart garbage collection with Apple Silicon optimization
    if (exists("hw_caps") && hw_caps$apple_silicon) {
      # Less aggressive GC on Apple Silicon to avoid expensive memory operations
      if (i %% 2 == 0 || i == length(batch_groups)) {
        # Use compact=TRUE to reduce memory fragmentation
        invisible(gc(verbose = FALSE, full = FALSE))
      }
    } else {
      # More aggressive GC for other platforms
      invisible(gc(verbose = FALSE, full = TRUE))
    }
    
    # Monitor memory usage (if possible) and adjust batch size dynamically
    if (i < length(batch_groups) && exists("get_available_memory")) {
      current_memory <- get_available_memory()
      memory_threshold <- reserved_memory_mb  # Use reserved memory as threshold
      if (current_memory < memory_threshold) {
        # Memory is getting low, reduce batch size for next iteration
        cat("Memory pressure detected, reducing batch size for next iteration\n")
        next_batch_size <- max(floor(batch_size * 0.7), 3)
        
        # Recalculate batches for remaining samples
        remaining_samples <- unlist(batch_groups[(i+1):length(batch_groups)])
        batch_groups <- c(
          batch_groups[1:i],
          split(remaining_samples, ceiling(seq_along(remaining_samples)/next_batch_size))
        )
      }
    }
  }
} else {
  # For smaller datasets, use standard approach
  # Calculate appropriate overlap parameter
  if(exists("expected_amplicon_size") && !is.null(expected_amplicon_size)) {
    curr_expected_overlap <- truncLen_forward + truncLen_reverse - expected_amplicon_size
    min_overlap <- if(curr_expected_overlap < 20) 10 else 12
  } else {
    # Default to 12 if we can't calculate
    min_overlap <- 12
  }
  
  mergers <- mergePairs(
    dadaFs, 
    derepFs, 
    dadaRs, 
    derepRs,
    verbose = TRUE,
    minOverlap = min_overlap
  )
}

# Inspect the merger data.frame for the first sample
cat("\nMerged read pairs for first sample:\n")
head(mergers[[1]])

# Create summary of merging
merge_summary <- data.frame(
  Sample = names(mergers),
  DenoisedReads = sapply(dadaFs, function(x) sum(getUniques(x$denoised))),
  MergedReads = sapply(mergers, function(x) sum(getUniques(x))),
  PercentMerged = sapply(names(mergers), function(sample) 
    round(sum(getUniques(mergers[[sample]])) / sum(getUniques(dadaFs[[sample]]$denoised)) * 100, 1))
)

kable(head(merge_summary, 10), caption = "Read Merging Summary (First 10 Samples)")

# Average merging stats
avg_merged_pct <- mean(merge_summary$PercentMerged)
cat("Average percentage of denoised reads successfully merged:", round(avg_merged_pct, 1), "%\n")

# Check if any samples have very low merging rates
low_merge_samples <- merge_summary[merge_summary$PercentMerged < 50, ]
if (nrow(low_merge_samples) > 0) {
  cat("WARNING:", nrow(low_merge_samples), "samples have low merging rates (<50%):\n")
  print(low_merge_samples)
}

# Create checkpoint after merging reads step with more tracking
save_checkpoint("after_merge_reads", 
               c("derepFs", "derepRs", "dadaFs", "dadaRs", "mergers", "sample.names",
                 "truncLen_forward", "truncLen_reverse", "merge_summary", "avg_merged_pct", 
                 if(exists("low_merge_samples")) "low_merge_samples" else NULL),
               overwrite = TRUE)
cat("Checkpoint saved after merging paired reads step\n")

# Save merging summary for dashboard visualization
if (!dir.exists("results")) dir.create("results")
saveRDS(merge_summary, "results/merge_summary.rds")
write.csv(merge_summary, "results/merge_summary.csv", row.names = FALSE)
cat("Merging summary saved to results/merge_summary.csv\n")

# Save problematic samples information if any exist
if (nrow(low_merge_samples) > 0) {
  saveRDS(low_merge_samples, "results/low_merge_samples.rds")
  write.csv(low_merge_samples, "results/low_merge_samples.csv", row.names = FALSE)
  cat("List of problematic samples with low merging rates saved to results/low_merge_samples.csv\n")
}
```

# Construct ASV Table

```{r seqtab}
# Construct sequence table
cat("Constructing ASV sequence table...\n")

# Special handling for multi-run data
if (is_multi_run && exists("file_to_run")) {
  cat("Multi-run mode: Creating separate sequence tables for each run first\n")
  
  # Get unique runs
  run_names <- unique(file_to_run)
  
  # Create a separate sequence table for each run
  run_seqtabs <- list()
  for (run in run_names) {
    cat(sprintf("Creating sequence table for run '%s'...\n", run))
    # Get mergers for this run
    run_samples <- names(mergers)[file_to_run == run]
    
    # Make sequence table for this run
    run_seqtab <- makeSequenceTable(mergers[run_samples])
    run_seqtabs[[run]] <- run_seqtab
    
    cat(sprintf("  Run '%s' sequence table dimensions: %d samples x %d ASVs\n", 
              run, nrow(run_seqtab), ncol(run_seqtab)))
  }
  
  # Merge all sequence tables
  cat("Merging sequence tables from all runs...\n")
  seqtab <- mergeSequenceTables(tables = run_seqtabs)
  
  # Add run info to sequence table attributes for tracking
  attr(seqtab, "run_info") <- file_to_run
} else {
  # Standard processing for single run
  seqtab <- makeSequenceTable(mergers)
}

# Display table dimensions
cat("Dimensions of ASV table:", dim(seqtab), "(samples × ASVs)\n")

# Count ASVs by length
asv_lengths <- table(nchar(getSequences(seqtab)))
asv_lengths_df <- data.frame(
  Length = names(asv_lengths),
  Count = as.vector(asv_lengths)
)

# Create a histogram of ASV lengths
asv_length_plot <- ggplot(asv_lengths_df, aes(x = Length, y = Count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(title = "Distribution of ASV Lengths",
       x = "Sequence Length (bp)",
       y = "Number of ASVs")

print(asv_length_plot)

# Get the most common ASV length
modal_length <- as.numeric(names(which.max(asv_lengths)))
cat("Most common ASV length:", modal_length, "bp\n")

# Compare to expected amplicon size
cat("Expected amplicon size based on primers:", expected_amplicon_size, "bp\n")
if (abs(modal_length - expected_amplicon_size) <= 10) {
  cat("ASV lengths match expected amplicon size ✓\n")
} else {
  cat("WARNING: Most common ASV length differs from expected amplicon size\n")
}

# Filter out suspiciously short or long ASVs with a wider acceptable range
min_allowed_len <- max(expected_amplicon_size - 100, 0.7 * modal_length)
max_allowed_len <- min(expected_amplicon_size + 100, 1.3 * modal_length)

cat("Filtering ASVs with lengths outside the range:", min_allowed_len, "-", max_allowed_len, "bp\n")
seqtab_filtered <- seqtab[, nchar(colnames(seqtab)) >= min_allowed_len & 
                           nchar(colnames(seqtab)) <= max_allowed_len]

# Report filtering results
cat("Removed", ncol(seqtab) - ncol(seqtab_filtered), "ASVs with suspicious lengths\n")
cat("Retained", ncol(seqtab_filtered), "ASVs for further analysis\n")
original_reads <- sum(seqtab)
filtered_reads <- sum(seqtab_filtered)
cat("Removed", original_reads - filtered_reads, "reads (", 
    round((original_reads - filtered_reads) / original_reads * 100, 1), "% of total)\n")

# Use filtered sequence table for further analysis
seqtab <- seqtab_filtered

# Create summary table of parameter optimization results
param_summary_table <- data.frame(
  Parameter = c(
    "Forward Truncation Length", 
    "Reverse Truncation Length", 
    "Forward maxEE", 
    "Reverse maxEE", 
    "Expected Amplicon Size", 
    "Expected Overlap"
  ),
  Value = c(
    truncLen_forward, 
    truncLen_reverse, 
    round(maxEE_forward, 1), 
    round(maxEE_reverse, 1), 
    expected_amplicon_size, 
    expected_overlap
  ),
  Description = c(
    "Optimized position to truncate forward reads",
    "Optimized position to truncate reverse reads",
    "Maximum expected errors for forward reads",
    "Maximum expected errors for reverse reads",
    "Expected amplicon size based on detected primers",
    "Expected overlap between paired reads after truncation"
  )
)

# Display the parameter summary table
kable(param_summary_table, caption = "Summary of Optimized DADA2 Parameters")

# If multi-run, create a summary of ASVs per run
if (is_multi_run && exists("file_to_run") && exists("run_seqtabs")) {
  # Create a table showing how many ASVs were found in each run
  run_asv_counts <- sapply(run_seqtabs, ncol)
  shared_asv_count <- ncol(seqtab)
  
  # Calculate percentage of ASVs shared across runs
  unique_asvs_all_runs <- length(unique(unlist(lapply(run_seqtabs, colnames))))
  shared_pct <- round(shared_asv_count / unique_asvs_all_runs * 100, 1)
  
  run_summary <- data.frame(
    Run = names(run_asv_counts),
    ASVs = run_asv_counts,
    SamplesInRun = sapply(run_seqtabs, nrow)
  )
  
  # Add overall row
  run_summary <- rbind(run_summary, 
                     c("COMBINED", ncol(seqtab), nrow(seqtab)))
  
  # Display summary table
  kable(run_summary, caption = "ASV Counts by Run")
  
  cat("Found", unique_asvs_all_runs, "unique ASVs across all runs\n")
  cat("After merging:", ncol(seqtab), "ASVs in the combined dataset\n")
}

# Create checkpoint after ASV table construction step with more tracking
save_checkpoint("after_asv_table", 
               c("mergers", "seqtab", "sample.names", "seqtab_filtered", 
                 if(exists("expected_amplicon_size")) "expected_amplicon_size" else NULL,
                 if(exists("is_multi_run")) "is_multi_run" else NULL, 
                 if(exists("file_to_run")) "file_to_run" else NULL, 
                 if(exists("run_seqtabs")) "run_seqtabs" else NULL, 
                 "original_reads", "filtered_reads",
                 "asv_lengths", "modal_length", "min_allowed_len", "max_allowed_len"),
               overwrite = TRUE)
cat("Checkpoint saved after ASV table construction step\n")

# Save ASV length distribution and filtering thresholds for documentation
asv_lengths_info <- list(
  distribution = asv_lengths_df,
  modal_length = modal_length,
  expected_amplicon_size = expected_amplicon_size,
  filtering_thresholds = c(min = min_allowed_len, max = max_allowed_len),
  original_counts = c(asvs = ncol(seqtab), reads = sum(seqtab)),
  filtered_counts = c(asvs = ncol(seqtab_filtered), reads = sum(seqtab_filtered))
)
if (!dir.exists("results")) dir.create("results")
saveRDS(asv_lengths_info, "results/asv_lengths_info.rds")
cat("ASV length distribution and filtering thresholds saved to results/asv_lengths_info.rds\n")

# Create a separate file with just the filtered sequences for easier downstream use
if (!dir.exists("results")) dir.create("results")
saveRDS(colnames(seqtab_filtered), "results/filtered_asv_sequences.rds")
cat("Filtered ASV sequences saved to results/filtered_asv_sequences.rds\n")
```

# Remove Chimeras

```{r chimeras}
# Remove chimeric sequences
cat("Removing chimeric sequences...\n")

# Optimize chimera removal for hardware
optimized_remove_chimeras <- function(seqtab, method = "consensus") {
  # CPU implementation with hardware-specific optimizations
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # Optimize for Apple Silicon
    old_veclib_setting <- Sys.getenv("VECLIB_MAXIMUM_THREADS")
    Sys.setenv(VECLIB_MAXIMUM_THREADS = as.character(hw_caps$optimal_workers))
    
    # Run chimera removal
    result <- removeBimeraDenovo(seqtab, method = method, multithread = TRUE, verbose = TRUE)
    
    # Restore original setting
    Sys.setenv(VECLIB_MAXIMUM_THREADS = old_veclib_setting)
    return(result)
  } else {
    # Standard CPU implementation
    return(removeBimeraDenovo(seqtab, method = method, multithread = TRUE, verbose = TRUE))
  }
}

# For smaller tables, use the non-batched approach
seqtab.nochim <- optimized_remove_chimeras(seqtab, method = "consensus")

# Calculate the percentage of non-chimeric sequences
chimera_pct <- sum(seqtab.nochim)/sum(seqtab) * 100
cat("Percentage of non-chimeric sequences:", round(chimera_pct, 1), "%\n")

# Chimera stats
cat("ASVs before chimera removal:", ncol(seqtab), "\n")
cat("ASVs after chimera removal:", ncol(seqtab.nochim), "\n")
cat("ASVs identified as chimeric:", ncol(seqtab) - ncol(seqtab.nochim), 
    "(", round((ncol(seqtab) - ncol(seqtab.nochim))/ncol(seqtab) * 100, 1), "%)\n")

# Create a summary dataframe
chimera_summary <- data.frame(
  Category = c("Before Chimera Removal", "After Chimera Removal", "Removed as Chimeric"),
  ASVs = c(ncol(seqtab), ncol(seqtab.nochim), ncol(seqtab) - ncol(seqtab.nochim)),
  TotalReads = c(sum(seqtab), sum(seqtab.nochim), sum(seqtab) - sum(seqtab.nochim)),
  Percentage = c(100, chimera_pct, 100 - chimera_pct)
)

kable(chimera_summary, caption = "Chimera Removal Summary")

# Check if chimera removal is excessive
if (chimera_pct < 50) {
  cat("WARNING: Less than 50% of sequences remain after chimera removal.\n")
  cat("This could indicate poor read merging or excessive chimera removal.\n")
  cat("Consider revisiting the truncation lengths and quality filtering parameters.\n")
  
  # Log problematic samples with high chimera rates
  sample_chimera_rates <- data.frame(
    Sample = rownames(seqtab),
    PreChimera = rowSums(seqtab),
    PostChimera = rowSums(seqtab.nochim),
    ChimeraPercent = round((1 - rowSums(seqtab.nochim)/rowSums(seqtab)) * 100, 1)
  )
  
  # Sort by chimera percentage (highest first)
  sample_chimera_rates <- sample_chimera_rates[order(sample_chimera_rates$ChimeraPercent, decreasing = TRUE),]
  
  # Display top 10 most problematic samples
  cat("\nSamples with highest chimera rates:\n")
  print(head(sample_chimera_rates, 10))
}

# Save chimera removal information
if (!dir.exists("results")) dir.create("results")
saveRDS(chimera_summary, "results/chimera_summary.rds")
write.csv(chimera_summary, "results/chimera_summary.csv", row.names = FALSE)
cat("Chimera removal summary saved to results/chimera_summary.csv\n")

# Save per-sample chimera rates if available
if (exists("sample_chimera_rates")) {
  saveRDS(sample_chimera_rates, "results/sample_chimera_rates.rds")
  write.csv(sample_chimera_rates, "results/sample_chimera_rates.csv", row.names = FALSE)
  cat("Sample-specific chimera rates saved to results/sample_chimera_rates.csv\n")
}
```

# Track Reads Through the Pipeline

```{r track}
# Track reads through the pipeline
getN <- function(x) sum(getUniques(x))

# Create a robust tracking function that handles empty or missing data
create_tracking_table <- function() {
  # First check that we have data to track
  if (length(names(derepFs)) == 0) {
    cat("ERROR: No samples available for tracking. Some processing steps may have failed.\n")
    # Return an empty tracking dataframe with the correct structure
    return(data.frame(
      Sample = character(),
      Input = numeric(),
      Filtered = numeric(),
      Denoised_F = numeric(),
      Denoised_R = numeric(),
      Merged = numeric(),
      NonChimeric = numeric()
    ))
  }
  
  # Get all possible sample names from any of the steps
  all_samples <- unique(c(
    rownames(out),
    names(derepFs),
    names(dadaFs),
    names(dadaRs),
    names(mergers)
  ))
  
  # Create a tracking data frame with NA values
  track_df <- data.frame(
    Sample = all_samples,
    Input = NA_real_,
    Filtered = NA_real_,
    Denoised_F = NA_real_,
    Denoised_R = NA_real_,
    Merged = NA_real_,
    NonChimeric = NA_real_,
    row.names = all_samples
  )
  
  # Fill in values from each step
  # Input reads
  if (nrow(out) > 0) {
    for (s in rownames(out)) {
      if (s %in% all_samples) {
        track_df[s, "Input"] <- out[s, "reads.in"]
      }
    }
  }
  
  # Filtered reads
  if (nrow(out) > 0) {
    for (s in rownames(out)) {
      if (s %in% all_samples) {
        track_df[s, "Filtered"] <- out[s, "reads.out"]
      }
    }
  }
  
  # Denoised forward reads
  for (s in names(dadaFs)) {
    if (s %in% all_samples) {
      track_df[s, "Denoised_F"] <- sum(getUniques(dadaFs[[s]]))
    }
  }
  
  # Denoised reverse reads
  for (s in names(dadaRs)) {
    if (s %in% all_samples) {
      track_df[s, "Denoised_R"] <- sum(getUniques(dadaRs[[s]]))
    }
  }
  
  # Merged reads
  for (s in names(mergers)) {
    if (s %in% all_samples) {
      track_df[s, "Merged"] <- sum(getUniques(mergers[[s]]))
    }
  }
  
  # Non-chimeric reads
  if (nrow(seqtab.nochim) > 0) {
    for (s in rownames(seqtab.nochim)) {
      if (s %in% all_samples) {
        track_df[s, "NonChimeric"] <- sum(seqtab.nochim[s, ])
      }
    }
  }
  
  # Keep only samples with complete data to avoid NA issues
  complete_samples <- rownames(track_df)[complete.cases(track_df)]
  if (length(complete_samples) < nrow(track_df)) {
    cat("Warning: ", nrow(track_df) - length(complete_samples), 
        " samples had incomplete data and were removed from tracking.\n")
    if (length(complete_samples) == 0) {
      cat("ERROR: No samples have complete data through all pipeline steps.\n")
      cat("This suggests a failure in one or more processing steps.\n")
      # Return the incomplete data anyway for diagnostics
      return(track_df)
    }
    track_df <- track_df[complete_samples, ]
  }
  
  return(track_df)
}

# Create the tracking table
track_df <- create_tracking_table()

# Check if we have data to proceed
if (nrow(track_df) == 0) {
  cat("WARNING: No samples could be tracked through the pipeline.\n")
  cat("Skipping tracking visualization.\n")
} else {
  cat("Successfully tracked", nrow(track_df), "samples through the pipeline.\n")
}

# Make sure we have the Sample column for display
if (!("Sample" %in% colnames(track_df)) && nrow(track_df) > 0) {
  track_df$Sample <- rownames(track_df)
}

# Calculate percentage retained at each step if we have data
if (nrow(track_df) > 0) {
  # Safe division function to handle division by zero
  safe_div <- function(num, denom) {
    ifelse(denom > 0, num / denom * 100, NA)
  }
  
  track_df$Percent_Filtered <- round(safe_div(track_df$Filtered, track_df$Input), 1)
  track_df$Percent_Denoised_F <- round(safe_div(track_df$Denoised_F, track_df$Filtered), 1)
  track_df$Percent_Merged <- round(safe_div(track_df$Merged, track_df$Denoised_F), 1)
  track_df$Percent_NonChimeric <- round(safe_div(track_df$NonChimeric, track_df$Merged), 1)
  track_df$Percent_Final <- round(safe_div(track_df$NonChimeric, track_df$Input), 1)
}

# Display tracking information for the first few samples
kable(head(track_df[, c("Sample", "Input", "Filtered", "Denoised_F", "Merged", "NonChimeric", "Percent_Final")], 10), 
      caption = "Read Tracking Through Pipeline (First 10 Samples)")

# Create a summary of read loss at each step if we have data
if (nrow(track_df) > 0) {
  # Safe sum function
  safe_sum <- function(x) {
    sum(x, na.rm = TRUE)
  }
  
  # Safe division function
  safe_percent <- function(numerator, denominator) {
    if (denominator > 0) {
      round(numerator / denominator * 100, 1)
    } else {
      NA
    }
  }
  
  # Calculate totals
  total_input <- safe_sum(track_df$Input)
  total_filtered <- safe_sum(track_df$Filtered)
  total_denoised <- safe_sum(track_df$Denoised_F)
  total_merged <- safe_sum(track_df$Merged)
  total_nonchim <- safe_sum(track_df$NonChimeric)
  
  # Create summary data frame
  pipeline_summary <- data.frame(
    Step = c("Raw Input", "After Filtering", "After Denoising", "After Merging", "After Chimera Removal"),
    Reads = c(total_input, total_filtered, total_denoised, total_merged, total_nonchim),
    PercentOfInput = c(
      100, 
      safe_percent(total_filtered, total_input),
      safe_percent(total_denoised, total_input),
      safe_percent(total_merged, total_input),
      safe_percent(total_nonchim, total_input)
    ),
    PercentOfPreviousStep = c(
      100,
      safe_percent(total_filtered, total_input),
      safe_percent(total_denoised, total_filtered),
      safe_percent(total_merged, total_denoised),
      safe_percent(total_nonchim, total_merged)
    )
  )
  
  kable(pipeline_summary, caption = "Summary of Reads Retained at Each Processing Step")
} else {
  cat("No tracking data available for pipeline summary.\n")
}

# Create a plot to visualize read tracking if we have data
if (nrow(track_df) > 0) {
  # Try to create visualizations with proper error handling
  tryCatch({
    # Reshape data for plotting
    track_long <- tidyr::pivot_longer(
      track_df[, c("Sample", "Input", "Filtered", "Denoised_F", "Merged", "NonChimeric")],
      cols = c("Input", "Filtered", "Denoised_F", "Merged", "NonChimeric"),
      names_to = "Step",
      values_to = "Reads"
    )
    
    # Convert Step to factor with specified order
    track_long$Step <- factor(track_long$Step, 
                              levels = c("Input", "Filtered", "Denoised_F", "Merged", "NonChimeric"))
    
    # Create the plot for all samples together
    overall_tracking_plot <- ggplot(track_long, aes(x = Step, y = Reads, group = Sample, color = Sample)) +
      geom_line(alpha = 0.5) +
      geom_point(size = 1) +
      theme_minimal() +
      labs(title = "Read Count Tracking Through DADA2 Pipeline",
           x = "Processing Step",
           y = "Number of Reads") +
      theme(legend.position = "none")  # Hide individual sample legend as it can be cluttered
    
    # Create an average tracking plot
    avg_tracking <- aggregate(Reads ~ Step, data = track_long, FUN = mean, na.rm = TRUE)
    avg_tracking$StdDev <- aggregate(Reads ~ Step, data = track_long, FUN = sd, na.rm = TRUE)$Reads
    
    avg_tracking_plot <- ggplot(avg_tracking, aes(x = Step, y = Reads, group = 1)) +
      geom_line(size = 1.5, color = "blue") +
      geom_point(size = 3, color = "blue") +
      geom_errorbar(aes(ymin = pmax(Reads - StdDev, 0), ymax = Reads + StdDev), width = 0.2, color = "blue") +
      theme_minimal() +
      labs(title = "Average Read Counts Through DADA2 Pipeline",
           subtitle = "Error bars show standard deviation across samples",
           x = "Processing Step",
           y = "Average Number of Reads")
    
    # Show both plots
    grid.arrange(overall_tracking_plot, avg_tracking_plot, ncol = 1)
  }, error = function(e) {
    cat("Error creating read tracking plots:", conditionMessage(e), "\n")
    cat("Displaying simple summary statistics instead.\n")
    
    # Display summary statistics as an alternative
    step_summary <- sapply(c("Input", "Filtered", "Denoised_F", "Merged", "NonChimeric"), function(step) {
      values <- track_df[[step]]
      c(Mean = mean(values, na.rm = TRUE),
        Min = min(values, na.rm = TRUE),
        Max = max(values, na.rm = TRUE))
    })
    
    print(step_summary)
  })
} else {
  cat("No tracking data available for visualization.\n")
}

# Save the tracking data for dashboard visualization if we have results directory and data
if (nrow(track_df) > 0) {
  if (!dir.exists("results")) dir.create("results")
  
  tryCatch({
    # Save the basic tracking data frame
    saveRDS(track_df, "results/read_tracking.rds")
    cat("Saved read tracking data to results/read_tracking.rds\n")
    
    # Save a more detailed tracking data that includes percentages
    tracking_detailed <- track_df
    if (ncol(track_df) >= 7) {
      write.csv(tracking_detailed, "results/read_tracking_detailed.csv")
      cat("Saved detailed tracking data to results/read_tracking_detailed.csv\n")
    }
    
    # Save pipeline summary info separately for easy access
    if (exists("pipeline_summary")) {
      saveRDS(pipeline_summary, "results/pipeline_summary.rds")
      write.csv(pipeline_summary, "results/pipeline_summary.csv")
      cat("Saved pipeline summary to results/pipeline_summary.csv\n")
    }
  }, error = function(e) {
    cat("Error saving tracking data:", conditionMessage(e), "\n")
  })
} else {
  cat("No tracking data available to save.\n")
}
```

# Assign Taxonomy with Multiple Methods

```{r taxonomy-setup}
# Load additional libraries for taxonomy assignment
if (!requireNamespace("taxize", quietly = TRUE)) {
  cat("Installing taxize package for taxonomic name resolution...\n")
  install.packages("taxize")
}

# Create directory for reference databases if it doesn't exist
if(!dir.exists("ref_db")) dir.create("ref_db")

# Function to download and manage reference databases
download_ref_db <- function(db_name) {
  cat("Setting up", db_name, "reference database...\n")
  
  # Database-specific setup
  if (db_name == "SILVA") {
    # Check if Silva files are available in the DADA2 package
    silva_train <- system.file("extdata", "silva_nr99_v138.1_train_set.fa.gz", package="dada2")
    silva_species <- system.file("extdata", "silva_species_assignment_v138.1.fa.gz", package="dada2")
    
    silva_files_in_package <- (file.exists(silva_train) && file.exists(silva_species))
    
    if(silva_files_in_package) {
      cat("Using Silva database files from DADA2 package\n")
    } else {
      cat("Silva files not found in DADA2 package, will download from Zenodo\n")
      
      # URLs for Silva files from Zenodo
      silva_url_base <- "https://zenodo.org/records/4587955/files/"
      silva_train_url <- paste0(silva_url_base, "silva_nr99_v138.1_train_set.fa.gz")
      silva_species_url <- paste0(silva_url_base, "silva_species_assignment_v138.1.fa.gz")
      
      # Local file paths for Silva files
      silva_train <- file.path("ref_db", "silva_train_set.fa.gz")
      silva_species <- file.path("ref_db", "silva_species.fa.gz")
      
      # Download training set if needed
      if(!file.exists(silva_train)) {
        cat("Downloading Silva taxonomy training file...\n")
        # Set extended timeout
        options(timeout = max(300, getOption("timeout")))
        
        tryCatch({
          download.file(silva_train_url, silva_train, method="auto", mode="wb")
          cat("Download successful!\n")
        }, error = function(e) {
          cat("Error downloading Silva taxonomy file:", conditionMessage(e), "\n")
          cat("Please try downloading it manually from:", silva_train_url, "\n")
          cat("and save it to:", silva_train, "\n")
          stop("Download failed. Please download files manually.")
        })
      }
      
      # Download species file if needed
      if(!file.exists(silva_species)) {
        cat("Downloading Silva species assignment file...\n")
        
        tryCatch({
          download.file(silva_species_url, silva_species, method="auto", mode="wb")
          cat("Download successful!\n")
        }, error = function(e) {
          cat("Error downloading Silva species file:", conditionMessage(e), "\n")
          cat("Please try downloading it manually from:", silva_species_url, "\n")
          cat("and save it to:", silva_species, "\n")
          stop("Download failed. Please download files manually.")
        })
      }
    }
    
    # Check if files exist before proceeding
    if(!file.exists(silva_train) || !file.exists(silva_species)) {
      stop("Required Silva files not available. Please download them manually as noted above.")
    }
    
    return(list(
      train = silva_train,
      species = silva_species
    ))
  }
  
  # Setup for RDP classifier/database
  else if (db_name == "RDP") {
    rdp_train <- file.path("ref_db", "rdp_train_set_18.fa.gz")
    rdp_species <- file.path("ref_db", "rdp_species_assignment_18.fa.gz")
    
    if (!file.exists(rdp_train) || !file.exists(rdp_species)) {
      cat("Downloading RDP training files...\n")
      
      # Updated URLs for RDP files from dada2 website
      rdp_train_url <- "https://zenodo.org/record/4310151/files/rdp_train_set_18.fa.gz"
      rdp_species_url <- "https://zenodo.org/record/4310151/files/rdp_species_assignment_18.fa.gz"
      
      # Download training files
      options(timeout = max(300, getOption("timeout")))
      
      if (!file.exists(rdp_train)) {
        tryCatch({
          download.file(rdp_train_url, rdp_train, method = "auto", mode = "wb")
          cat("RDP training set downloaded successfully!\n")
        }, error = function(e) {
          cat("Error downloading RDP training set:", conditionMessage(e), "\n")
          stop("Download failed. Please download files manually.")
        })
      }
      
      if (!file.exists(rdp_species)) {
        tryCatch({
          download.file(rdp_species_url, rdp_species, method = "auto", mode = "wb")
          cat("RDP species assignment file downloaded successfully!\n")
        }, error = function(e) {
          cat("Error downloading RDP species file:", conditionMessage(e), "\n")
          stop("Download failed. Please download files manually.")
        })
      }
    } else {
      cat("Using existing RDP database files\n")
    }
    
    return(list(
      train = rdp_train,
      species = rdp_species
    ))
  }
  
  # If unknown database is requested
  else {
    stop("Unknown database: ", db_name)
  }
}

# Get standard taxonomy names across databases
standardize_taxonomy <- function(taxa_df, method) {
  # Convert to data frame if it's a matrix
  if(is.matrix(taxa_df)) {
    taxa_df <- as.data.frame(taxa_df)
  }
  
  # Make all column names consistent
  standard_cols <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
  
  # RDP specific standardization
  if (method == "RDP_DADA2") {
    # RDP sometimes uses Root instead of Kingdom
    if ("Root" %in% colnames(taxa_df)) {
      colnames(taxa_df)[colnames(taxa_df) == "Root"] <- "Kingdom"
    }
  }
  
  # Convert all column names to standard case
  colnames(taxa_df) <- toupper(colnames(taxa_df))
  
  # Map common variations to standard names
  col_map <- c(
    "KINGDOM" = "Kingdom",
    "DOMAIN" = "Kingdom",
    "ROOT" = "Kingdom",
    "PHYLUM" = "Phylum",
    "CLASS" = "Class",
    "ORDER" = "Order",
    "FAMILY" = "Family",
    "GENUS" = "Genus",
    "SPECIES" = "Species"
  )
  
  for(old_col in names(col_map)) {
    if(old_col %in% colnames(taxa_df)) {
      colnames(taxa_df)[colnames(taxa_df) == old_col] <- col_map[old_col]
    }
  }
  
  # Ensure all standard columns exist, filling with NA if missing
  result_df <- data.frame(matrix(NA, nrow=nrow(taxa_df), ncol=length(standard_cols)))
  colnames(result_df) <- standard_cols
  
  # Copy data from original to standardized columns
  for(col in standard_cols) {
    if(col %in% colnames(taxa_df)) {
      result_df[[col]] <- taxa_df[[col]]
    }
  }
  
  return(result_df)
}

# This function will be used to summarize taxonomy assignment across methods
summarize_taxonomy <- function(taxa_list) {
  # Create a data frame to store summary
  summary_df <- data.frame(
    Method = character(),
    Kingdom = numeric(),
    Phylum = numeric(),
    Class = numeric(),
    Order = numeric(),
    Family = numeric(),
    Genus = numeric(),
    Species = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Define standard levels
  standard_levels <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
  
  # Calculate assignments for each method
  for (method_name in names(taxa_list)) {
    # Get this taxonomy result
    taxa_df <- taxa_list[[method_name]]
    
    # Convert to data frame if it's a matrix
    if(is.matrix(taxa_df)) {
      taxa_df <- as.data.frame(taxa_df, stringsAsFactors = FALSE)
    }
    
    # Create the row for this method
    method_row <- c(method_name)
    
    # Calculate percentage for each level
    for(level in standard_levels) {
      if(level %in% colnames(taxa_df)) {
        # If column exists, calculate percentage of non-NA values
        percent <- round(sum(!is.na(taxa_df[[level]])) / nrow(taxa_df) * 100, 1)
      } else {
        # If column doesn't exist, percentage is 0
        percent <- 0
      }
      method_row <- c(method_row, percent)
    }
    
    # Add to summary data frame
    summary_df <- rbind(summary_df, method_row)
  }
  
  # Set column names
  colnames(summary_df) <- c("Method", standard_levels)
  
  # Convert percentage columns to numeric
  for (col in standard_levels) {
    summary_df[[col]] <- as.numeric(summary_df[[col]])
  }
  
  return(summary_df)
}

# Function to plot taxonomy assignment comparison
plot_taxonomy_comparison <- function(summary_df) {
  # Reshape data for plotting
  summary_long <- tidyr::pivot_longer(
    summary_df, 
    cols = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    names_to = "TaxonomicLevel", 
    values_to = "PercentAssigned"
  )
  
  # Create the comparison plot
  ggplot(summary_long, aes(x = TaxonomicLevel, y = PercentAssigned, fill = Method)) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.9)) +
    geom_text(aes(label = paste0(PercentAssigned, "%")), 
              position = position_dodge(width = 0.9), 
              vjust = -0.5, size = 3) +
    theme_minimal() +
    labs(title = "Taxonomic Assignment Comparison Across Methods",
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_fill_brewer(palette = "Set1")
}

# Choose which taxonomic classification methods to use
# Set to TRUE/FALSE to enable/disable methods
taxonomy_methods <- list(
  SILVA_DADA2 = TRUE,      # DADA2's implementation with SILVA database
  RDP_DADA2 = TRUE,        # DADA2's implementation with RDP database
  IDTAXA = FALSE           # DECIPHER's IDTAXA implementation (DISABLED)
)

# Storage for taxonomy results
taxa_results <- list()
```
# SILVA Taxonomy with Confidence Scores

## Enhanced Taxonomy Assignment with Confidence Scores

This section implements reference-based taxonomy confidence scoring functionality to improve taxonomy assignment reliability assessment.

Key enhancements include:

1. **Bootstrap confidence scores for taxonomic assignments**
   - Uses DADA2's bootstrap confidence estimation
   - Reports confidence percentages (0-100%) for each taxonomic level
   - Provides species-level confidence for exact matches

2. **Visualization of confidence scores**
   - Distribution of confidence values by taxonomic level
   - Identification of low-confidence assignments
   - Summary statistics of assignment confidence

3. **Confidence-based taxonomy filtering**
   - Optional filtering based on confidence thresholds
   - Customizable thresholds by taxonomic level
   - Comparison between raw and filtered assignments

4. **Data export**
   - Taxonomic assignments with confidence scores
   - Filtered taxonomy based on confidence thresholds
   - Visualization outputs for reports

The confidence scores help researchers:
- Assess reliability of taxonomic assignments
- Identify potentially problematic assignments
- Apply appropriate filtering based on research needs
- Report assignment quality metrics in publications

Confidence thresholds can be customized based on study requirements, with more stringent thresholds typically applied at lower taxonomic levels where assignment uncertainty is higher.

```{r silva-taxonomy}

# Initialize empty taxa_results list if not already created
if(!exists("taxa_results")) {
  taxa_results <- list()
}

# DADA2 with SILVA database (default method)
if (taxonomy_methods$SILVA_DADA2) {
  cat("\n=== Method 1: DADA2 with SILVA database (with confidence scores) ===\n")
  
  # Download/locate SILVA database
  silva_files <- download_ref_db("SILVA")
  
  # Assign taxonomy at the genus level with optimized parallelization and include confidence scores
  cat("Assigning taxonomy using Silva database with confidence scores and optimized parallelization...\n")

  # For large datasets, use batched taxonomy assignment
  if(ncol(seqtab.nochim) > 3000) {
    cat("Using batched approach for taxonomy assignment due to large number of ASVs...\n")
    
    # Split ASVs into batches
    batch_size <- 1000  # Process 1000 ASVs at a time
    asv_sequences <- colnames(seqtab.nochim)
    n_batches <- ceiling(length(asv_sequences) / batch_size)
    
    # Initialize results matrices
    taxa_silva <- NULL
    taxa_silva_confidence <- NULL
    
    # Process each batch
    for(i in 1:n_batches) {
      cat(sprintf("Processing taxonomy batch %d of %d...\n", i, n_batches))
      
      # Calculate batch indices
      start_idx <- (i-1) * batch_size + 1
      end_idx <- min(i * batch_size, length(asv_sequences))
      
      # Extract batch
      batch_seqs <- asv_sequences[start_idx:end_idx]
      
      # Create a mini sequence table with just these ASVs
      batch_seqtab <- seqtab.nochim[, batch_seqs, drop=FALSE]
      
      # Assign taxonomy for this batch with confidence scores
      batch_taxa_result <- assignTaxonomy(batch_seqtab, silva_files$train, multithread=TRUE, 
                                         outputBootstraps=TRUE)
      
      # Extract taxonomic assignments and bootstrap confidence values
      batch_taxa <- batch_taxa_result$tax
      batch_confidence <- batch_taxa_result$boot
      
      # Add species assignments with confidence scores
      batch_species_result <- addSpecies(batch_taxa, silva_files$species, allowMultiple=FALSE,
                                        tryRC=TRUE, verbose=TRUE)
      
      # Prepare species-level confidence (assumed 100% for exact matches, NA for no match)
      n_asvs <- nrow(batch_taxa)
      species_confidence <- rep(NA, n_asvs)
      species_confidence[!is.na(batch_species_result[, "Species"])] <- 100
      
      # Add species confidence to the bootstrap confidence matrix
      batch_confidence <- cbind(batch_confidence, Species=species_confidence)
      
      # Combine with previous results
      if(is.null(taxa_silva)) {
        taxa_silva <- batch_species_result
        taxa_silva_confidence <- batch_confidence
      } else {
        taxa_silva <- rbind(taxa_silva, batch_species_result)
        taxa_silva_confidence <- rbind(taxa_silva_confidence, batch_confidence)
      }
      
      # Force garbage collection
      gc()
    }
  } else {
    # For smaller datasets, use the standard approach with confidence scores
    cat("Assigning taxonomy with bootstrap confidence values...\n")
    taxa_result <- assignTaxonomy(seqtab.nochim, silva_files$train, multithread=TRUE, 
                                  outputBootstraps=TRUE)
    
    # Extract taxonomic assignments and bootstrap confidence values
    taxa_silva <- taxa_result$tax
    taxa_silva_confidence <- taxa_result$boot
    
    # Add species-level assignments
    cat("Adding species-level assignments...\n")
    taxa_silva <- addSpecies(taxa_silva, silva_files$species, allowMultiple=FALSE,
                            tryRC=TRUE, verbose=TRUE)
    
    # Prepare species-level confidence (assumed 100% for exact matches, NA for no match)
    n_asvs <- nrow(taxa_silva)
    species_confidence <- rep(NA, n_asvs)
    species_confidence[!is.na(taxa_silva[, "Species"])] <- 100
    
    # Add species confidence to the bootstrap confidence matrix
    taxa_silva_confidence <- cbind(taxa_silva_confidence, Species=species_confidence)
  }
  
  # View taxonomic assignments and confidence scores
  cat("First few taxonomic assignments with confidence scores:\n")
  print(head(taxa_silva))
  cat("\nCorresponding confidence scores (bootstrap support %):\n")
  print(head(taxa_silva_confidence))
  
  # Create a combined data frame with taxa and confidence for easier visualization and export
  taxa_with_confidence <- as.data.frame(taxa_silva)
  taxa_with_confidence$ASV_ID <- rownames(taxa_with_confidence)
  
  # Create confidence data frame
  confidence_df <- as.data.frame(taxa_silva_confidence)
  confidence_df$ASV_ID <- rownames(confidence_df)
  
  # Add confidence score columns with prefix "Confidence_"
  for (level in c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")) {
    if (level %in% colnames(confidence_df)) {
      confidence_col_name <- paste0("Confidence_", level)
      taxa_with_confidence[[confidence_col_name]] <- confidence_df[[level]]
    }
  }
  
  # Save to results list
  taxa_results$SILVA_DADA2 <- taxa_silva
  taxa_results$SILVA_DADA2_confidence <- taxa_silva_confidence
  taxa_results$SILVA_DADA2_combined <- taxa_with_confidence
  
  # Export the combined taxonomy and confidence scores
  if (!dir.exists("results")) dir.create("results")
  write.csv(taxa_with_confidence, "results/taxonomy_with_confidence.csv", row.names = FALSE)
  cat("Saved taxonomy with confidence scores to results/taxonomy_with_confidence.csv\n")
  
  # Create a taxonomy summary for SILVA
  silva_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_silva[, "Kingdom"])),
      sum(!is.na(taxa_silva[, "Phylum"])),
      sum(!is.na(taxa_silva[, "Class"])),
      sum(!is.na(taxa_silva[, "Order"])),
      sum(!is.na(taxa_silva[, "Family"])),
      sum(!is.na(taxa_silva[, "Genus"])),
      sum(!is.na(taxa_silva[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_silva[, "Kingdom"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Phylum"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Class"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Order"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Family"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Genus"])) / nrow(taxa_silva) * 100, 1),
      round(sum(!is.na(taxa_silva[, "Species"])) / nrow(taxa_silva) * 100, 1)
    ),
    MeanConfidence = c(
      mean(taxa_silva_confidence[, "Kingdom"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Phylum"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Class"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Order"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Family"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Genus"], na.rm = TRUE),
      mean(taxa_silva_confidence[, "Species"], na.rm = TRUE)
    )
  )
  
  # Display SILVA summary table
  kable(silva_summary, caption = "SILVA Taxonomic Assignment Summary with Confidence Scores")
  
  # Visualize confidence scores distribution by taxonomic level
  confidence_long <- tidyr::pivot_longer(
    as.data.frame(taxa_silva_confidence),
    cols = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    names_to = "TaxonomicLevel",
    values_to = "Confidence"
  )
  
  # Remove NA values
  confidence_long <- confidence_long[!is.na(confidence_long$Confidence), ]
  
  # Order taxonomic levels by assignment hierarchy
  confidence_long$TaxonomicLevel <- factor(confidence_long$TaxonomicLevel, 
                                        levels = c("Kingdom", "Phylum", "Class", 
                                                  "Order", "Family", "Genus", "Species"))
  
  # Create violin plot showing confidence score distributions
  confidence_violin_plot <- ggplot(confidence_long, aes(x = TaxonomicLevel, y = Confidence, fill = TaxonomicLevel)) +
    geom_violin(alpha = 0.7) +
    geom_boxplot(width = 0.1, fill = "white", color = "darkgray") +
    stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") +
    scale_y_continuous(limits = c(0, 100)) +
    labs(title = "Distribution of Taxonomy Assignment Confidence Scores",
         x = "Taxonomic Level",
         y = "Bootstrap Confidence (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "none")
  
  print(confidence_violin_plot)
  
  # Create heatmap for low confidence assignments
  # Calculate confidence threshold
  confidence_threshold <- 80
  
  # Filter for low confidence assignments
  low_confidence <- confidence_long %>%
    filter(Confidence < confidence_threshold) %>%
    group_by(TaxonomicLevel) %>%
    summarize(Count = n(),
              Percentage = round(n() / sum(!is.na(taxa_silva_confidence[, TaxonomicLevel])) * 100, 1))
  
  if (nrow(low_confidence) > 0) {
    # Plot low confidence summary
    low_confidence_plot <- ggplot(low_confidence, aes(x = TaxonomicLevel, y = Percentage, fill = TaxonomicLevel)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = paste0(Percentage, "%\n(n=", Count, ")")), vjust = -0.5) +
      labs(title = paste0("Low Confidence (<", confidence_threshold, "%) Assignments by Taxonomic Level"),
           x = "Taxonomic Level",
           y = "Percentage of Assignments") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            legend.position = "none")
    
    print(low_confidence_plot)
  } else {
    cat("No low confidence assignments found (<", confidence_threshold, "%).\n")
  }
  
  # Create a bar plot for SILVA results with confidence info
  # Order the data by percent assigned (descending)
  silva_summary$TaxonomicLevel <- factor(silva_summary$TaxonomicLevel, 
                                       levels = silva_summary$TaxonomicLevel[order(silva_summary$PercentAssigned, decreasing = TRUE)])
  
  # Create the plot with confidence overlay
  silva_plot <- ggplot(silva_summary, aes(x = TaxonomicLevel, y = PercentAssigned)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    geom_text(aes(label = paste0(PercentAssigned, "%")), vjust = -0.3) +
    geom_text(aes(label = paste0("(", round(MeanConfidence, 1), "% conf.)")), vjust = 1.5, color = "darkblue") +
    theme_minimal() +
    labs(title = "SILVA: Percentage of ASVs with Taxonomic Assignment",
         subtitle = "Mean confidence score shown in parentheses",
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, max(silva_summary$PercentAssigned) * 1.2))
  
  print(silva_plot)
  
  # Save visualization files
  ggsave("results/confidence_distribution.pdf", confidence_violin_plot, width = 8, height = 6)
  ggsave("results/confidence_distribution.png", confidence_violin_plot, width = 8, height = 6, dpi = 300)
  
  if (nrow(low_confidence) > 0) {
    ggsave("results/low_confidence_summary.pdf", low_confidence_plot, width = 8, height = 6)
    ggsave("results/low_confidence_summary.png", low_confidence_plot, width = 8, height = 6, dpi = 300)
  }
  
  ggsave("results/taxonomy_assignment_with_confidence.pdf", silva_plot, width = 8, height = 6)
  ggsave("results/taxonomy_assignment_with_confidence.png", silva_plot, width = 8, height = 6, dpi = 300)
}
```

# RDP taxonomy

```{r rdp-taxonomy}
# DADA2 with RDP database (optional method)
if (taxonomy_methods$RDP_DADA2) {
  cat("\n=== Method 2: DADA2 with RDP database ===\n")
  
  # Download/locate RDP database
  rdp_files <- download_ref_db("RDP")
  
  # Assign taxonomy with RDP
  cat("Assigning taxonomy using RDP database...\n")
  taxa_rdp <- assignTaxonomy(seqtab.nochim, rdp_files$train, multithread=TRUE)
  
  # Add species-level assignments
  cat("Adding species-level assignments...\n")
  taxa_rdp <- addSpecies(taxa_rdp, rdp_files$species)
  
  # View RDP taxonomic assignments
  cat("First few RDP taxonomic assignments:\n")
  print(head(taxa_rdp))
  
  # Make sure columns follow the standard naming
  standard_cols <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
  for(col in standard_cols) {
    if(!(col %in% colnames(taxa_rdp))) {
      taxa_rdp[, col] <- NA
    }
  }
  
  # Make sure only standard columns are kept
  taxa_rdp <- taxa_rdp[, standard_cols]
  
  # Save to results list
  taxa_results$RDP_DADA2 <- taxa_rdp
  
  # Create a taxonomy summary for RDP
  rdp_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_rdp[, "Kingdom"])),
      sum(!is.na(taxa_rdp[, "Phylum"])),
      sum(!is.na(taxa_rdp[, "Class"])),
      sum(!is.na(taxa_rdp[, "Order"])),
      sum(!is.na(taxa_rdp[, "Family"])),
      sum(!is.na(taxa_rdp[, "Genus"])),
      sum(!is.na(taxa_rdp[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_rdp[, "Kingdom"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Phylum"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Class"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Order"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Family"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Genus"])) / nrow(taxa_rdp) * 100, 1),
      round(sum(!is.na(taxa_rdp[, "Species"])) / nrow(taxa_rdp) * 100, 1)
    )
  )
  
  # Display RDP summary table
  kable(rdp_summary, caption = "RDP Taxonomic Assignment Summary")
}
```

# IDTAXA Taxonomy (DISABLED)

```{r idtaxa-taxonomy}
# IDTAXA implementation has been removed per user request
cat("\n=== IDTAXA implementation has been disabled ===\n")
cat("The IDTAXA taxonomy assignment functionality has been removed from this workflow.\n")
cat("Taxonomy assignment is now performed using only the SILVA and RDP methods via DADA2.\n")
```

# Taxonomy comparison

```{r taxonomy-comparison}
# Compare taxonomy assignments from different methods
if (length(taxa_results) > 1) {
  cat("\n=== Comparing Taxonomy Assignments Across Methods ===\n")
  
  # Create taxonomy assignment summary across methods
  taxonomy_summary <- summarize_taxonomy(taxa_results)
  
  # Display taxonomy comparison table
  kable(taxonomy_summary, caption = "Taxonomic Assignment Comparison Across Methods")
  
  # Create a comparative visualization
  comparison_plot <- plot_taxonomy_comparison(taxonomy_summary)
  print(comparison_plot)
  
  # Create a consensus taxonomy
  cat("\nCreating consensus taxonomy from all methods...\n")
  
  # Function to create consensus taxonomy
  create_consensus_taxonomy <- function(taxa_list, min_methods = 2) {
    # Get all ASV identifiers
    all_asvs <- rownames(taxa_list[[1]])
    
    # Initialize consensus taxonomy matrix
    consensus <- matrix(NA, nrow = length(all_asvs), ncol = 7)
    colnames(consensus) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
    rownames(consensus) <- all_asvs
    
    # For each taxonomic level and ASV
    for (level in colnames(consensus)) {
      for (asv in all_asvs) {
        # Collect assignments from all methods
        assignments <- sapply(taxa_list, function(x) x[asv, level])
        
        # Remove NAs
        assignments <- assignments[!is.na(assignments)]
        
        # If we have enough assignments, find the most common one
        if (length(assignments) >= min_methods) {
          # Count occurrences of each assignment
          counts <- table(assignments)
          
          # Get the most common assignment
          most_common <- names(counts)[which.max(counts)]
          
          # Only use it if it appears in at least min_methods
          if (counts[most_common] >= min_methods) {
            consensus[asv, level] <- most_common
          }
        }
      }
    }
    
    return(as.data.frame(consensus))
  }
  
  # Create consensus taxonomy
  min_methods <- min(2, length(taxa_results))  # At least 2 methods must agree, or all if fewer than 2
  taxa_consensus <- create_consensus_taxonomy(taxa_results, min_methods)
  
  # Summarize consensus taxonomy
  consensus_summary <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    AssignmentCount = c(
      sum(!is.na(taxa_consensus[, "Kingdom"])),
      sum(!is.na(taxa_consensus[, "Phylum"])),
      sum(!is.na(taxa_consensus[, "Class"])),
      sum(!is.na(taxa_consensus[, "Order"])),
      sum(!is.na(taxa_consensus[, "Family"])),
      sum(!is.na(taxa_consensus[, "Genus"])),
      sum(!is.na(taxa_consensus[, "Species"]))
    ),
    PercentAssigned = c(
      round(sum(!is.na(taxa_consensus[, "Kingdom"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Phylum"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Class"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Order"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Family"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Genus"])) / nrow(taxa_consensus) * 100, 1),
      round(sum(!is.na(taxa_consensus[, "Species"])) / nrow(taxa_consensus) * 100, 1)
    )
  )
  
  # Display consensus summary table
  kable(consensus_summary, caption = paste0("Consensus Taxonomy (Minimum ", min_methods, " Methods Agreement)"))
  
  # Create a bar plot for consensus results
  consensus_summary$TaxonomicLevel <- factor(consensus_summary$TaxonomicLevel, 
                                          levels = consensus_summary$TaxonomicLevel[order(consensus_summary$PercentAssigned, decreasing = TRUE)])
  
  consensus_plot <- ggplot(consensus_summary, aes(x = TaxonomicLevel, y = PercentAssigned)) +
    geom_bar(stat = "identity", fill = "purple") +
    geom_text(aes(label = paste0(PercentAssigned, "%")), vjust = -0.3) +
    theme_minimal() +
    labs(title = "Consensus: Percentage of ASVs with Taxonomic Assignment",
         subtitle = paste0("Minimum ", min_methods, " methods must agree"),
         x = "Taxonomic Level",
         y = "Percent Assigned") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, max(consensus_summary$PercentAssigned) * 1.1))
  
  print(consensus_plot)
  
  # Use the consensus taxonomy as the final result if it exists
  if (nrow(taxa_consensus) > 0) {
    taxa <- as.matrix(taxa_consensus)
    cat("Using consensus taxonomy for final results\n")
  } else {
    # Use the first method as backup
    taxa <- taxa_results[[1]]
    cat("Using", names(taxa_results)[1], "for final results (consensus not available)\n")
  }
} else if (length(taxa_results) == 1) {
  # If only one method was used, use its results
  taxa <- taxa_results[[1]]
  cat("Using", names(taxa_results)[1], "for final results\n")
} else {
  # Error if no taxonomy method was successful
  stop("No taxonomy assignment method was successful!")
}

# Function to filter taxonomy based on confidence thresholds
filter_taxonomy_by_confidence <- function(taxa, confidence, thresholds = NULL) {
  # Default thresholds by taxonomic level - becomes more stringent at lower levels
  if (is.null(thresholds)) {
    thresholds <- c(
      Kingdom = 50,
      Phylum = 60, 
      Class = 70,
      Order = 75,
      Family = 80,
      Genus = 85,
      Species = 95  # Very high threshold for species
    )
  }
  
  # Create a copy of the taxonomy
  filtered_taxa <- taxa
  
  # Filter each level based on confidence
  for (level in names(thresholds)) {
    if (level %in% colnames(confidence)) {
      threshold <- thresholds[level]
      mask <- !is.na(confidence[, level]) & confidence[, level] < threshold
      filtered_taxa[mask, level] <- NA
      cat(sprintf("Filtered %d %s-level assignments below %d%% confidence\n", 
                 sum(mask), level, threshold))
    }
  }
  
  return(filtered_taxa)
}

# Function to add confidence-filtered taxonomy to a phyloseq object
add_confidence_filtered_taxonomy <- function(ps, taxa_filtered) {
  # Check if input phyloseq has tax_table
  if (is.null(tax_table(ps, errorIfNULL=FALSE))) {
    stop("Input phyloseq object does not have a taxonomy table")
  }
  
  # Ensure tax_table and filtered_taxa have the same dimensions and taxa
  if (!identical(dim(tax_table(ps)), dim(taxa_filtered)) || 
      !all(taxa_names(ps) == rownames(taxa_filtered))) {
    stop("Filtered taxonomy does not match the dimensions or taxa in the phyloseq object")
  }
  
  # Create a copy of the phyloseq object
  ps_filtered <- ps
  
  # Replace tax_table with filtered version
  tax_table(ps_filtered) <- tax_table(as.matrix(taxa_filtered))
  
  return(ps_filtered)
}

# Example of using confidence filtering if confidence scores are available
if (exists("taxa_silva") && exists("taxa_silva_confidence")) {
  cat("\n=== Example: Filtering Taxonomy by Confidence Scores ===\n")
  
  # Define custom confidence thresholds (optional)
  custom_thresholds <- c(
    Kingdom = 60,
    Phylum = 70, 
    Class = 75,
    Order = 80,
    Family = 85,
    Genus = 90,
    Species = 98  # Very high threshold for species
  )
  
  # Filter taxonomy using custom thresholds
  taxa_silva_filtered <- filter_taxonomy_by_confidence(
    taxa_silva, 
    taxa_silva_confidence,
    thresholds = custom_thresholds
  )
  
  # Compare original and filtered taxonomy
  compare_df <- data.frame(
    TaxonomicLevel = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"),
    Original = c(
      sum(!is.na(taxa_silva[, "Kingdom"])),
      sum(!is.na(taxa_silva[, "Phylum"])),
      sum(!is.na(taxa_silva[, "Class"])),
      sum(!is.na(taxa_silva[, "Order"])),
      sum(!is.na(taxa_silva[, "Family"])),
      sum(!is.na(taxa_silva[, "Genus"])),
      sum(!is.na(taxa_silva[, "Species"]))
    ),
    Filtered = c(
      sum(!is.na(taxa_silva_filtered[, "Kingdom"])),
      sum(!is.na(taxa_silva_filtered[, "Phylum"])),
      sum(!is.na(taxa_silva_filtered[, "Class"])),
      sum(!is.na(taxa_silva_filtered[, "Order"])),
      sum(!is.na(taxa_silva_filtered[, "Family"])),
      sum(!is.na(taxa_silva_filtered[, "Genus"])),
      sum(!is.na(taxa_silva_filtered[, "Species"]))
    )
  )
  
  # Calculate percentage removed
  compare_df$PercentRemoved <- round((compare_df$Original - compare_df$Filtered) / compare_df$Original * 100, 1)
  
  # Display comparison table
  kable(compare_df, caption = "Comparison of Taxonomic Assignments Before and After Confidence Filtering")
  
  # Plot the comparison
  compare_long <- tidyr::pivot_longer(
    compare_df,
    cols = c("Original", "Filtered"),
    names_to = "Dataset",
    values_to = "Count"
  )
  
  # Order taxonomic levels
  compare_long$TaxonomicLevel <- factor(compare_long$TaxonomicLevel, 
                                      levels = c("Kingdom", "Phylum", "Class", 
                                               "Order", "Family", "Genus", "Species"))
  
  # Create comparison plot
  comparison_plot <- ggplot(compare_long, aes(x = TaxonomicLevel, y = Count, fill = Dataset)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    geom_text(aes(label = Count), position = position_dodge(width = 0.9), vjust = -0.5) +
    labs(title = "Effect of Confidence Filtering on Taxonomic Assignments",
         subtitle = paste("Thresholds:", paste(names(custom_thresholds), custom_thresholds, sep = "=", collapse = ", ")),
         x = "Taxonomic Level",
         y = "Number of ASVs with Assignment") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(comparison_plot)
  
  # Export filtered taxonomy
  write.csv(as.data.frame(taxa_silva_filtered), "results/taxonomy_confidence_filtered.csv", row.names = TRUE)
  cat("Saved confidence-filtered taxonomy to results/taxonomy_confidence_filtered.csv\n")
  
  # Create phyloseq object with filtered taxonomy if phyloseq exists
  if (exists("ps")) {
    # Create a version with confidence-filtered taxonomy
    ps_filtered <- add_confidence_filtered_taxonomy(ps, taxa_silva_filtered)
    
    # Save the filtered phyloseq object
    saveRDS(ps_filtered, "results/phyloseq_confidence_filtered.rds")
    cat("Created and saved phyloseq object with confidence-filtered taxonomy to results/phyloseq_confidence_filtered.rds\n")
  }
}

# Save all taxonomy results
if(!dir.exists("results")) dir.create("results")
saveRDS(taxa_results, "results/all_taxonomy_results.rds")
cat("Saved all taxonomy results to results/all_taxonomy_results.rds\n")

# Save the final consensus taxonomy as CSV
write.csv(taxa, "results/taxonomy.csv")
cat("Saved final taxonomy to results/taxonomy.csv\n")

# Create checkpoint after taxonomy assignment step
save_checkpoint("after_taxonomy", 
               c("seqtab.nochim", "taxa", "taxa_results", "sample.names", 
                 if(exists("track_df")) "track_df" else NULL),
               overwrite = TRUE)
cat("Checkpoint saved after taxonomy assignment step\n")
```

# Create Phyloseq Object

```{r phyloseq2}
# Create a directory for results if it doesn't exist
if(!dir.exists("results")) dir.create("results")

# Create sample data frame
cat("Creating sample metadata...\n")

# Extract sample group information (if follows naming pattern like Sample_GroupName_...)
sample_metadata <- data.frame(
  SampleID = sample.names,
  Group = sample.names,  # Default to sample name if no pattern is found
  row.names = sample.names
)

# Add run information if in multi-run mode
if (is_multi_run && exists("file_to_run")) {
  cat("Adding sequencing run information to sample metadata\n")
  sample_metadata$Run <- file_to_run
  
  # Try to extract experimental groups without the run prefix 
  # First remove the run prefix from sample names
  clean_names <- gsub("^[^_]*_", "", sample.names)
} else {
  # When not in multi-run mode, use the original sample names
  clean_names <- sample.names
}

# Try to extract experimental groups from sample names using a common pattern
# This is just a generic example - modify based on your actual naming convention
group_pattern <- ".*?_(.*?)_.*|.*?-(.*?)-.*"
group_matches <- regmatches(clean_names, 
                          regexec(group_pattern, clean_names))

# Extract group information if the pattern matched
valid_matches <- sapply(group_matches, length) > 1
if (any(valid_matches)) {
  for (i in which(valid_matches)) {
    match <- group_matches[[i]]
    # Use the first capturing group that matched
    for (j in 2:length(match)) {
      if (!is.na(match[j]) && match[j] != "") {
        sample_metadata$Group[i] <- match[j]
        break
      }
    }
  }
}

# Count the number of samples in each group
group_counts <- table(sample_metadata$Group)
cat("Detected sample groups:\n")
print(group_counts)

# Display run summary if in multi-run mode
if (is_multi_run && exists("file_to_run")) {
  # Cross-tabulate samples by group and run
  if (length(unique(sample_metadata$Group)) > 1) {
    group_by_run <- table(sample_metadata$Group, sample_metadata$Run)
    cat("\nSample counts by group and run:\n")
    print(group_by_run)
  } else {
    run_counts <- table(sample_metadata$Run)
    cat("\nSample counts by run:\n")
    print(run_counts)
  }
}

# First create basic phyloseq components
otu <- otu_table(seqtab.nochim, taxa_are_rows = FALSE)
tax <- tax_table(taxa)
sam <- sample_data(sample_metadata)

# Add ASV sequences as reference sequences
dna <- Biostrings::DNAStringSet(colnames(seqtab.nochim))
names(dna) <- colnames(seqtab.nochim)

# Replace sequence names with ASV_1, ASV_2, etc. for easier reference
asv_ids <- paste0("ASV_", seq(ncol(otu)))
taxa_names(otu) <- asv_ids
taxa_names(tax) <- asv_ids
names(dna) <- asv_ids

# Create phyloseq object (we'll add the tree later if available)
ps <- phyloseq(otu, tax, sam, dna)

# Inspect phyloseq object
cat("Created phyloseq object with:\n")
cat(" -", nsamples(ps), "samples\n")
cat(" -", ntaxa(ps), "ASVs\n")
cat(" -", sum(sample_sums(ps)), "total reads\n")
cat(" - Mean", round(mean(sample_sums(ps))), "reads per sample\n")

# Display sample read counts
ps_counts <- tibble(
  SampleID = sample_names(ps),
  Group = sample_data(ps)$Group,
  ReadCount = sample_sums(ps)
) %>%
  arrange(desc(ReadCount))

kable(head(ps_counts, 10), caption = "Sample Read Counts (Top 10)")

# Create checkpoint after phyloseq creation step
save_checkpoint("after_phyloseq", 
               c("seqtab.nochim", "taxa", "ps", "sample_metadata", 
                 if(exists("track_df")) "track_df" else NULL),
               overwrite = TRUE)
cat("Checkpoint saved after phyloseq creation step\n")
```

# Phylogenetic Tree Construction

```{r build-phylogenetic-tree}
# Function to build phylogenetic tree from ASV sequences
build_phylogenetic_tree <- function(seqs, method = "ML", bootstrap = 100, cores = NULL, 
                                    progressbar = TRUE, verbose = TRUE) {
  # Check if required packages are available
  if (!requireNamespace("DECIPHER", quietly = TRUE) || !requireNamespace("phangorn", quietly = TRUE)) {
    cat("ERROR: DECIPHER and phangorn packages are required for tree building.\n")
    cat("Please install them with BiocManager::install(c('DECIPHER', 'phangorn'))\n")
    return(NULL)
  }
  
  # Set number of cores for parallel processing
  if (is.null(cores)) {
    cores <- min(parallel::detectCores() - 1, 8)
    
    # Apple Silicon optimization
    if (exists("hw_caps") && hw_caps$apple_silicon) {
      perf_cores <- min(parallel::detectCores() * 0.7, 10)
      cores <- max(floor(perf_cores), 2)
      if (verbose) {
        cat(sprintf("Apple Silicon optimization: Using %d cores for tree building\n", cores))
      }
    }
  }
  
  # Convert sequences to DNAStringSet if they aren't already
  if (!inherits(seqs, "DNAStringSet")) {
    if (is.character(seqs)) {
      seqs <- Biostrings::DNAStringSet(seqs)
    } else {
      stop("Sequences must be DNAStringSet or character vector")
    }
  }
  
  if (verbose) cat("Starting phylogenetic tree construction with method:", method, "\n")
  
  # Phase 1: Multiple sequence alignment with DECIPHER
  if (verbose) cat("Performing multiple sequence alignment...\n")
  
  # Optimize for Apple Silicon if detected
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    old_veclib_setting <- Sys.getenv("VECLIB_MAXIMUM_THREADS")
    Sys.setenv(VECLIB_MAXIMUM_THREADS = as.character(cores))
    
    alignment <- DECIPHER::AlignSeqs(seqs, anchor = NA, processors = cores,
                                    verbose = progressbar)
    
    Sys.setenv(VECLIB_MAXIMUM_THREADS = old_veclib_setting)
  } else {
    alignment <- DECIPHER::AlignSeqs(seqs, anchor = NA, processors = cores, 
                                    verbose = progressbar)
  }
  
  # Convert to phangorn format
  if (verbose) cat("Converting alignment to phyDat format...\n")
  phang.align <- phangorn::phyDat(as(alignment, "matrix"), type = "DNA")
  
  # Phase 2: Calculate distance matrix
  if (verbose) cat("Calculating distance matrix...\n")
  dm <- phangorn::dist.ml(phang.align)
  
  # Phase 3: Build initial tree with Neighbor-Joining
  if (verbose) cat("Building initial Neighbor-Joining tree...\n")
  nj_tree <- phangorn::NJ(dm)
  
  # Check if we should return just the NJ tree
  if (method == "NJ") {
    if (verbose) cat("NJ tree construction complete.\n")
    return(nj_tree)
  } 
  
  # Phase 4: Maximum Likelihood optimization
  if (verbose) cat("Optimizing tree with Maximum Likelihood method...\n")
  
  # Initial fit
  if (verbose) cat("  Initial tree fitting...\n")
  fit <- phangorn::pml(nj_tree, data = phang.align)
  
  # Apply ML optimization
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    # Optimize for Apple Silicon
    fit_gtr <- phangorn::optim.pml(fit, model = "JC", 
                                  rearrangement = "stochastic",
                                  control = phangorn::pml.control(trace = progressbar, maxit = 15),
                                  optNni = TRUE,
                                  optBf = TRUE,
                                  multicore = TRUE,
                                  mc.cores = cores)
  } else {
    # Standard optimization
    fit_gtr <- phangorn::optim.pml(fit, model = "JC", 
                                 rearrangement = "stochastic",
                                 control = phangorn::pml.control(trace = progressbar))
  }
  
  # Phase 5: Bootstrap support (if requested)
  if (bootstrap > 0) {
    if (verbose) cat("Calculating bootstrap support with", bootstrap, "replicates...\n")
    
    bs <- phangorn::bootstrap.pml(fit_gtr, bs = bootstrap, 
                                multicore = TRUE, 
                                mc.cores = cores)
    
    tree <- phangorn::plotBS(fit_gtr$tree, bs, type = "phylogram")
    if (verbose) cat("Bootstrap analysis complete.\n")
  } else {
    tree <- fit_gtr$tree
  }
  
  if (verbose) cat("Phylogenetic tree construction complete.\n")
  return(tree)
}

# Check if we can build trees
if (can_build_trees) {
  cat("Building phylogenetic tree from ASV sequences...\n")
  
  # Get ASV sequences
  asv_sequences <- colnames(seqtab.nochim)
  
  # Convert to DNAStringSet
  dna <- Biostrings::DNAStringSet(asv_sequences)
  names(dna) <- paste0("ASV_", seq_along(asv_sequences))
  
  # Set tree building parameters based on dataset size
  large_asv_threshold <- 5000
  moderate_asv_threshold <- 1000
  
  # Adjust thresholds for Apple Silicon
  if (exists("hw_caps") && hw_caps$apple_silicon) {
    large_asv_threshold <- 8000
    moderate_asv_threshold <- 2000
    cat("Apple Silicon optimization: Increased ASV thresholds for tree building\n")
  }
  
  if (length(dna) > large_asv_threshold) {
    cat("Large number of ASVs detected (", length(dna), "). ",
        "Tree building may take a very long time.\n", sep = "")
    cat("Consider reducing the dataset before building a tree.\n")
    cat("Proceeding with NJ method only (faster)...\n")
    tree_method <- "NJ"
    bootstrap_reps <- 0
  } else if (length(dna) > moderate_asv_threshold) {
    cat("Moderately large number of ASVs (", length(dna), "). ",
        "Using NJ method with limited bootstrap...\n", sep = "")
    tree_method <- "NJ"
    bootstrap_reps <- 100
  } else {
    cat("ASV count suitable for ML tree building (", length(dna), ").\n", sep = "")
    tree_method <- "ML"
    bootstrap_reps <- 100
  }
  
  # Time the tree construction
  start_time <- Sys.time()
  
  # Build the tree
  asv_tree <- build_phylogenetic_tree(
    dna, 
    method = tree_method,
    bootstrap = bootstrap_reps,
    verbose = TRUE
  )
  
  # Report time taken
  end_time <- Sys.time()
  time_taken <- difftime(end_time, start_time, units = "mins")
  cat("Tree construction completed in", round(time_taken, 2), "minutes\n")
  
  # Save tree to file
  if (!dir.exists("results")) dir.create("results")
  tree_file <- "results/asv_tree.rds"
  saveRDS(asv_tree, tree_file)
  cat("Phylogenetic tree saved to", tree_file, "\n")
  
  # Create a summary of the tree's properties
  tree_metadata <- list(
    method = tree_method,
    bootstrap = bootstrap_reps,
    n_tips = length(asv_tree$tip.label),
    build_time = as.numeric(time_taken, units = "mins"),
    timestamp = Sys.time()
  )
  
  saveRDS(tree_metadata, "results/tree_metadata.rds")
  cat("Tree metadata saved to results/tree_metadata.rds\n")
  
  # Update phyloseq object with the tree if available
  if (!is.null(asv_tree)) {
    cat("Updating phyloseq object with phylogenetic tree...\n")
    
    # Make sure tree has the correct labels
    asv_ids <- taxa_names(ps)
    if (!identical(asv_ids, asv_tree$tip.label)) {
      cat("Adjusting tree tip labels to match ASV IDs...\n")
      # Map original sequence names to ASV IDs
      seq_to_asv <- structure(asv_ids, names = colnames(seqtab.nochim))
      asv_tree$tip.label <- seq_to_asv[match(asv_tree$tip.label, names(seq_to_asv))]
      
      # Check if any NAs in tree tips
      if (any(is.na(asv_tree$tip.label))) {
        warning("Some tree tips could not be mapped to ASV IDs")
        # Fix NAs by assigning sequential numbers
        na_tips <- which(is.na(asv_tree$tip.label))
        asv_tree$tip.label[na_tips] <- paste0("ASV_unknown_", seq_along(na_tips))
      }
    }
    
    # Add tree to phyloseq object
    phy_tree(ps) <- asv_tree
    cat("Phylogenetic tree added to phyloseq object\n")
  }
} else {
  cat("Skipping phylogenetic tree construction due to missing dependencies.\n")
  cat("Install DECIPHER and phangorn to enable this feature.\n")
  asv_tree <- NULL
}
```

# Rarefaction Analysis

```{r rarefaction-analysis}
# Check required packages for rarefaction analysis
can_do_rarefaction <- requireNamespace("vegan", quietly = TRUE)

if (can_do_rarefaction) {
  cat("Beginning rarefaction analysis...\n")
  
  # Function to calculate rarefaction curves
  calculate_rarefaction_curves <- function(ps, 
                                          step_size = 10, 
                                          label = NULL,
                                          max_depth = NULL) {
    
    cat("Calculating rarefaction curves...\n")
    
    # Get the abundance matrix
    abundances <- as.matrix(otu_table(ps))
    if(taxa_are_rows(ps)) {
      abundances <- t(abundances)
    }
    
    # Get sample data
    sample_data <- as.data.frame(sample_data(ps))
    
    # Get max depth if not specified
    if(is.null(max_depth)) {
      max_depth <- max(rowSums(abundances))
    }
    
    # Define depths at which to rarefy
    depths <- seq(1, max_depth, by = step_size)
    depths <- unique(c(depths, max_depth))
    
    # Initialize the results data frame
    rarefaction_curves <- data.frame(depth = numeric(0), 
                                    richness = numeric(0), 
                                    sample = character(0),
                                    stringsAsFactors = FALSE)
    
    # Process samples one by one to avoid dimension mismatches
    sample_names <- rownames(abundances)
    
    cat("Processing", length(sample_names), "samples...\n")
    
    for (s in seq_along(sample_names)) {
      sample_name <- sample_names[s]
      sample_counts <- abundances[s, ]
      sample_sum <- sum(sample_counts)
      
      # Create vector of depths that are valid for this sample
      valid_depths <- depths[depths <= sample_sum]
      
      if (length(valid_depths) == 0) {
        cat("Skipping sample", sample_name, "with insufficient reads\n")
        next
      }
      
      # Process each valid depth for this sample
      for (depth in valid_depths) {
        # Use vegan's rarecurve directly on a single sample
        # This is different from using rarefy on multiple samples
        tryCatch({
          # Convert sample to presence/absence
          sample_pa <- sample_counts > 0
          
          # Count richness at this depth using simple random sampling
          # Avoid vegan's rarefy function since that's triggering the error
          n_trials <- 10  # Number of random samples to average
          richness_vals <- numeric(n_trials)
          
          for (i in 1:n_trials) {
            # Randomly sample from the abundance vector
            # This is a simplification of what rarefy does
            random_sample <- sample(rep(seq_along(sample_counts), sample_counts), 
                                   size = depth, replace = FALSE)
            # Count unique taxa
            richness_vals[i] <- length(unique(random_sample))
          }
          
          # Calculate mean richness across trials
          richness <- mean(richness_vals)
          
          # Add to results
          sample_result <- data.frame(
            depth = depth,
            richness = richness,
            sample = sample_name,
            stringsAsFactors = FALSE
          )
          
          rarefaction_curves <- rbind(rarefaction_curves, sample_result)
        }, error = function(e) {
          cat("Error processing sample", sample_name, "at depth", depth, ":", 
              conditionMessage(e), "\n")
        })
      }
      
      # Progress indicator
      if (s %% 5 == 0 || s == length(sample_names)) {
        cat(sprintf("Processed %d of %d samples...\n", s, length(sample_names)))
      }
    }
    
    # Sort results by sample and depth
    rarefaction_curves <- rarefaction_curves[order(rarefaction_curves$sample, 
                                                 rarefaction_curves$depth), ]
    
    # Add sample metadata if available
    if(!is.null(sample_data) && nrow(sample_data) > 0) {
      # Ensure sample names match
      sample_data$sample_id <- rownames(sample_data)
      rarefaction_curves$sample_id <- rarefaction_curves$sample
      
      # Merge with sample data
      rarefaction_curves <- merge(rarefaction_curves, sample_data, 
                                by = "sample_id", all.x = TRUE)
    }
    
    return(rarefaction_curves)
  }
  
  # Rarefy at specific depth to create a rarefied phyloseq object
  rarefy_phyloseq <- function(ps, depth = NULL, seed = 123) {
    # Get the abundance matrix
    abundances <- as.matrix(otu_table(ps))
    if(taxa_are_rows(ps)) {
      abundances <- t(abundances)
    }
    
    # Determine rarefaction depth if not specified
    if(is.null(depth)) {
      depth <- min(rowSums(abundances))
      cat("Auto-selecting minimum sample depth:", depth, "reads\n")
    } else {
      cat("Rarefying to specified depth:", depth, "reads\n")
    }
    
    # Check if any samples have fewer reads than the rarefaction depth
    if(any(rowSums(abundances) < depth)) {
      excluded_samples <- rownames(abundances)[rowSums(abundances) < depth]
      cat("WARNING: The following samples have fewer reads than the rarefaction depth and will be excluded:\n")
      cat(paste(excluded_samples, collapse = ", "), "\n")
    }
    
    # Perform rarefaction
    ps_rare <- NULL
    tryCatch({
      ps_rare <- rarefy_even_depth(ps, sample.size = depth, 
                                  rngseed = seed, replace = FALSE, 
                                  trimOTUs = TRUE, verbose = TRUE)
      cat("Rarefaction successful!\n")
    }, error = function(e) {
      cat("Error in rarefaction:", conditionMessage(e), "\n")
      cat("Try a different rarefaction depth or check your data.\n")
    })
    
    return(ps_rare)
  }
  
  # Define an appropriate step size based on the dataset size
  # For larger datasets, we use larger steps to reduce computation time
  max_depth <- max(sample_sums(ps))
  if (max_depth > 100000) {
    step_size <- 1000  # Very large datasets
  } else if (max_depth > 50000) {
    step_size <- 500   # Large datasets
  } else if (max_depth > 10000) {
    step_size <- 200   # Medium datasets
  } else {
    step_size <- 100   # Small datasets
  }
  
  cat("Calculating rarefaction curves with step size:", step_size, "\n")
  
  # Generate rarefaction curves
  rarefaction_curves <- calculate_rarefaction_curves(ps, step_size = step_size)
  
  # Plot rarefaction curves
  if(!is.null(rarefaction_curves)) {
    # Get metadata for grouping
    sample_metadata <- as.data.frame(sample_data(ps))
    
    # Find suitable categorical variable for grouping
    metadata_cols <- colnames(sample_metadata)
    categorical_cols <- sapply(sample_metadata, function(col) {
      is.character(col) || is.factor(col) || length(unique(col)) < 10
    })
    
    group_var <- if(any(categorical_cols)) {
      metadata_cols[which(categorical_cols)[1]]  # Choose first categorical column
    } else {
      NULL  # No suitable grouping variable
    }
    
    # Basic rarefaction plot with grouping if available
    plot_title <- "Rarefaction Curves"
    
    if(!is.null(group_var) && group_var %in% colnames(rarefaction_curves)) {
      # Plot with grouping
      rare_plot <- ggplot(rarefaction_curves, 
                         aes(x = depth, y = richness, group = sample, 
                             color = .data[[group_var]])) +
        geom_line(alpha = 0.7) +
        labs(title = plot_title,
             subtitle = paste("Grouped by", group_var),
             x = "Sequencing Depth",
             y = "ASV Richness") +
        theme_minimal() +
        theme(legend.position = "right")
    } else {
      # Plot without grouping
      rare_plot <- ggplot(rarefaction_curves, 
                         aes(x = depth, y = richness, group = sample)) +
        geom_line(alpha = 0.7) +
        labs(title = plot_title,
             x = "Sequencing Depth",
             y = "ASV Richness") +
        theme_minimal()
    }
    
    # Enhanced visualization
    rare_plot <- rare_plot +
      geom_vline(xintercept = min(sample_sums(ps)), 
                linetype = "dashed", color = "blue", alpha = 0.7) +
      geom_vline(xintercept = median(sample_sums(ps)), 
                linetype = "dashed", color = "darkgreen", alpha = 0.7) +
      annotate("text", x = min(sample_sums(ps)), y = max(rarefaction_curves$richness),
              label = "Min depth", hjust = -0.1, vjust = 0, color = "blue") +
      annotate("text", x = median(sample_sums(ps)), y = max(rarefaction_curves$richness),
              label = "Median depth", hjust = -0.1, vjust = 1, color = "darkgreen")
    
    print(rare_plot)
    
    # Find suitable rarefaction depth
    # Typically, we choose a depth that retains most samples while reaching near-asymptotic richness
    sample_depths <- data.frame(
      sample = names(sample_sums(ps)),
      depth = sample_sums(ps)
    )
    
    # Recommend a rarefaction depth
    min_depth <- min(sample_depths$depth)
    median_depth <- median(sample_depths$depth)
    mean_depth <- mean(sample_depths$depth)
    
    cat("\nSample sequencing depth summary:\n")
    cat("Minimum depth:", min_depth, "reads\n")
    cat("Mean depth:", round(mean_depth, 1), "reads\n")
    cat("Median depth:", median_depth, "reads\n")
    cat("Maximum depth:", max(sample_depths$depth), "reads\n")
    
    # Create depth distribution plot
    depth_dist_plot <- ggplot(sample_depths, aes(x = depth)) +
      geom_histogram(bins = 30, fill = "steelblue", color = "black") +
      geom_vline(xintercept = min_depth, linetype = "dashed", color = "blue") +
      geom_vline(xintercept = median_depth, linetype = "dashed", color = "darkgreen") +
      labs(title = "Distribution of Sample Sequencing Depths",
           subtitle = "Dashed lines show min (blue) and median (green) depths",
           x = "Sequencing Depth (reads)",
           y = "Number of Samples") +
      theme_minimal()
    
    print(depth_dist_plot)
    
    # Analyze rarefaction curve saturation
    # To evaluate if samples have reached a plateau in their rarefaction curves
    evaluate_rarefaction_saturation <- function(curves, threshold = 0.05) {
      # Group by sample
      sample_curves <- split(curves, curves$sample)
      
      # Calculate rate of change at the end of each curve
      saturation_status <- data.frame(
        sample = character(),
        plateau_reached = logical(),
        plateau_depth = numeric(),
        final_slope = numeric(),
        stringsAsFactors = FALSE
      )
      
      for (sample_name in names(sample_curves)) {
        curve_data <- sample_curves[[sample_name]]
        # Sort by depth
        curve_data <- curve_data[order(curve_data$depth), ]
        
        # Need at least 3 points to calculate saturation
        if (nrow(curve_data) < 3) next
        
        # Calculate slopes between consecutive points
        depths <- curve_data$depth
        richness <- curve_data$richness
        slopes <- diff(richness) / diff(depths)
        
        # Check the last 20% of the curve (or at least the last 3 points)
        end_index <- max(3, round(length(slopes) * 0.2))
        end_slopes <- tail(slopes, end_index)
        
        # Calculate average slope at the end
        final_slope <- mean(end_slopes)
        
        # Normalize by maximum richness to get relative rate of change
        max_richness <- max(richness)
        relative_slope <- final_slope / max_richness
        
        # Determine if plateau has been reached (slope is below threshold)
        plateau_reached <- relative_slope < threshold
        
        # If plateau reached, find approximate depth where it started
        plateau_depth <- NA
        if (plateau_reached) {
          # Find where the curve first flattens (slope drops below threshold)
          relative_slopes <- slopes / max_richness
          # Check if any values meet the condition before accessing the first element
          if (any(relative_slopes < threshold, na.rm=TRUE)) {
            plateau_idx <- which(relative_slopes < threshold)[1]
            if (!is.na(plateau_idx)) {
              plateau_depth <- depths[plateau_idx]
            }
          }
        }
        
        # Add to results
        saturation_status <- rbind(saturation_status, data.frame(
          sample = sample_name,
          plateau_reached = plateau_reached,
          plateau_depth = plateau_depth,
          final_slope = relative_slope,
          stringsAsFactors = FALSE
        ))
      }
      
      return(saturation_status)
    }
    
    # Evaluate plateau status for each sample
    saturation_results <- evaluate_rarefaction_saturation(rarefaction_curves)
    
    # Summarize plateau results
    plateau_count <- sum(saturation_results$plateau_reached, na.rm = TRUE)
    total_samples <- nrow(saturation_results)
    
    cat("\nRarefaction curve saturation analysis:\n")
    cat(plateau_count, "out of", total_samples, "samples (", 
        round(plateau_count/total_samples*100, 1), "%) have reached a rarefaction plateau.\n", sep = "")
    
    # Suggest optimal rarefaction depth based on saturation analysis
    if (plateau_count > 0) {
      # Calculate median plateau depth for samples that reached plateau
      median_plateau <- median(saturation_results$plateau_depth, na.rm = TRUE)
      cat("Median plateau depth:", round(median_plateau), "reads\n")
      
      # Suggest a conservative depth that captures most plateaus but doesn't exclude too many samples
      suggested_depth <- min(median_plateau, median_depth)
      cat("Suggested rarefaction depth:", round(suggested_depth), "reads\n")
      
      # Count how many samples would be excluded at this depth
      excluded_count <- sum(sample_depths$depth < suggested_depth)
      if (excluded_count > 0) {
        cat("NOTE: Using this depth would exclude", excluded_count, "samples (", 
            round(excluded_count/nrow(sample_depths)*100, 1), "% of total)\n", sep = "")
      }
    } else {
      cat("No samples have clearly reached a plateau. Consider using the minimum depth for conservative rarefaction.\n")
      suggested_depth <- min_depth
    }
    
    # Create rarefied phyloseq object at suggested depth
    cat("\nCreating rarefied phyloseq object at suggested depth...\n")
    ps_rarefied <- rarefy_phyloseq(ps, depth = round(suggested_depth))
    
    # Create rarefied phyloseq object at minimum depth (most conservative approach)
    if (suggested_depth > min_depth) {
      cat("\nAlso creating a conservative rarefied phyloseq object at minimum depth...\n")
      ps_rarefied_min <- rarefy_phyloseq(ps, depth = min_depth)
    } else {
      ps_rarefied_min <- ps_rarefied  # They're the same in this case
    }
    
    # Compare alpha diversity before and after rarefaction
    if(!is.null(ps_rarefied)) {
      # Calculate alpha diversity metrics for all versions
      alpha_div_raw <- estimate_richness(ps, measures = c("Observed", "Shannon", "Simpson"))
      alpha_div_rare <- estimate_richness(ps_rarefied, measures = c("Observed", "Shannon", "Simpson"))
      
      # Add sample names as a column
      alpha_div_raw$sample <- rownames(alpha_div_raw)
      alpha_div_rare$sample <- rownames(alpha_div_rare)
      
      # Add data source column
      alpha_div_raw$data <- "Raw"
      alpha_div_rare$data <- "Rarefied"
      
      # Combine data
      alpha_div_combined <- rbind(alpha_div_raw, alpha_div_rare)
      
      # Add sample metadata
      sample_data_df <- as.data.frame(sample_data(ps))
      sample_data_df$sample <- rownames(sample_data_df)
      alpha_div_combined <- merge(alpha_div_combined, sample_data_df, by = "sample", all.x = TRUE)
      
      # Plot comparison
      p1 <- ggplot(alpha_div_combined, aes(x = data, y = Observed, fill = data)) +
        geom_boxplot() +
        labs(title = "Observed ASVs", x = "", y = "Richness") +
        theme_minimal() +
        theme(legend.position = "none")
      
      p2 <- ggplot(alpha_div_combined, aes(x = data, y = Shannon, fill = data)) +
        geom_boxplot() +
        labs(title = "Shannon Diversity", x = "", y = "Shannon Index") +
        theme_minimal() +
        theme(legend.position = "none")
      
      # Plot side by side
      grid.arrange(p1, p2, ncol = 2, top = "Alpha Diversity Before and After Rarefaction")
      
      # Correlation of diversity metrics before and after rarefaction
      common_samples <- intersect(alpha_div_raw$sample, alpha_div_rare$sample)
      
      if (length(common_samples) > 1) {  # Need at least 2 samples for correlation
        alpha_div_raw_common <- alpha_div_raw[alpha_div_raw$sample %in% common_samples, ]
        alpha_div_rare_common <- alpha_div_rare[alpha_div_rare$sample %in% common_samples, ]
        
        # Ensure same ordering
        alpha_div_rare_common <- alpha_div_rare_common[match(alpha_div_raw_common$sample, alpha_div_rare_common$sample), ]
        
        # Calculate correlations with proper error handling
        cor_observed <- tryCatch({
          # Check if there's enough variation to calculate correlation
          if (sd(alpha_div_raw_common$Observed) > 0 && sd(alpha_div_rare_common$Observed) > 0) {
            cor(alpha_div_raw_common$Observed, alpha_div_rare_common$Observed)
          } else {
            NA
          }
        }, error = function(e) NA)
        
        cor_shannon <- tryCatch({
          # Check if there's enough variation to calculate correlation
          if (sd(alpha_div_raw_common$Shannon) > 0 && sd(alpha_div_rare_common$Shannon) > 0) {
            cor(alpha_div_raw_common$Shannon, alpha_div_rare_common$Shannon)
          } else {
            NA
          }
        }, error = function(e) NA)
        
        cat("\nCorrelation between raw and rarefied diversity metrics:\n")
        if (is.na(cor_observed)) {
          cat("Observed ASVs: Not enough variation to calculate correlation\n")
        } else {
          cat("Observed ASVs: r =", round(cor_observed, 3), "\n")
        }
        
        if (is.na(cor_shannon)) {
          cat("Shannon Index: Not enough variation to calculate correlation\n")
        } else {
          cat("Shannon Index: r =", round(cor_shannon, 3), "\n")
        }
        
        # Create correlation plots (with error handling)
        par(mfrow = c(1, 2))
        
        # Plot for Observed ASVs
        plot(alpha_div_raw_common$Observed, alpha_div_rare_common$Observed,
             xlab = "Raw", ylab = "Rarefied", main = "Observed ASVs",
             pch = 19, col = alpha("darkblue", 0.7))
        abline(0, 1, lty = 2, col = "red")
        
        # Add correlation text if applicable
        if (!is.na(cor_observed)) {
          text_x <- max(alpha_div_raw_common$Observed, na.rm = TRUE) * 0.8
          text_y <- min(alpha_div_rare_common$Observed, na.rm = TRUE) * 1.2
          text(text_x, text_y, paste("r =", round(cor_observed, 3)))
        } else {
          text_x <- mean(range(alpha_div_raw_common$Observed, na.rm = TRUE))
          text_y <- min(alpha_div_rare_common$Observed, na.rm = TRUE) * 1.2
          text(text_x, text_y, "No correlation\n(insufficient variation)")
        }
        
        # Plot for Shannon Index
        plot(alpha_div_raw_common$Shannon, alpha_div_rare_common$Shannon,
             xlab = "Raw", ylab = "Rarefied", main = "Shannon Index",
             pch = 19, col = alpha("darkgreen", 0.7))
        abline(0, 1, lty = 2, col = "red")
        
        # Add correlation text if applicable
        if (!is.na(cor_shannon)) {
          text_x <- max(alpha_div_raw_common$Shannon, na.rm = TRUE) * 0.8
          text_y <- min(alpha_div_rare_common$Shannon, na.rm = TRUE) * 1.2
          text(text_x, text_y, paste("r =", round(cor_shannon, 3)))
        } else {
          text_x <- mean(range(alpha_div_raw_common$Shannon, na.rm = TRUE))
          text_y <- min(alpha_div_rare_common$Shannon, na.rm = TRUE) * 1.2
          text(text_x, text_y, "No correlation\n(insufficient variation)")
        }
        
        par(mfrow = c(1, 1))
      }
    }
    
    # Save rarefied phyloseq objects
    if(!is.null(ps_rarefied)) {
      ps_rarefied_path <- "results/phyloseq_rarefied.rds"
      saveRDS(ps_rarefied, ps_rarefied_path)
      cat("Saved rarefied phyloseq object to", ps_rarefied_path, "\n")
      
      if (exists("ps_rarefied_min") && !identical(ps_rarefied, ps_rarefied_min)) {
        ps_rarefied_min_path <- "results/phyloseq_rarefied_min.rds"
        saveRDS(ps_rarefied_min, ps_rarefied_min_path)
        cat("Saved conservative rarefied phyloseq object to", ps_rarefied_min_path, "\n")
      }
    }
    
    # Save rarefaction curves data
    rarefaction_path <- "results/rarefaction_curves.rds"
    saveRDS(rarefaction_curves, rarefaction_path)
    cat("Saved rarefaction curves data to", rarefaction_path, "\n")
    
    # Save rarefaction saturation analysis
    saturation_path <- "results/rarefaction_saturation.rds"
    saveRDS(saturation_results, saturation_path)
    cat("Saved rarefaction saturation analysis to", saturation_path, "\n")
    
    # Create checkpoint after rarefaction analysis step with additional metadata
    rarefaction_metadata <- list(
      timestamp = Sys.time(),
      step_size = step_size,
      sample_count = nsamples(ps),
      suggested_depth = if(exists("suggested_depth")) suggested_depth else NULL,
      min_depth = min_depth,
      median_depth = median_depth,
      mean_depth = mean_depth,
      plateau_percentage = if(exists("plateau_count")) round(plateau_count/total_samples*100, 1) else NULL
    )

    save_checkpoint("after_rarefaction", 
                  c("ps", "ps_rarefied", "rarefaction_curves", "saturation_results", 
                    "rarefaction_metadata", "alpha_div_combined"),
                  overwrite = TRUE)
    cat("Checkpoint saved after rarefaction analysis step\n")

    # Save rarefaction metadata separately for dashboard access
    if (!dir.exists("results")) dir.create("results")
    saveRDS(rarefaction_metadata, "results/rarefaction_metadata.rds")
    cat("Rarefaction metadata saved to results/rarefaction_metadata.rds\n")

    # Save alpha diversity comparison data if available
    if (exists("alpha_div_combined")) {
      saveRDS(alpha_div_combined, "results/alpha_diversity_comparison.rds")
      write.csv(alpha_div_combined, "results/alpha_diversity_comparison.csv", row.names = FALSE)
      cat("Alpha diversity comparison data saved to results/alpha_diversity_comparison.csv\n")
    }
  }
} else {
  cat("Skipping rarefaction analysis due to missing dependencies.\n")
  cat("Install 'vegan' package to enable this feature.\n")
}
```

# Export Final Results

```{r save-results}
# Create directory for results if it doesn't exist
if(!dir.exists("results")) dir.create("results")

# Save sequence table with ASV IDs
asv_tab <- as.data.frame(t(otu_table(ps)))
asv_tab$ASV_ID <- rownames(asv_tab)
write.csv(asv_tab, "results/seqtab_nochim.csv", row.names = FALSE)
cat("Saved ASV table to results/seqtab_nochim.csv\n")

# Save taxonomy table with ASV IDs
tax_tab <- as.data.frame(tax_table(ps))
tax_tab$ASV_ID <- rownames(tax_tab)
write.csv(tax_tab, "results/taxonomy.csv", row.names = FALSE)
cat("Saved taxonomy table to results/taxonomy.csv\n")

# The phyloseq object has already been saved earlier, just confirm it contains latest changes
# Check if the phyloseq object contains a tree and mention it in the message
if (!is.null(phy_tree(ps, errorIfNULL = FALSE))) {
  cat("Note: The saved phyloseq object includes a phylogenetic tree\n")
}

# Extract and save ASV sequences with IDs
asv_seqs <- refseq(ps)
asv_headers <- names(asv_seqs)
asv_strings <- paste(asv_seqs)

# Interleave headers and sequences for FASTA format
asv_fasta <- c(rbind(paste0(">", asv_headers), asv_strings))
write(asv_fasta, "results/ASVs.fasta")
cat("Saved ASV sequences to results/ASVs.fasta\n")

# Save parameter optimization results for future reference
param_optimization <- list(
  platform = if(exists("quality_profiles")) quality_profiles$platform else "Unknown",
  max_read_lengths = if(exists("quality_profiles")) 
    c(forward = quality_profiles$max_forward_cycle, reverse = quality_profiles$max_reverse_cycle) 
    else c(forward = NA, reverse = NA),
  truncation_lengths = c(forward = truncLen_forward, reverse = truncLen_reverse),
  maxEE = c(forward = maxEE_forward, reverse = maxEE_reverse),
  expected_amplicon_size = if(exists("expected_amplicon_size")) expected_amplicon_size else NA,
  expected_overlap = if(exists("expected_overlap")) expected_overlap else NA,
  primers = if(exists("primer_info")) primer_info else NA
)
saveRDS(param_optimization, "results/parameter_optimization.rds")
cat("Saved parameter optimization results to results/parameter_optimization.rds\n")

# Save workflow results summary with enhanced metadata
workflow_summary <- list(
  date = Sys.Date(),
  timestamp = Sys.time(),
  num_samples = nsamples(ps),
  num_asvs = ntaxa(ps),
  total_reads = sum(sample_sums(ps)),
  mean_reads_per_sample = mean(sample_sums(ps)),
  read_tracking = track_df,
  parameters = param_optimization,
  run_info = list(
    r_version = R.version.string,
    run_time = format(Sys.time()),
    has_tree = !is.null(phy_tree(ps, errorIfNULL = FALSE))
  ),
  read_stats = list(
    min_reads = min(sample_sums(ps)),
    max_reads = max(sample_sums(ps)),
    median_reads = median(sample_sums(ps)),
    read_distribution = quantile(sample_sums(ps), probs = c(0, 0.25, 0.5, 0.75, 1))
  ),
  taxonomy_stats = list(
    kingdom = sum(!is.na(tax_table(ps)[, "Kingdom"])),
    phylum = sum(!is.na(tax_table(ps)[, "Phylum"])),
    class = sum(!is.na(tax_table(ps)[, "Class"])),
    order = sum(!is.na(tax_table(ps)[, "Order"])),
    family = sum(!is.na(tax_table(ps)[, "Family"])),
    genus = sum(!is.na(tax_table(ps)[, "Genus"])),
    species = sum(!is.na(tax_table(ps)[, "Species"]))
  )
)
saveRDS(workflow_summary, "results/workflow_summary.rds")
write.csv(data.frame(
  Metric = names(unlist(workflow_summary[c("num_samples", "num_asvs", "total_reads", "mean_reads_per_sample")])),
  Value = unlist(workflow_summary[c("num_samples", "num_asvs", "total_reads", "mean_reads_per_sample")])
), "results/workflow_summary_stats.csv", row.names = FALSE)
cat("Saved enhanced workflow summary to results/workflow_summary.rds\n")
```

# Workflow Optimization Summary

This optimized workflow provides a streamlined approach to 16S rRNA amplicon sequence analysis with DADA2. Key optimizations include:

1. **Hardware Detection and Optimization**:
   - Automatic detection of available CPU cores and memory
   - Special optimizations for Apple Silicon processors
   - CPU parallelization adjusted to system capabilities

2. **Parameter Optimization**:
   - Automatic detection of optimal truncation lengths based on quality profiles
   - Optimization of expected error thresholds for quality filtering
   - Amplicon size estimation based on detected primers

3. **Memory Management**:
   - Batch processing for larger datasets to manage memory usage
   - Smart garbage collection optimized for different processor types
   - Pre-allocation of result objects to reduce memory fragmentation

4. **Performance Optimizations**:
   - Efficient parallel processing for all compute-intensive steps
   - Sample sub-selection for error learning to speed up processing
   - Optimized phylogenetic tree construction

The code has been simplified to remove unnecessary components:
- Removed unused GPU acceleration code
- Removed multi-run processing functionality
- Reduced excessive checkpointing
- Streamlined library and package detection

This optimized workflow should scale well from small to moderately large datasets while efficiently using available computational resources.



